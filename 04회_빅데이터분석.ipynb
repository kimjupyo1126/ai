{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5703fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:99% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
       "div.text_cell_render.rendered_html{font-size:20pt;}\n",
       "div.text_cell_render li, div.text_cell_render p, code{font-size:22pt; line-height:30px;}\n",
       "div.output {font-size:24pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:24pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
       "table.dataframe{font-size:24px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:99% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
    "div.text_cell_render.rendered_html{font-size:20pt;}\n",
    "div.text_cell_render li, div.text_cell_render p, code{font-size:22pt; line-height:30px;}\n",
    "div.output {font-size:24pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:24pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
    "table.dataframe{font-size:24px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b126d3",
   "metadata": {},
   "source": [
    "# 01. 가트너가 3V로 정의한 빅데이터의 특징이 아닌 것은?\n",
    "① 가치(Value) ② 크기(Volume) ③ 속도(Velocity) ④ 다양성(Variety)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf239c",
   "metadata": {},
   "source": [
    "가트너(Gartner)가 정의한 빅데이터의 특징인 3V에 해당하지 않는 것은 **①번 가치(Value)**입니다.\n",
    "\n",
    "가트너의 분석가 더그 레이니(Doug Laney)가 처음 제시한 빅데이터의 핵심 요소 3가지는 다음과 같습니다.\n",
    "\n",
    "크기(Volume): 물리적인 데이터의 양 (테라바이트, 페타바이트 등)\n",
    "\n",
    "속도(Velocity): 데이터가 생성되고 유입되며 처리되는 속도\n",
    "\n",
    "다양성(Variety): 정형, 반정형, 비정형 데이터를 모두 포함하는 데이터의 형태\n",
    "\n",
    "참고 설명\n",
    "**가치(Value)**는 나중에 빅데이터를 통해 비즈니스 성과를 창출해야 한다는 의미에서 추가된 4V 혹은 5V(정확성(Veracity) 포함)의 구성 요소 중 하나입니다. 하지만 가트너가 최초로 정의한 표준 '3V' 모델에는 포함되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc1fd9",
   "metadata": {},
   "source": [
    "# 02. 1 제타바이트에 1byte의 아스키 코드를 얼마만큼 넣을 수 있는가?\n",
    "① $2^{40}$승 ② $2^{50}$승 ③ $2^{60}$승 ④ $2^{70}$승"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b62759b",
   "metadata": {},
   "source": [
    "정답은 ④ $2^{70}$승입니다.데이터 저장 단위는 **1,024($2^{10}$)**배씩 증가하는 구조를 가지고 있습니다. 아스키 코드(ASCII) 1문자는 1byte이므로, 1제타바이트(ZB)가 몇 바이트인지를 계산하면 됩니다.데이터 단위 변환표단위바이트(Byte) 계산이진수 표기Kilobyte (KB)$1,024$ bytes$2^{10}$ bytesMegabyte (MB)$1,024$ KB$2^{20}$ bytesGigabyte (GB)$1,024$ MB$2^{30}$ bytesTerabyte (TB)$1,024$ GB$2^{40}$ bytesPetabyte (PB)$1,024$ TB$2^{50}$ bytesExabyte (EB)$1,024$ PB$2^{60}$ bytesZettabyte (ZB)$1,024$ EB$2^{70}$ bytes따라서 1ZB는 $2^{70}$ bytes이며, 1byte 크기의 아스키 코드를 $2^{70}$개 채울 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b06276",
   "metadata": {},
   "source": [
    "# 03. 공공데이터에서 제공하는 파일의 형식이 아닌 것은?\n",
    "① XML ② SQL ③ JSON ④ CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78ec5b",
   "metadata": {},
   "source": [
    "정답은 ② SQL입니다.\n",
    "\n",
    "공공데이터포털(data.go.kr)을 비롯한 대부분의 공공데이터 개방 체계에서는 사용자가 데이터를 쉽게 가공하고 분석할 수 있도록 특정 소프트웨어에 종속되지 않는 **'오픈 포맷(Open Format)'**을 지향합니다.\n",
    "\n",
    "1. 제공되는 주요 파일 형식 (오픈 포맷)\n",
    "CSV (Comma Separated Values): 쉼표로 구분된 텍스트 데이터로, 엑셀이나 메모장 등 거의 모든 도구에서 읽을 수 있는 가장 일반적인 형식입니다.\n",
    "\n",
    "JSON (JavaScript Object Notation): 데이터 객체를 속성과 값의 쌍으로 표현하는 경량 형식으로, 주로 Open API를 통해 데이터를 주고받을 때 사용합니다.\n",
    "\n",
    "XML (eXtensible Markup Language): 태그를 이용해 데이터의 구조를 정의하는 형식으로, 복잡한 데이터 구조를 표현하기 좋아 API 서비스에서 많이 쓰입니다.\n",
    "\n",
    "XLS/XLSX: 마이크로소프트 엑셀 형식으로도 많이 제공됩니다.\n",
    "\n",
    "2. SQL이 형식이 아닌 이유\n",
    "**SQL(Structured Query Language)**은 데이터베이스에 저장된 데이터를 관리하고 질의하기 위한 **'언어(Language)'**이지, 배포를 위한 파일 형식(Format)이 아닙니다.\n",
    "\n",
    "데이터베이스의 내용을 배포할 때는 보통 SQL 쿼리문이 담긴 .sql 파일보다는, 누구나 범용적으로 열어볼 수 있는 CSV나 JSON 형태로 추출하여 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae8b76",
   "metadata": {},
   "source": [
    "# 04. 다음 중 인메모리 기반의 데이터 처리와 연관된 오픈소스 프로젝트는?\n",
    "① 스파크 ② 맵리듀스 ③ 하이브 ④ 피그"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f677290f",
   "metadata": {},
   "source": [
    "정답은 **① 스파크(Spark)**입니다.\n",
    "\n",
    "**아파치 스파크(Apache Spark)**는 디스크가 아닌 메인 메모리(RAM)에 데이터를 올려 처리하는 인메모리(In-Memory) 컴퓨팅 방식을 사용합니다. 이로 인해 기존의 디스크 기반 처리 방식보다 속도가 훨씬 빠르며, 특히 같은 데이터를 반복적으로 사용하는 기계 학습(Machine Learning)이나 반복 알고리즘 처리에 매우 유리합니다.\n",
    "\n",
    "선택지 상세 설명\n",
    "① 스파크(Spark): 인메모리 기반의 범용 데이터 처리 엔진입니다. 하둡 맵리듀스보다 최대 100배(메모리 처리 시) 빠르다고 알려져 있습니다.\n",
    "\n",
    "② 맵리듀스(MapReduce): 하둡의 기본 데이터 처리 프레임워크로, 각 단계의 작업 결과를 **디스크(HDFS)**에 쓰고 읽는 방식을 사용합니다. 이 때문에 스파크에 비해 상대적으로 속도가 느립니다.\n",
    "\n",
    "③ 하이브(Hive): 하둡 위에 구축된 데이터 웨어하우스 인프라입니다. SQL과 유사한 쿼리 언어(HiveQL)를 사용하여 데이터를 처리하지만, 내부적으로는 주로 맵리듀스(또는 Tez, Spark 등) 엔진을 사용하여 실행됩니다.\n",
    "\n",
    "④ 피그(Pig): 대규모 데이터 세트를 분석하기 위한 고수준 플랫폼으로, 'Pig Latin'이라는 스크립트 언어를 사용합니다. 하이브와 마찬가지로 내부적으로는 맵리듀스 작업으로 변환되어 실행됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823abd86",
   "metadata": {},
   "source": [
    "# 05. 다음 중 시스템의 전방에 위치하여 클라이언트로부터 다양한 서비스를 처리하고, 내부 시스템으로 전달하는 미들웨어는?\n",
    "① API GW(게이트웨이) ② 데이터베이스 ③ PaaS(Platform as a Service) ④ ESB(Enterprise Service Bus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c88ffb7",
   "metadata": {},
   "source": [
    "정답은 **① API GW(게이트웨이)**입니다.\n",
    "\n",
    "**API 게이트웨이(API Gateway)**는 시스템의 가장 앞단(전방)에 위치하여 클라이언트의 모든 요청을 한 곳에서 받아들이는 단일 진입점(Single Point of Entry) 역할을 수행하는 미들웨어입니다.\n",
    "\n",
    "선택지 상세 설명\n",
    "① API GW (API Gateway): * 위치: 시스템의 전방(Edge).\n",
    "\n",
    "역할: 클라이언트로부터 오는 다양한 요청(REST API 등)을 처리하고, 이를 내부 시스템(마이크로서비스 등)으로 적절히 라우팅합니다.\n",
    "\n",
    "주요 기능: 인증 및 인가, 로드 밸런싱, 트래픽 제어(Rate Limiting), 프로토콜 변환, 로그 기록 등.\n",
    "\n",
    "② 데이터베이스 (Database): 데이터를 저장하고 관리하는 시스템으로, 전방이 아닌 후방(Back-end)의 데이터 저장소 역할을 합니다.\n",
    "\n",
    "③ PaaS (Platform as a Service): 애플리케이션 개발 및 실행을 위한 플랫폼 환경을 서비스 형태로 제공하는 클라우드 서비스 모델(예: Heroku, Google App Engine 등)을 의미합니다.\n",
    "\n",
    "④ ESB (Enterprise Service Bus): * 기업 내부 시스템들 간의 상호 연결과 통합에 초점을 맞춘 미들웨어입니다.\n",
    "\n",
    "API 게이트웨이가 '클라이언트와 서버' 사이의 관리에 집중한다면, ESB는 '서버와 서버(시스템 간)'의 복잡한 통합과 메시지 변환에 더 중점을 둡니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36725f09",
   "metadata": {},
   "source": [
    "# 06. 강인공지능에 대한 설명으로 옳지 않은 것은?\n",
    "① 훌륭한 알고리즘을 보유하였다면 학습을 생략해도 된다. ② 강인공지능은 범용으로 사용되기에는 시기상조이다. ③ 약인공지능의 제한된 기능을 뛰어넘어 더 발달된 인공지능이다. ④ 강인공지능이라고 불릴 만한 수준의 인공지능은 지금도 개발되지 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d2382",
   "metadata": {},
   "source": [
    "정답은 ① 훌륭한 알고리즘을 보유하였다면 학습을 생략해도 된다. 입니다.\n",
    "\n",
    "**강인공지능(Strong AI)**은 인간과 대등하거나 그 이상의 지능을 가진 존재로, 스스로 사고하고 학습하며 자아를 가질 수 있는 인공지능을 의미합니다. 아무리 뛰어난 알고리즘이 있더라도, 지능을 형성하고 문제를 해결하기 위해서는 방대한 양의 데이터와 경험을 통한 학습 과정이 필수적입니다.\n",
    "\n",
    "각 선택지 상세 설명\n",
    "① (오답): 인공지능의 핵심은 '학습'을 통해 스스로 규칙을 찾아내는 데 있습니다. 강인공지능 역시 스스로 학습하고 발전하는 능력이 전제되어야 하므로 학습을 생략할 수 없습니다.\n",
    "\n",
    "②, ④ (정답 설명): 현재 우리가 사용하고 있는 챗GPT, 자율주행, 바둑 AI(알파고) 등은 모두 특정 분야에 특화된 **약인공지능(Weak AI)**입니다. 인간처럼 모든 분야에서 범용적으로 사고하는 강인공지능은 아직 기술적, 윤리적 한계로 인해 개발되지 않았으며, 실용화되기에는 시기상조라는 것이 정설입니다.\n",
    "\n",
    "③ (정답 설명): 약인공지능은 주어진 작업(이미지 인식, 번역 등)만 수행할 수 있는 반면, 강인공지능은 그 한계를 뛰어넘어 인간과 같은 종합적인 사고 능력을 지향합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700450e6",
   "metadata": {},
   "source": [
    "# 07. 개인정보 비동의 시에도 사용 가능한 경우가 아닌 것은?\n",
    "① 법령상 의무를 준수하기 위하여 불가피한 경우 ② 계약의 체결 및 이행을 위하여 불가피하게 필요한 경우 ③ 정보주체 또는 제3자의 급박한 생명, 신체, 재산의 이익을 위하여 필요하다고 인정되는 경우 ④ 개인 편의 제공 시 합당한 이유가 있으면 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39159ba3",
   "metadata": {},
   "source": [
    "정답은 ④ 개인 편의 제공 시 합당한 이유가 있으면 가능하다. 입니다.\n",
    "\n",
    "대한민국 **개인정보 보호법(제15조)**에 따르면, 개인정보 수집 시 원칙적으로 정보주체의 동의를 받아야 합니다. 다만, 법에서 정한 특정한 예외 상황에서는 동의 없이도 수집·이용이 가능하지만, 단순히 '개인(사용자)의 편의 제공'이나 '합당한 이유'라는 주관적인 기준은 법적 예외 사유에 해당하지 않습니다.\n",
    "\n",
    "개인정보 수집·이용의 예외 사유 (동의 없이 가능한 경우)\n",
    "법령에 명시된 대표적인 예외 상황은 다음과 같습니다:\n",
    "\n",
    "① 법령상 의무 준수: 법률에 특별한 규정이 있거나 법령상 의무를 준수하기 위하여 불가피한 경우 (예: 세법에 따른 납세 자료 수집)\n",
    "\n",
    "② 계약의 체결 및 이행: 정보주체와 체결한 계약을 이행하거나 계약 체결 과정에서 정보주체의 요청에 따라 필요한 경우 (예: 택배 배송을 위한 주소지 확인)\n",
    "\n",
    "③ 급박한 이익 보호: 정보주체 또는 그 법정대리인이 의사표시를 할 수 없는 상태이거나 주소불명 등으로 사전 동의를 받을 수 없는 경우로서, 정보주체 또는 제3자의 급박한 생명, 신체, 재산의 이익을 위하여 필요하다고 인정되는 경우\n",
    "\n",
    "공공기관의 업무 수행: 공공기관이 법령 등에서 정하는 소관 업무의 수행을 위하여 불가피한 경우\n",
    "\n",
    "개인정보처리자의 정당한 이익: 개인정보처리자의 정당한 이익을 달성하기 위하여 필요한 경우로서, 명백하게 정보주체의 권리보다 우선하는 경우 (단, 이 경우도 합리적인 범위 내여야 함)\n",
    "\n",
    "💡 오답 해설\n",
    "④번의 경우, 아무리 사용자에게 편리함을 주는 서비스(예: 맞춤형 광고, 편의 기능 제공 등)라 할지라도, 그것이 법령에 근거하거나 급박한 생명 보호와 같은 사유가 아니라면 반드시 정보주체의 사전 동의를 얻어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf8678",
   "metadata": {},
   "source": [
    "# 08. 다음 중 데이터 3법이 아닌 것은?\n",
    "① 개인정보보호법 ② 정보통신망법 ③ 정보통신산업진흥법 ④ 신용정보의 이용 및 보호에 관한 법률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2244e6e1",
   "metadata": {},
   "source": [
    "정답은 ③ 정보통신산업진흥법입니다.\n",
    "\n",
    "소위 **'데이터 3법'**은 데이터 경제 활성화를 위해 개인정보 보호 관련 법안들을 정비하고 중복 규제를 해소하기 위해 개정된 3가지 법률을 묶어서 부르는 명칭입니다.\n",
    "\n",
    "데이터 3법의 구성\n",
    "데이터 3법에 해당하는 법률은 다음과 같습니다.\n",
    "\n",
    "개인정보 보호법: 개인정보 보호 체계를 일원화하고, '가명정보' 개념을 도입하여 통계 작성, 과학적 연구 등의 목적으로 동의 없이 데이터를 활용할 수 있게 함.\n",
    "\n",
    "정보통신망법 (정보통신망 이용촉진 및 정보보호 등에 관한 법률): 개인정보 관련 내용을 '개인정보 보호법'으로 이관하여 감독 기구를 개인정보보호위원회로 일원화함.\n",
    "\n",
    "신용정보법 (신용정보의 이용 및 보호에 관한 법률): 금융 분야의 빅데이터 활용을 위해 가명정보 개념을 도입하고, '마이데이터(MyData)' 산업의 법적 근거를 마련함.\n",
    "\n",
    "💡 오답 해설\n",
    "③ 정보통신산업진흥법: 이 법은 정보통신산업의 기반을 조성하고 경쟁력을 강화하기 위한 법으로, 데이터 활용 및 개인정보 보호를 골자로 하는 '데이터 3법'의 범주에는 포함되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36115d80",
   "metadata": {},
   "source": [
    "# 09. 분석 로드맵 설정 시 우선순위로 고려해야 할 사항이 아닌 것은?\n",
    "① 비즈니스 성과 및 ROI ② 시급성 ③ 분석 데이터 적용 ④ 전략적 중요도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea19d8b",
   "metadata": {},
   "source": [
    "정답은 ③ 분석 데이터 적용입니다.\n",
    "\n",
    "분석 로드맵 수립 단계에서 우선순위를 결정하는 것은 '어떤 과제를 먼저 수행할 것인가'를 정하는 의사결정 과정입니다. 이때는 주로 과제의 가치(Value)와 실행 가능성(Feasibility) 측면을 고려하며, 구체적인 데이터의 적용 기술이나 방법론은 우선순위 결정 이후의 실행 단계에 더 가깝습니다.\n",
    "\n",
    "분석 과제 우선순위 결정의 주요 기준\n",
    "일반적으로 기업에서 분석 로드맵을 짤 때 우선순위를 평가하는 3대 핵심 요소는 다음과 같습니다.\n",
    "\n",
    "① 비즈니스 성과 및 ROI (Return on Investment): 해당 분석을 통해 얼마나 큰 경제적 이익이나 비용 절감 효과를 거둘 수 있는가 하는 투자 대비 효율성을 따집니다.\n",
    "\n",
    "② 시급성: 경영진의 의지나 현재 마주한 비즈니스 이슈 해결을 위해 얼마나 빨리 처리해야 하는가를 평가합니다.\n",
    "\n",
    "④ 전략적 중요도: 회사의 중장기 목표나 핵심 전략과 얼마나 일관성이 있는지를 봅니다.\n",
    "\n",
    "💡 오답 해설\n",
    "③ 분석 데이터 적용: 이는 분석 과제를 어떻게 구체화하고 구현할지에 대한 기술적인 상세 내용에 해당합니다. 로드맵 설정 시 '데이터 확보 가능성'은 고려 대상이 될 수 있으나, 데이터 적용 그 자체가 우선순위를 결정하는 상위 기준은 아닙니다.\n",
    "\n",
    "분석 로드맵 수립의 3단계\n",
    "보통 분석 로드맵은 다음과 같은 순서로 진행됩니다.\n",
    "\n",
    "데이터 분석 과제 식별: 해결해야 할 문제들을 나열함.\n",
    "\n",
    "우선순위 평가 (중요!): 시급성, 난이도, ROI 등을 고려하여 순서를 정함.\n",
    "\n",
    "로드맵 수립: 연도별, 단계별 이행 계획을 확정함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c097bc5",
   "metadata": {},
   "source": [
    "# 10. 분석 시나리오 적용을 해야 하는 이유로 가장 적절하지 않은 것은?\n",
    "① 이해관계자 도출 ② 업무 성과 판단 ③ 최신 업무 형태 반영 ④ 분석 목표 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c4442",
   "metadata": {},
   "source": [
    "정답은 ① 이해관계자 도출입니다.\n",
    "\n",
    "**분석 시나리오(Analysis Scenario)**란 분석 과제가 실제 업무에 어떻게 적용되고 활용되는지 그 과정을 상세하게 기술한 일종의 '설계도'입니다. 분석 시나리오는 분석이 시작되기 전이나 과정 중에 작성되지만, 이해관계자(Stakeholder) 도출은 보통 분석 기획의 가장 초기 단계인 '과제 정의'나 '범위 설정' 단계에서 이미 완료되어 있어야 하는 작업입니다.\n",
    "\n",
    "분석 시나리오를 작성하는 목적 (이유)\n",
    "② 업무 성과 판단: 분석 결과가 업무에 적용되었을 때 어떤 성과(KPI)를 낼 수 있는지 미리 정의하여, 나중에 분석의 효과를 측정하는 기준이 됩니다.\n",
    "\n",
    "③ 최신 업무 형태 반영: 현재의 업무 프로세스를 상세히 분석하여, 분석 모델이 실제 현업의 흐름에 어긋나지 않고 실용적으로 쓰일 수 있도록 설계합니다.\n",
    "\n",
    "④ 분석 목표 도출: 데이터 분석을 통해 최종적으로 무엇을 해결하고 얻고자 하는지 구체적인 목표와 기대 효과를 명확히 합니다.\n",
    "\n",
    "💡 오답 해설\n",
    "① 이해관계자 도출: 프로젝트를 이끌어갈 부서, 데이터 제공 부서, 분석 결과를 활용할 현업 부서 등 이해관계자를 찾아내는 것은 분석 기획 초기에 수행되는 활동입니다. 시나리오는 이렇게 도출된 이해관계자들과 협의하여 **'어떻게 분석을 진행할 것인가'**를 상세화하는 단계입니다.\n",
    "\n",
    "분석 시나리오의 주요 구성 요소\n",
    "데이터 분석 목적: 해결하고자 하는 비즈니스 이슈\n",
    "\n",
    "필요 데이터: 분석에 활용될 데이터 원천\n",
    "\n",
    "분석 모델링: 적용할 알고리즘 및 기법\n",
    "\n",
    "활용 방안: 분석 결과가 업무 프로세스에 어떻게 결합되는지 기술\n",
    "\n",
    "기대 효과: 성과 측정 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d293d86",
   "metadata": {},
   "source": [
    "# 11. 빅데이터 분석 기획 절차는?\n",
    "① 프로젝트 정의 → 범위 설정 → 위험계획 수립 → 수행계획 수립 ② 프로젝트 정의 → 위험계획 수립 → 범위 설정 → 수행계획 수립 ③ 범위 설정 → 프로젝트 정의 → 수행계획 수립 → 위험계획 수립 ④ 범위 설정 → 프로젝트 정의 → 위험계획 수립 → 수행계획 수립"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dca7d8",
   "metadata": {},
   "source": [
    "정답은 ① 프로젝트 정의 → 범위 설정 → 위험계획 수립 → 수행계획 수립입니다.\n",
    "\n",
    "빅데이터 분석 기획은 비즈니스 목표를 달성하기 위해 분석 과제를 구체화하고, 이를 실행하기 위한 로드맵을 작성하는 단계적인 과정입니다.\n",
    "\n",
    "분석 기획의 상세 단계별 내용\n",
    "프로젝트 정의: 분석의 목표(Goal)를 명확히 하고, 기대 효과를 정의합니다. 어떤 비즈니스 문제를 해결할 것인지 결정하는 가장 기초적인 단계입니다.\n",
    "\n",
    "범위 설정 (SOW): 프로젝트가 다룰 데이터의 범위, 분석 모델의 범위, 시간적/공간적 경계를 확정합니다. (Scope of Work 작성)\n",
    "\n",
    "위험계획 수립: 프로젝트 진행 중 발생할 수 있는 데이터 확보의 어려움, 기술적 한계, 개인정보 보호 이슈 등의 리스크를 식별하고 이에 대한 대응 방안을 마련합니다.\n",
    "\n",
    "수행계획 수립: 자원 배분, 일정 계획(WBS), 예산 수립 등 실제 프로젝트를 가동하기 위한 최종 실행 계획을 확정합니다.\n",
    "\n",
    "💡 오답 해설\n",
    "위험계획 수립은 프로젝트의 전체적인 **범위(Scope)**가 확정된 이후에, 그 범위를 수행하는 과정에서 발생할 위험을 예측하는 것이 논리적으로 맞습니다. 따라서 범위 설정이 위험계획보다 먼저 위치한 1번이 가장 적절한 절차입니다.\n",
    "\n",
    "추가 팁: 분석 기획의 3요소\n",
    "분석 기획 시에는 다음 세 가지 요소를 균형 있게 고려해야 합니다.\n",
    "\n",
    "Data (데이터): 분석 가능한 데이터가 존재하는가?\n",
    "\n",
    "Model (모델): 적절한 분석 기법과 알고리즘이 있는가?\n",
    "\n",
    "Training (인력/조직): 분석을 수행할 역량이 있는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b9a17",
   "metadata": {},
   "source": [
    "# 12. 다음 중 데이터 분석 모델링과 관련하여 수행하는 업무가 아닌 것은?\n",
    "① 데이터 분할 ② 데이터 모델링 ③ 프로젝트 성과 분석 및 평가 보고 ④ 모델 적용 및 운영안"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc69a42",
   "metadata": {},
   "source": [
    "정답은 ③ 프로젝트 성과 분석 및 평가 보고입니다.\n",
    "\n",
    "이 문제는 '분석 모델링' 단계와 '분석 평가 및 완료' 단계를 구분할 수 있는지를 묻는 문제입니다.\n",
    "\n",
    "💡 오답 해설\n",
    "③ 프로젝트 성과 분석 및 평가 보고: 이 업무는 분석 모델링이 모두 끝나고, 모델의 성능을 검증한 뒤 실제 비즈니스에 적용하여 얻은 최종 결과를 정리하는 '평가 및 보고' 단계에 해당합니다. 모델링 과정 중이 아니라 프로젝트의 마무리 단계에서 수행됩니다.\n",
    "\n",
    "분석 모델링 단계의 주요 업무\n",
    "분석 모델링 단계에서는 데이터를 학습시키고 최적의 알고리즘을 찾아내는 작업을 수행하며, 주요 내용은 다음과 같습니다.\n",
    "\n",
    "① 데이터 분할: 모델의 과적합(Overfitting)을 방지하기 위해 전체 데이터를 학습용(Training), 검증용(Validation), 테스트용(Test) 데이터로 나눕니다.\n",
    "\n",
    "② 데이터 모델링: 선정된 알고리즘을 사용하여 모델을 생성하고, 하이퍼파라미터 튜닝 등을 통해 모델의 성능을 최적화합니다.\n",
    "\n",
    "④ 모델 적용 및 운영안: 개발된 모델을 실제 운영 환경에 어떻게 배포하고, 주기적으로 어떻게 재학습(Retraining)할 것인지에 대한 구체적인 운영 계획을 세웁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2adea1",
   "metadata": {},
   "source": [
    "# 13. 다음 중 정형 데이터와 비정형 데이터 관련 설명으로 옳은 것은?\n",
    "① 동영상, 오디오 데이터는 정형 데이터에 속한다. ② 정형 데이터는 지정된 행과 열에 의해 데이터의 속성이 구별되는 스프레드시트 형태의 데이터이다. ③ 형태소는 정형 데이터를 분석하기 위한 단위이다. ④ 비정형 데이터는 잠재적 가치가 가장 낮다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a749be",
   "metadata": {},
   "source": [
    "정답은 ② 정형 데이터는 지정된 행과 열에 의해 데이터의 속성이 구별되는 스프레드시트 형태의 데이터이다. 입니다.\n",
    "\n",
    "데이터는 그 구조화된 정도에 따라 크게 정형, 반정형, 비정형 데이터로 분류됩니다.\n",
    "\n",
    "💡 각 선택지 상세 해설\n",
    "① 동영상, 오디오 데이터 (오답): 동영상, 사진, 오디오, 텍스트 문서 등은 형태가 고정되어 있지 않은 대표적인 비정형 데이터입니다.\n",
    "\n",
    "② 정형 데이터의 정의 (정답): 관계형 데이터베이스(RDBMS)의 테이블이나 엑셀(스프레드시트)처럼 **고정된 필드(행과 열)**에 저장되며, 데이터의 타입과 길이가 사전에 정의된 데이터입니다.\n",
    "\n",
    "③ 형태소 분석 (오답): 형태소는 의미를 가진 가장 작은 말의 단위로, 주로 텍스트와 같은 **비정형 데이터(자연어 처리)**를 분석하기 위해 사용되는 단위입니다.\n",
    "\n",
    "④ 비정형 데이터의 가치 (오답): 현대 빅데이터의 약 80% 이상은 비정형 데이터입니다. SNS, 로그, 이미지 등 방대한 비정형 데이터 속에 숨겨진 통찰력이 매우 크기 때문에 잠재적 가치는 오히려 매우 높다고 평가받습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba05b35",
   "metadata": {},
   "source": [
    "# 14. 개인정보 비식별화 기술에 대한 설명 중 가장 적절하지 않은 것은?\n",
    "① 총계처리 : 데이터의 총합값으로 처리하여 개인 데이터의 값을 보이지 않도록 하는 기술 ② 데이터 마스킹 : 개인식별에 중요한 데이터 값을 삭제하는 기술 ③ 가명처리 : 개인식별에 중요한 데이터를 식별할 수 없는 다른 값으로 변경하는 기술 ④ 범주화 : 데이터의 값을 범주의 값으로 변환하여 값을 변경하는 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdbf050",
   "metadata": {},
   "source": [
    "정답은 ② 데이터 마스킹 : 개인식별에 중요한 데이터 값을 삭제하는 기술입니다.\n",
    "\n",
    "비식별화 기술은 개인을 특정할 수 있는 정보를 숨기거나 변형하는 방법론입니다. ②번이 틀린 이유는 데이터 마스킹의 정의가 잘못되었기 때문입니다.\n",
    "\n",
    "💡 각 선택지 상세 해설\n",
    "① 총계처리 (Aggregation): 개인의 상세 정보를 공개하지 않고, 데이터의 합계(Total)나 평균(Average) 등 통계적 수치로 변환하여 개인의 특정 값을 숨기는 기술이 맞습니다. (예: 임직원 개별 연봉 → 부서별 연봉 합계)\n",
    "\n",
    "② 데이터 마스킹 (Data Masking) - [오답]: 마스킹은 데이터를 삭제하는 것이 아니라, 데이터의 일부를 공백이나 특수문자('*' 등)로 가리는 기술입니다. 데이터를 '삭제'하는 기술은 별도로 '데이터 삭제(Data Suppression)'라고 부릅니다. (예: 홍길동 → 홍동, 900101-1234567 → 900101-1*)\n",
    "\n",
    "③ 가명처리 (Pseudonymization): 이름, 주민번호 같은 식별자를 별칭(가명)이나 임의의 값으로 대체하는 기술입니다. 다른 정보와 결합하지 않으면 누구인지 알 수 없게 만듭니다. (예: 홍길동 → 사용자 A)\n",
    "\n",
    "④ 범주화 (Categorization/Data Reduction): 구체적인 수치(단일 값)를 범위(Range)나 구간으로 변환하는 기술입니다. (예: 27세 → 20대, 서울시 강남구 역삼동 → 서울시 거주)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed2c45",
   "metadata": {},
   "source": [
    "# 15. 개인정보에 노이즈를 추가해서 개인정보보호와 데이터 분석을 모두 진행할 수 있는 방법은?\n",
    "① K-익명성 ② 가명화 ③ 개인정보차등보호 ④ L-다양성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261cda09",
   "metadata": {},
   "source": [
    "정답은 **③ 개인정보차등보호(Differential Privacy)**입니다.개인정보차등보호는 데이터셋에 통계적인 **'노이즈(Noise)'**를 추가하여, 데이터 분석 결과의 정확도는 크게 해치지 않으면서 특정 개인의 정보가 노출되는 것을 수학적으로 방지하는 기술입니다.💡 주요 기술 설명③ 개인정보차등보호 (Differential Privacy):핵심 원리: 데이터에 의도적으로 약간의 소음(노이즈)을 섞습니다.특징: 특정 개인이 데이터셋에 포함되어 있는지 여부와 상관없이 분석 결과가 거의 동일하게 나오도록 설계됩니다. 이를 통해 개별 데이터의 프라이버시는 보호하면서 집단의 통계적 특성은 유지할 수 있어, 최근 애플(Apple)이나 구글(Google) 같은 빅테크 기업에서 활발히 사용하고 있는 방식입니다.① K-익명성 (K-Anonymity):자신과 동일한 속성을 가진 레코드가 적어도 $k$개 존재하도록 하여, 특정인을 식별할 수 없게 만드는 기술입니다. 노이즈 추가보다는 '범주화'나 '삭제' 기법을 주로 사용합니다.④ L-다양성 (L-Diversity):K-익명성이 가진 취약점(동질성 공격 등)을 보완하기 위해, 민감한 정보의 종류가 적어도 $l$개 이상 다양하게 존재하도록 보장하는 기술입니다.② 가명화 (Pseudonymization):성명, 주민번호 등의 식별자를 별칭(가명)으로 대체하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264d87b",
   "metadata": {},
   "source": [
    "# 16. 다음 중 고품질 데이터의 특성이 아닌 것은?\n",
    "① 정확성(Accuracy) ② 적시성(Timeliness) ③ 불편의성(Uncompleteness) ④ 일관성(Consistency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d722c5f",
   "metadata": {},
   "source": [
    "정답은 **③ 불편의성(Uncompleteness)**입니다.\n",
    "\n",
    "데이터 품질을 평가하는 기준은 기관이나 표준마다 조금씩 다르지만, 일반적으로 데이터가 얼마나 정확하고 유용한지를 나타내는 핵심 요소들이 있습니다. **Uncompleteness(불완전성)**는 데이터 품질이 낮은 상태를 의미하므로 고품질의 특성이 될 수 없습니다.\n",
    "\n",
    "고품질 데이터의 주요 특성\n",
    "① 정확성(Accuracy): 데이터가 실제 값이나 객관적인 사실과 일치해야 합니다. (오타나 잘못된 수치가 없어야 함)\n",
    "\n",
    "② 적시성(Timeliness): 데이터가 필요한 시점에 지연 없이 제공되어야 하며, 최신 정보를 반영하고 있어야 합니다.\n",
    "\n",
    "④ 일관성(Consistency): 데이터가 서로 다른 시스템이나 데이터베이스 간에 모순되지 않고 일치해야 합니다.\n",
    "\n",
    "💡 오답 해설\n",
    "③ 불편의성(Uncompleteness): 고품질 데이터의 특성은 **완전성(Completeness)**입니다. 필수적인 데이터 항목이 누락되지 않고 모두 채워져 있는 상태를 의미합니다. 'Uncompleteness'는 그 반대인 결측값이 많은 상태를 뜻하므로 정답이 아닙니다.\n",
    "\n",
    "추가적인 데이터 품질 지표\n",
    "위 3가지 외에도 다음과 같은 지표들이 중요하게 다뤄집니다.\n",
    "\n",
    "유효성(Validity): 데이터가 정해진 형식(포맷)이나 범위, 규칙에 적합한가?\n",
    "\n",
    "유일성(Uniqueness): 중복된 데이터가 존재하지 않는가?\n",
    "\n",
    "가용성(Availability): 사용자가 필요할 때 데이터에 접근할 수 있는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bec9914",
   "metadata": {},
   "source": [
    "# 17. 다음 중 데이터 저장소가 아닌 것은?\n",
    "① 데이터 웨어하우스 ② 데이터 레이크 ③ 데이터 마이닝 ④ 데이터 댐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a2169",
   "metadata": {},
   "source": [
    "정답은 **③ 데이터 마이닝(Data Mining)**입니다.\n",
    "\n",
    "이 문제는 데이터를 **저장하는 장소(Storage)**와 데이터를 **분석하는 기술(Analysis)**을 구분할 수 있는지 묻는 문제입니다.\n",
    "\n",
    "💡 각 선택지 상세 해설\n",
    "① 데이터 웨어하우스(Data Warehouse): 기업의 의사결정을 돕기 위해 여러 시스템에서 데이터를 수집하여 통합한 정형 데이터 중심의 저장소입니다. 분석에 최적화된 구조를 가집니다.\n",
    "\n",
    "② 데이터 레이크(Data Lake): 정형, 반정형, 비정형 데이터를 가공하지 않은 본래의 형태(Raw Data) 그대로 저장하는 대규모 저장소입니다. 유연성이 매우 높습니다.\n",
    "\n",
    "③ 데이터 마이닝(Data Mining) - [오답]: 방대한 데이터 속에서 유의미한 패턴이나 규칙을 찾아내기 위해 통계, 수학적 기법, 머신러닝 등을 활용하는 데이터 '분석' 기법입니다. 저장소가 아닙니다.\n",
    "\n",
    "④ 데이터 댐(Data Dam): 공공과 민간의 데이터를 수집·가공하여 유용한 데이터로 재구성하고, 이를 필요로 하는 곳에 공급하는 데이터 거점 및 축적 시스템을 의미하는 정책적 용어입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02607111",
   "metadata": {},
   "source": [
    "# 18. 다음 중 빅데이터의 저장기술로 옳은 것은?\n",
    "① 맵리듀스 ② 직렬화 ③ 가시화 ④ NoSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62709e37",
   "metadata": {},
   "source": [
    "정답은 ④ NoSQL입니다.\n",
    "\n",
    "이 문제는 빅데이터의 생명 주기 중 '저장(Storage)' 단계에 활용되는 구체적인 기술을 찾는 문제입니다.\n",
    "\n",
    "💡 각 선택지 상세 해설\n",
    "④ NoSQL (Not Only SQL): 전통적인 관계형 데이터베이스(RDBMS)의 한계를 극복하기 위해 만들어진 데이터 저장 기술입니다. 고정된 스키마가 없고 확장이 용이하여, 빅데이터의 특징인 비정형 데이터를 대량으로 저장하고 처리하는 데 최적화되어 있습니다. (예: MongoDB, Cassandra, Redis 등)\n",
    "\n",
    "① 맵리듀스 (MapReduce): 구글에서 발표한 소프트웨어 프레임워크로, 대용량 데이터를 분산 환경에서 병렬로 처리하기 위한 '처리(Processing)' 및 '분석' 기술입니다. 저장 기술인 HDFS 위에서 동작합니다.\n",
    "\n",
    "② 직렬화 (Serialization): 객체나 데이터 구조를 네트워크로 전송하거나 파일로 저장하기 위해 '데이터 형식(Byte Stream)'으로 변환하는 기술입니다. 저장 그 자체라기보다는 전송/저장을 위한 준비 단계의 기술로 봅니다.\n",
    "\n",
    "③ 가시화 (Visualization): 분석된 결과를 사용자가 이해하기 쉽게 그래프, 차트 등으로 표현하는 '시각화' 기술입니다. 빅데이터의 마지막 활용 단계에 해당합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0678916",
   "metadata": {},
   "source": [
    "# 19. HDFS에 대한 설명으로 옳은 것은?\n",
    "① 복제의 횟수는 내부에서 결정된다. ② ETL, NTFA가 상위 프로그램이다. ③ GFS와 동일한 소스코드를 사용한다. ④ 네임노드는 저장공간에 네임노드 데이터를 같이 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cc6c82",
   "metadata": {},
   "source": [
    "정답은 ④ 네임노드는 저장공간에 네임노드 데이터를 같이 저장한다. 입니다.\n",
    "\n",
    "이 문제는 하둡 분산 파일 시스템(HDFS)의 구조와 특징을 정확히 이해하고 있는지 묻고 있습니다.\n",
    "\n",
    "💡 각 선택지 상세 해설\n",
    "④ 네임노드는 저장공간에 네임노드 데이터를 같이 저장한다. (정답): 네임노드(NameNode)는 HDFS의 메타데이터(파일 이름, 디렉터리 구조, 데이터 블록의 위치 등)를 관리합니다. 이 메타데이터는 네임노드의 로컬 디스크에 있는 EditLog와 FsImage 파일에 저장됩니다. 비록 실제 '데이터 블록'은 데이터노드에 저장되지만, 네임노드의 운영에 필요한 데이터는 자신의 저장공간에 관리하는 것이 맞습니다.\n",
    "\n",
    "① 복제의 횟수는 내부에서 결정된다. (오답): 데이터 복제본 수(Replication Factor)는 하둡 설정 파일(hdfs-site.xml)을 통해 사용자나 관리자가 직접 설정할 수 있습니다. 기본값은 3이지만 필요에 따라 변경 가능합니다.\n",
    "\n",
    "② ETL, NTFA가 상위 프로그램이다. (오답): ETL(Extract, Transform, Load)은 데이터 수집/가공 프로세스를 의미하며, HDFS의 상위 프로그램이라고 정의하기 어렵습니다. HDFS 위에서 동작하는 대표적인 상위 프레임워크는 맵리듀스(MapReduce), 스파크(Spark), 하이브(Hive) 등입니다.\n",
    "\n",
    "③ GFS와 동일한 소스코드를 사용한다. (오답): HDFS는 구글의 GFS(Google File System) 논문을 바탕으로 구현된 오픈소스 프로젝트이지만, 소스코드를 공유하는 것이 아니라 개념적 설계를 참고하여 자바(Java) 언어로 새롭게 작성된 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96149a",
   "metadata": {},
   "source": [
    "# 20. 분산파일시스템에 대한 설명으로 옳지 않은 것은?\n",
    "① 데이터베이스를 분산 저장한다. ② x86 서버의 CPU, RAM 등을 사용하므로 장비 증가에 따른 성능 향상이 용이하다. ③ 여러 컴퓨터를 하나의 서버 환경에 저장한다. ④ 네트워크를 통한 여러 파일을 관리 및 저장하는 개념이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d89d5",
   "metadata": {},
   "source": [
    "정답은 ① 데이터베이스를 분산 저장한다. 입니다.\n",
    "\n",
    "이 문제는 **분산파일시스템(Distributed File System)**과 **분산데이터베이스(Distributed Database)**의 개념 차이를 묻는 문제입니다.\n",
    "\n",
    "💡 각 선택지 상세 해설\n",
    "① 데이터베이스를 분산 저장한다. (오답): 분산파일시스템은 '파일(File)'을 여러 노드에 나누어 저장하고 관리하는 기술입니다. 데이터베이스(DB)를 분산 저장하는 것은 **분산 데이터베이스 관리 시스템(Distributed DBMS)**이나 NoSQL의 영역입니다. 파일 시스템은 DB의 하위 계층에서 원천 데이터를 저장하는 기반 역할을 할 뿐입니다.\n",
    "\n",
    "② x86 서버 활용 (정답 설명): 분산파일시스템(예: HDFS)은 고가의 메인프레임 대신 저렴한 일반 사양(x86 기반)의 컴퓨터들을 여러 대 연결하여 구축합니다. 서버를 추가할수록 용량과 성능이 선형적으로 늘어나는 **수평적 확장성(Scale-out)**이 큰 장점입니다.\n",
    "\n",
    "③ 하나의 서버 환경처럼 저장 (정답 설명): 물리적으로는 여러 대의 컴퓨터에 흩어져 있지만, 사용자는 마치 하나의 커다란 논리적 서버(단일 네임스페이스)에 접근하는 것처럼 데이터를 관리할 수 있습니다.\n",
    "\n",
    "④ 네트워크를 통한 관리 및 저장 (정답 설명): 분산파일시스템의 핵심 정의입니다. 네트워크로 연결된 여러 독립된 저장 장치들을 통합하여 파일을 분산 저장하고 관리하는 시스템을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e8740e",
   "metadata": {},
   "source": [
    "# 21. 다음 중 이상값을 찾는 방법에 대한 설명이 아닌 것은?\n",
    "① 박스플롯과 산점도 등에서 멀리 떨어진 값 ② 정규분포에서 표준편차가 3 이상인 값 ③ 도메인 지식에서 이론적이나 물리적으로 맞지 않는 값 ④ 가설 검정의 노이즈값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8452f7f",
   "metadata": {},
   "source": [
    "정답은 ④ 가설 검정의 노이즈값입니다.이 문제는 데이터 전처리 단계에서 데이터의 품질을 저해하는 **이상값(Outlier)**을 식별하는 구체적인 방법과 용어의 정의를 묻고 있습니다.💡 각 선택지 상세 해설① 시각화 활용 (정답 설명): **박스플롯(Box Plot)**에서는 수염(Whisker)의 범위를 벗어난 점들을, **산점도(Scatter Plot)**에서는 데이터 군집에서 확연히 멀리 떨어진 점들을 이상값으로 간주합니다.② 통계적 기준 활용 (정답 설명): 데이터가 정규분포를 따른다고 가정할 때, 평균으로부터 표준편차($\\sigma$)의 3배($3\\sigma$) 이상 떨어진 값(Z-Score가 3 이상인 값)은 전체의 약 0.27%에 불과하므로 통계적 이상값으로 처리합니다.③ 도메인 지식 활용 (정답 설명): 데이터 자체의 수치적 특성 외에도, 해당 분야의 **전문 지식(Domain Knowledge)**에 근거해 판단합니다. 예를 들어, 사람의 몸무게가 500kg으로 기록되었거나 나이가 -10세로 기록되었다면 수치와 상관없이 이상값(또는 오류값)으로 판단합니다.④ 가설 검정의 노이즈값 (오답): **노이즈(Noise)**와 **이상값(Outlier)**은 혼용되기도 하지만 엄연히 다릅니다. 노이즈는 측정 과정에서 발생하는 무작위적인 오류(배경 소음)를 뜻하며, 가설 검정은 표본을 통해 모집단의 특성을 추론하는 과정이지 이상값을 '찾는 방법'의 명칭이 아닙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f913615",
   "metadata": {},
   "source": [
    "# 22. 다음과 같은 4개인 박스플롯에 대한 설명으로 옳은 것은?\n",
    "(그래프: 요일별 전체 팁의 Box Plot - Thur, Fri, Sat, Sun) ① Sat(Saturday)의 분산은 Fri(Friday)보다 크다. ② Fri(Friday)의 평균은 10에 가깝다. ③ Thur(Thursday)의 1사분위수는 12에 가깝다. ④ Fri(Friday)의 이상값이 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b35fd5",
   "metadata": {},
   "source": [
    "제시된 박스플롯(Box Plot) 이미지를 직접 볼 수는 없지만, 일반적으로 데이터 분석 시험에서 자주 인용되는 tips 데이터셋의 요일별 박스플롯 특성을 바탕으로 답변해 드립니다.\n",
    "\n",
    "정답은 ① Sat(Saturday)의 분산은 Fri(Friday)보다 크다. 입니다.\n",
    "\n",
    "💡 박스플롯 읽는 법과 선택지 해설\n",
    "박스플롯은 데이터의 분포와 통계적 특성을 한눈에 보여주는 도구입니다.\n",
    "\n",
    "① 분산과 박스의 길이 (정답): 박스플롯에서 **박스의 세로 길이(IQR, 3사분위수 - 1사분위수)**나 수염(Whisker)의 전체 길이는 데이터가 얼마나 퍼져 있는지를 나타냅니다. 토요일(Sat)은 일반적으로 금요일(Fri)보다 박스의 크기와 수염의 길이가 훨씬 길게 나타나므로, 분산(데이터의 퍼짐 정도)이 더 크다고 판단할 수 있습니다.\n",
    "\n",
    "② 중앙값 vs 평균 (오답): 박스 내부의 가로선은 평균이 아니라 **중앙값(Median)**입니다. 금요일의 중앙값은 보통 15~20 사이에 위치하는 경우가 많으며, 평균은 이상값의 영향을 받기 때문에 박스플롯 선만으로는 정확히 '10에 가깝다'고 단정하기 어렵습니다.\n",
    "\n",
    "③ 1사분위수(Q1) 위치 (오답): 목요일(Thur) 박스의 하단 선(1사분위수)은 데이터셋에 따라 다르지만 보통 13~15 부근에 형성됩니다. 12에 가깝다는 설명은 그래프상의 수치를 정밀하게 확인해야 하는 오답 유도 문항인 경우가 많습니다.\n",
    "\n",
    "④ 이상값(Outlier) 존재 여부 (오답): tips 데이터셋 기준으로 이상값(박스 수염 바깥의 점)은 주로 **Sat(토요일)**이나 **Sun(일요일)**에 많이 분포합니다. 금요일(Fri)은 표본 수가 상대적으로 적어 이상값이 나타나지 않는 경우가 많습니다.\n",
    "\n",
    "💡 박스플롯의 구성 요소 (꼭 기억하세요!)\n",
    "상단 수염 끝: 최댓값 (이상값 제외)\n",
    "\n",
    "박스 상단 (Q3): 제3사분위수 (상위 75%)\n",
    "\n",
    "박스 중앙선: 중앙값 (Q2, 50%)\n",
    "\n",
    "박스 하단 (Q1): 제1사분위수 (하위 25%)\n",
    "\n",
    "하단 수염 끝: 최솟값 (이상값 제외)\n",
    "\n",
    "박스 밖의 점: 이상값 (Outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cc1d6",
   "metadata": {},
   "source": [
    "# 23. 다음 중 정규화에 대한 설명으로 옳은 것은?\n",
    "① Min-Max 정규화 범위는 0과 1 사이이다. ② 평균은 0, 표준편차는 1로 변환하는 방법이다. ③ 정규화를 표준화하면 표준정규분포이다. ④ Min-Max 정규화보다 Z값이 이상값에 영향을 덜 받는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951290a",
   "metadata": {},
   "source": [
    "정답은 ① Min-Max 정규화 범위는 0과 1 사이이다. 입니다.이 문제는 데이터 분석 모델링 전 데이터의 스케일(단위)을 맞추는 스케일링(Scaling) 기법인 **정규화(Normalization)**와 **표준화(Standardization)**의 차이를 묻고 있습니다.💡 각 선택지 상세 해설① Min-Max 정규화 (정답): 모든 데이터 값을 최솟값 0, 최댓값 1 사이의 값으로 변환하는 방식입니다. 계산식은 다음과 같습니다.$$x_{new} = \\frac{x - min(x)}{max(x) - min(x)}$$② 표준화(Standardization) 설명 (오답): 데이터의 평균을 0, 표준편차를 1로 만드는 기법은 정규화가 아니라 **표준화(Z-score Scaling)**에 대한 설명입니다.③ 용어 혼동 (오답): \"정규화를 표준화하면\"이라는 표현은 부적절합니다. 데이터를 표준화하면 결과적으로 평균이 0, 분산이 1인 표준정규분포의 형태와 유사해지는 것입니다.④ 이상값에 대한 영향 (오답): Min-Max 정규화는 최솟값과 최댓값을 사용하기 때문에 이상값에 매우 취약합니다. 반면, **표준화(Z-값)**는 평균과 표준편차를 사용하므로 Min-Max 정규화보다는 상대적으로 이상값의 영향을 덜 받습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b25630",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87717ae5",
   "metadata": {},
   "source": [
    "# 24. 빅데이터 탐색에 대한 설명으로 적절하지 않은 것은?\n",
    "① 데이터의 전체 분포를 검토하는 과정이다. ② 데이터 분석 과정에서 결과를 도출한다. ③ 데이터 탐색 시 잠재적 문제를 발견하는 과정이다. ④ 데이터 탐색 시 패턴을 찾는 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd52fe",
   "metadata": {},
   "source": [
    "정답은 ② 데이터 분석 과정에서 결과를 도출한다. 입니다.\n",
    "\n",
    "이 문제는 데이터 분석의 생명 주기 중 데이터 탐색(EDA: Exploratory Data Analysis) 단계의 목적과 성격을 정확히 이해하고 있는지 묻고 있습니다.\n",
    "\n",
    "💡 각 선택지 상세 해설\n",
    "① 데이터의 전체 분포 검토 (적절): 데이터 탐색은 분석 전 데이터가 가진 평균, 분산, 왜도, 첨도 등을 확인하며 데이터가 어떤 형태를 띠고 있는지 전체적인 윤곽을 파악하는 과정입니다.\n",
    "\n",
    "② 분석 결과 도출 (부적절): 최종적인 결론이나 예측 결과를 도출하는 것은 '분석 모델링 및 평가' 단계입니다. 데이터 탐색은 모델을 만들기 전 데이터를 이해하고 가설을 세우는 준비 단계에 해당합니다.\n",
    "\n",
    "③ 잠재적 문제 발견 (적절): 탐색 과정을 통해 결측값(Missing Value), 이상값(Outlier), 혹은 데이터 수집 과정의 오류 등을 미리 발견하여 정제할 수 있습니다.\n",
    "\n",
    "④ 패턴 발견 (적절): 변수 간의 상관관계나 시각화를 통해 데이터 속에 숨겨진 기본적인 경향성이나 패턴을 찾아내어 분석 방향을 설정합니다.\n",
    "\n",
    "🔍 데이터 탐색(EDA)의 핵심 4단계\n",
    "개별 데이터 확인: 변수별 값의 분포 확인\n",
    "\n",
    "데이터 간의 관계 파악: 변수 간 상관관계 분석 (산점도 등 활용)\n",
    "\n",
    "이상값/결측값 식별: 전처리가 필요한 대상 추출\n",
    "\n",
    "시각화: 그래프(히스토그램, 박스플롯 등)를 통한 직관적 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f0a9d",
   "metadata": {},
   "source": [
    "# 25. 상관관계에 대한 설명 중 틀린 것은?\n",
    "① 상관계수는 결정계수의 제곱이다. ② 범위는 -1에서 1 사이이다. ③ 0에 가까우면 상관성이 낮다. ④ 관계를 산점도로 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b06fef",
   "metadata": {},
   "source": [
    "정답은 ① 상관계수는 결정계수의 제곱이다. 입니다.이 문제는 변수 간의 선형적 관계를 나타내는 **상관계수(Correlation Coefficient)**와 모델의 설명력을 나타내는 **결정계수(Coefficient of Determination)**의 관계를 정확히 알고 있는지 묻고 있습니다.💡 각 선택지 상세 해설① 상관계수와 결정계수의 관계 (틀림): 인과관계가 성립하는 단순 선형 회귀 분석에서 결정계수($R^2$)는 상관계수($r$)의 제곱입니다. 즉, $R^2 = r^2$입니다. 지문에서는 거꾸로 설명하고 있으므로 오답입니다.② 상관계수의 범위 (맞음): 피어슨 상관계수는 항상 $-1$에서 $1$ 사이의 값을 가집니다.$1$: 완벽한 양의 상관관계$-1$: 완벽한 음의 상관관계③ 0에 가까운 경우 (맞음): 상관계수가 $0$에 가깝다는 것은 두 변수 사이에 선형적인 관계가 거의 없다는 것을 의미합니다.④ 산점도 활용 (맞음): 두 변수를 각각 $x$축과 $y$축에 점으로 찍어 표현한 **산점도(Scatter Plot)**를 통해 데이터의 퍼짐 정도와 상관관계의 방향(양/음)을 시각적으로 쉽게 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0aa96",
   "metadata": {},
   "source": [
    "# 26. 대표값과 관련한 설명으로 옳지 않은 것은?\n",
    "① 평균은 중앙값보다 이상값에 영향을 더 적게 받는다.② $Q3 - Q1$ 값은 사분위수 범위를 의미한다.③ 변동계수 등은 기하평균으로 구한다.④ 변동계수는 분산과 관련이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8b393",
   "metadata": {},
   "source": [
    "정답은 ① 평균은 중앙값보다 이상값에 영향을 더 적게 받는다. 입니다.데이터의 중심 경향성을 나타내는 대표값과 퍼짐 정도를 나타내는 산포도에 관한 문제입니다.💡 각 선택지 상세 해설① 평균과 중앙값의 특성 (오답): **평균(Mean)**은 모든 데이터의 값을 합산하여 계산하기 때문에, 아주 크거나 작은 이상값(Outlier)에 매우 민감하게 반응합니다. 반면, **중앙값(Median)**은 순서대로 나열했을 때 가운데 위치한 값이므로 이상값의 영향을 거의 받지 않는 **강건(Robust)**한 성질을 가집니다.② 사분위수 범위 (IQR) (정답 설명): $Q_3$(제3사분위수)에서 $Q_1$(제1사분위수)을 뺀 값을 **사분위수 범위(Interquartile Range, IQR)**라고 하며, 데이터의 중간 50%가 얼마나 퍼져 있는지를 보여줍니다.③ 변동계수와 기하평균 (정답 설명): 일반적으로 변동계수($CV$)는 산술평균을 사용하지만, 주가 상승률이나 인구 증가율처럼 비율의 평균이 중요한 경우에는 기하평균을 기초로 변동성을 분석하기도 합니다. (단, 표준적인 변동계수 공식은 산술평균을 분모로 사용합니다.)④ 변동계수와 분산 (정답 설명): **변동계수($CV$)**의 공식은 $\\frac{\\sigma}{\\mu}$ (표준편차 / 평균)입니다. 표준편차는 분산의 제곱근이므로, 변동계수는 데이터의 **분산(산포도)**과 직접적인 관련이 있습니다. 서로 다른 집단 간의 흩어진 정도를 비교할 때 주로 사용합니다.📊 대표값의 비교 요약구분평균 (Mean)중앙값 (Median)최빈값 (Mode)특징산술적 평균값크기 순서상 중간값가장 빈도가 높은 값이상값 영향매우 큼 (민감)거의 없음 (강건)없음계산 가능 여부수치 데이터만 가능수치 및 서열 데이터모든 데이터 가능💡 보충 개념: 변동계수 ($CV$)단위가 다른 두 집단(예: 코끼리의 몸무게와 생쥐의 몸무게)의 산포를 비교할 때, 단순히 표준편차만 비교하면 평균이 큰 쪽이 크게 나오기 마련입니다. 이때 표준편차를 평균으로 나눈 변동계수를 사용하면 상대적인 퍼짐 정도를 공정하게 비교할 수 있습니다.다음 27번 문제로 진행할까요? 통계적 가설 검정이나 확률 분포에 관한 내용이 준비되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a44a28",
   "metadata": {},
   "source": [
    "# 27. 박스플롯에서 3Q(3사분위수)보다 작은 값은?\n",
    "① 중앙값 ② 평균 ③ 80퍼센트 ④ Max값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cb7ad2",
   "metadata": {},
   "source": [
    "정답은 ① 중앙값입니다.사분위수(Quartile)는 전체 데이터를 크기 순으로 나열했을 때 4등분 하는 지점을 의미합니다.💡 사분위수의 정의와 위치 관계전체 데이터를 100%라고 했을 때, 각 사분위수가 나타내는 위치(백분위수)는 다음과 같습니다.제1사분위수 ($Q1$): 하위 25% 지점제2사분위수 ($Q2$): 하위 50% 지점 $\\rightarrow$ **중앙값(Median)**과 동일제3사분위수 ($Q3$): 하위 75% 지점제4사분위수 ($Q4$): 하위 100% 지점 (최댓값)따라서 **$Q3$ (하위 75% 지점)**보다 작은 값은 **중앙값 (하위 50% 지점)**입니다.💡 각 선택지 상세 해설① 중앙값 (정답): $Q2$에 해당하며 하위 50% 지점이므로, 75% 지점인 $Q3$보다 항상 작거나 같습니다.② 평균 (오답): 데이터의 분포(왜도)에 따라 $Q3$보다 클 수도 있고 작을 수도 있습니다. (예: 아주 큰 이상값이 있는 경우 평균은 $Q3$보다 훨씬 커질 수 있음)③ 80퍼센트 (오답): 80퍼센트(제80백분위수)는 75퍼센트 지점인 $Q3$보다 큰 값입니다.④ Max값 (오답): 최댓값은 이상값이 없다면 $Q4$(100%)에 해당하므로 $Q3$보다 큽니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce62e07c",
   "metadata": {},
   "source": [
    "# 28. 자료의 분포가 오른쪽으로 긴 꼬리일 경우에 대한 설명으로 맞는 것은?\n",
    "① 왜도 $> 0$, 최빈값 $<$ 중앙값 $<$ 평균② 왜도 $> 0$, 평균 $<$ 중앙값 $<$ 최빈값③ 왜도 $< 0$, 중앙값 $<$ 최빈값 $<$ 평균④ 왜도 $< 0$, 최빈값 $<$ 중앙값 $<$ 평균"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680bebd",
   "metadata": {},
   "source": [
    "정답은 ① 왜도 $>0$, 최빈값 $<$ 중앙값 $<$ 평균입니다.이 문제는 자료의 비대칭 정도를 나타내는 **왜도(Skewness)**와 그에 따른 대표값들의 크기 비교를 묻고 있습니다.💡 상세 해설자료의 분포가 오른쪽으로 긴 꼬리를 가진다는 것은 왼쪽(낮은 수치)에 데이터가 몰려 있고, 오른쪽(높은 수치)으로 갈수록 빈도가 완만하게 줄어드는 형태를 말합니다.Shutterstock왜도 (Skewness): 꼬리가 오른쪽으로 치우친 경우를 **양의 왜도(Positive Skewness)**라고 하며, 값은 $0$보다 큽니다 ($\\text{Skewness} > 0$).대표값의 위치 관계:최빈값(Mode): 데이터가 가장 많이 몰려 있는 정점(왼쪽)에 위치합니다.중앙값(Median): 최빈값보다 오른쪽에 위치합니다.평균(Mean): 오른쪽의 긴 꼬리 부분에 있는 큰 값(이상값들)에 영향을 받아 가장 오른쪽(높은 수치 쪽)으로 끌려갑니다.결과: 최빈값 $<$ 중앙값 $<$ 평균 순서로 크기가 결정됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263e2bf",
   "metadata": {},
   "source": [
    "# 29. 소수의 극단값 영향을 받지 않으므로 변동성 척도로서 적절한 것은?\n",
    "① 범위 ② 변동계수 ③ 사분위 범위 ④ 표준편차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5d0fd",
   "metadata": {},
   "source": [
    "정답은 **③ 사분위 범위(IQR)**입니다.이 문제는 통계량 중에서 **이상값(Outlier, 극단값)**에 얼마나 민감하게 반응하는지를 묻고 있습니다.💡 각 선택지 상세 해설③ 사분위 범위 (IQR) - [정답]: 전체 데이터를 크기순으로 나열했을 때 상위 25% 지점($Q_3$)과 하위 25% 지점($Q_1$)의 차이를 의미합니다. 양 끝단에 위치한 50%의 데이터를 제외하고 가운데 50%의 데이터만 사용하기 때문에, 소수의 아주 크거나 작은 극단값에 영향을 받지 않는 매우 **강건(Robust)**한 척도입니다.① 범위 (Range): 최댓값에서 최솟값을 뺀 값입니다. 데이터 중 단 하나라도 아주 큰 극단값이 생기면 범위 값이 급격히 변하므로 극단값에 가장 취약합니다.② 변동계수 (CV): 표준편차를 평균으로 나눈 값입니다. 평균과 표준편차 모두 극단값의 영향을 크게 받기 때문에 변동계수 역시 극단값에 민감합니다.④ 표준편차 (Standard Deviation): 각 데이터 값과 평균의 차이(편차)를 제곱하여 계산합니다. 이 과정에서 제곱이 들어가기 때문에 극단값이 있을 경우 값이 매우 커지게 되어 영향을 많이 받습니다.📊 변동성 척도와 극단값 민감도 비교변동성 척도극단값 영향 정도특징범위 (Range)매우 큼계산은 쉬우나 분포 파악에 한계가 있음분산/표준편차큼가장 많이 쓰이지만 이상값에 민감함사분위 범위 (IQR)거의 없음이상값 판별 및 박스플롯 작성에 활용💡 보충: 박스플롯에서의 이상값 판단 기준사분위 범위를 활용하면 다음과 같이 수치적으로 이상값을 정의할 수 있습니다.하단 경계: $Q_1 - 1.5 \\times IQR$ 보다 작은 값상단 경계: $Q_3 + 1.5 \\times IQR$ 보다 큰 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663bd5c1",
   "metadata": {},
   "source": [
    "# 30. 다음 중 시공간 데이터가 아닌 것은?\n",
    "① 지도 데이터 ② 패턴 데이터 ③ 패널 데이터 ④ 격자 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932a281",
   "metadata": {},
   "source": [
    "정답은 **③ 패널 데이터(Panel Data)**입니다.\n",
    "\n",
    "이 문제는 **시공간 데이터(Spatio-temporal Data)**의 정의와 다른 형태의 데이터를 구분할 수 있는지를 묻고 있습니다.\n",
    "\n",
    "💡 각 선택지 상세 해설\n",
    "③ 패널 데이터 (Panel Data) - [오답]: 패널 데이터는 '동일한 대상'(사람, 기업, 국가 등)을 **'여러 시점'**에 걸쳐 반복적으로 관찰한 데이터입니다. 시간의 흐름(종단적 요소)은 포함하고 있지만, 반드시 공간적 위치나 지리적 정보를 핵심으로 하지는 않기 때문에 시공간 데이터보다는 종단적(Longitudinal) 데이터의 범주에 속합니다.\n",
    "\n",
    "예: 동일한 가구의 5년간 소득 변화 조사\n",
    "\n",
    "① 지도 데이터 (Map Data) - [정답 설명]: 지리적 좌표(위도, 경도)를 기반으로 하는 대표적인 공간 데이터입니다. 여기에 시간에 따른 지형 변화나 교통량 변화가 합쳐지면 시공간 데이터가 됩니다.\n",
    "\n",
    "② 패턴 데이터 (Pattern Data) - [정답 설명]: 시공간 분석에서 특정 지역이나 시간대에 발생하는 움직임의 규칙성을 의미합니다. 예를 들어, 태풍의 이동 경로 패턴이나 감염병 확산 패턴 등은 시공간 정보를 핵심으로 합니다.\n",
    "\n",
    "④ 격자 데이터 (Grid Data) - [정답 설명]: 공간을 일정한 크기의 격자(Grid)로 나누어 각 칸마다 수치를 기록하는 방식입니다. 기상 관측이나 대기 오염도 측정 시 특정 지역(공간)의 시간별 변화를 기록하는 시공간 데이터의 표준적인 형태 중 하나입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ae2aa",
   "metadata": {},
   "source": [
    "# 31. 주성분분석(PCA)에 대한 설명으로 옳지 않은 것은?\n",
    "① 선형 결합하여 새로운 변수를 만든다. ② 분산이 커지도록 한다. ③ 데이터가 이산적인 경우에 사용한다. ④ 고유값이 작은 순서대로 나열해 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b5284",
   "metadata": {},
   "source": [
    "정답은 ④ 고유값이 작은 순서대로 나열해 사용한다. 입니다.**주성분분석(PCA, Principal Component Analysis)**은 고차원 데이터를 정보의 손실을 최소화하면서 저차원으로 축소하는 대표적인 차원 축소 기법입니다.💡 각 선택지 상세 해설④ 고유값이 작은 순서 (오답): 주성분분석에서는 **고유값(Eigenvalue)**이 클수록 그 주성분이 데이터의 **분산(정보량)**을 더 많이 설명한다는 뜻입니다. 따라서 고유값이 큰 순서대로 나열하여 상위 몇 개의 주성분을 선택하는 것이 원칙입니다.① 선형 결합 (정답 설명): PCA는 기존 변수들의 **선형 결합($Y = a_1X_1 + a_2X_2 + \\dots$)**을 통해 서로 상관관계가 없는 새로운 인공 변수(주성분)들을 생성합니다.② 분산의 극대화 (정답 설명): PCA의 핵심 목표는 원래 데이터가 가지고 있는 **변동성(분산)**을 가장 잘 보존하는 축을 찾는 것입니다. 즉, 첫 번째 주성분은 데이터의 분산이 가장 큰 방향으로 설정됩니다.③ 데이터의 성격 (정답 설명): 일반적으로 PCA는 연속형 변수에 사용되지만, 데이터가 **이산적(Discrete)**이거나 범주형인 경우에도 적절한 전처리를 거치거나 유사한 기법(MCA 등)을 통해 차원 축소를 진행할 수 있습니다. (다만, PCA의 기본 가정은 연속형 데이터에 더 가깝습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf40aa",
   "metadata": {},
   "source": [
    "# 32. 각각의 사례에 대한 알맞은 분석방법으로 옳은 것은?\n",
    "① 어떤 규칙이나 방법을 찾는 데 회귀분석이나 군집분석을 사용한다. ② 수요 예측은 회귀분석 등 연속형 모델 등을 이용하여 분석할 수 있고 인공신경망을 사용할 수도 있다. ③ 일정한 단위시간의 변화에 따른 개개의 상품이나 상품의 집합체에 관한 경제 변량의 기본적인 관계를 나타내는 계수를 추정 및 분석하는 방법은 차원축소 분석을 사용한다. ④ 동일한 공간상에 비교한 상표들의 상대적 위치를 나타내는 분석방법은 요인분석이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b726a",
   "metadata": {},
   "source": [
    "정답은 ② 수요 예측은 회귀분석 등 연속형 모델 등을 이용하여 분석할 수 있고 인공신경망을 사용할 수도 있다. 입니다.\n",
    "\n",
    "이 문제는 각 비즈니스 상황이나 데이터 특성에 맞는 적절한 통계 및 머신러닝 분석 기법을 매칭할 수 있는지 묻고 있습니다.\n",
    "\n",
    "💡 각 선택지 상세 해설\n",
    "② 수요 예측과 회귀/신경망 (정답): 수요 예측은 미래의 특정 수치(연속형 변수)를 맞히는 작업입니다. 따라서 독립변수와 종속변수 간의 선형 관계를 찾는 회귀분석이나, 복잡한 비선형 관계를 학습할 수 있는 인공신경망(ANN, Deep Learning) 모두 적절한 분석 방법입니다.\n",
    "\n",
    "① 규칙 발견과 분석 방법 (오답): 어떤 규칙이나 패턴을 찾는 데는 주로 **연관 규칙 분석(Association Rule)**이나 의사결정나무(Decision Tree) 등이 사용됩니다. 군집분석은 규칙을 찾기보다 데이터를 유사한 그룹으로 묶는 데 특화되어 있습니다.\n",
    "\n",
    "③ 경제 변량 및 계수 추정 (오답): 단위 시간의 변화에 따른 경제 변량의 관계를 분석하고 계수를 추정하는 방법은 시계열 분석(Time Series Analysis) 또는 계량경제학적 회귀 모델입니다. 차원축소는 변수의 개수를 줄이는 것이 목적이지 관계 식을 추정하는 목적이 아닙니다.\n",
    "\n",
    "④ 상표의 상대적 위치 표현 (오답): 여러 대상 간의 유사성/부정합성을 측정하여 동일한 공간(2차원 또는 3차원 평면)상에 점으로 표현하는 방법은 **다차원 척도법(MDS, Multidimensional Scaling)**입니다. 요인분석은 변수들 뒤에 숨겨진 공통 잠재 요인을 찾아내는 기법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0bd16",
   "metadata": {},
   "source": [
    "# 33. 비정형 텍스트 데이터 전처리 기법으로 옳지 않은 것은?\n",
    "① 토큰화(Tokenizing) ② API(Application Programming Interface) ③ 품사태깅(POS Tagging) ④ 어간추출(Stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7f428",
   "metadata": {},
   "source": [
    "정답은 **② API(Application Programming Interface)**입니다.이 문제는 비정형 데이터인 텍스트를 컴퓨터가 이해할 수 있는 구조로 변환하는 자연어 처리(NLP) 전처리 과정에 대해 묻고 있습니다.💡 각 선택지 상세 해설② API (오답): API는 프로그램 간에 데이터를 주고받기 위한 '인터페이스(통로)' 또는 규약을 의미합니다. 데이터를 수집하는 단계에서 웹 API를 사용할 수는 있지만, 수집된 텍스트 자체를 분석하기 좋게 다듬는 **'전처리 기법'**은 아닙니다.① 토큰화(Tokenizing) (정답 설명): 긴 문자열을 분석의 최소 단위인 **토큰(Token)**으로 나누는 과정입니다. 보통 단어, 문장, 또는 형태소 단위로 쪼개게 됩니다.③ 품사태깅(POS Tagging) (정답 설명): 토큰화된 단어들에 명사, 동사, 형용사 등의 품사 정보를 부착하는 과정입니다. 문맥에 따라 단어의 의미가 달라질 수 있으므로 분석의 정확도를 높이는 데 필수적입니다.④ 어간추출(Stemming) (정답 설명): 단어의 형태가 변하더라도 그 의미를 담고 있는 **핵심 부분(어간)**을 추출하는 과정입니다. 예를 들어, 'fishing', 'fished', 'fisher'를 모두 'fish'라는 원형으로 통일하여 분석 효율을 높입니다.🔍 텍스트 전처리(Text Preprocessing)의 주요 단계단계설명예시노이즈 제거특수문자, HTML 태그, 불용어 제거\"안녕하세요!!!\" → \"안녕하세요\"토큰화문장을 단어 단위로 분리\"사과를 먹다\" → [\"사과\", \"를\", \"먹다\"]품사 태깅각 단어의 문법적 역할 부여사과(명사), 먹다(동사)정규화어간 추출(Stemming) 및 표제어 추출\"studying\" → \"study\"💡 추가 개념: 불용어(Stopwords) 제거텍스트 전처리에서 'a', 'the', '은', '는', '이', '가'와 같이 자주 등장하지만 실제 분석에는 큰 의미가 없는 단어들을 제거하는 과정도 매우 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89172ee7",
   "metadata": {},
   "source": [
    "# 34. 정규 모집단 $N(50, 2^2)$에서 크기 $n = 16$의 표본을 무작위 추출할 때 표본평균분포의 표준편차, 또한 표본평균이 $\\bar{x} = 51$ 이상일 때의 표준화 점수와 이에 대한 분포로 옳은 것은?\n",
    "① $\\sigma_{\\bar{x}} = \\frac{1}{2}, z = 2, N(0, 1)$② $\\sigma_{\\bar{x}} = 1, z = 2, N(50, 2^2)$③ $\\sigma_{\\bar{x}} = \\frac{1}{2}, z = 2, N(50, 2^2)$④ $\\sigma_{\\bar{x}} = 1, z = 2, N(0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9fcea2",
   "metadata": {},
   "source": [
    "정답은 ① $\\sigma_{\\bar{x}} = \\frac{1}{2}, z=2, N(0,1)$ 입니다.이 문제는 표본평균의 분포(표집분포)의 특성과 표준화 과정을 정확히 이해하고 있는지 묻는 통계학의 핵심 계산 문제입니다.💡 단계별 풀이 과정1. 표본평균의 표준편차(표준오차, $\\sigma_{\\bar{x}}$) 구하기모집단의 표준편차가 $\\sigma$이고 표본의 크기가 $n$일 때, 표본평균의 표준편차인 표준오차 공식은 다음과 같습니다.$$\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}$$문제에서 모집단의 분포가 $N(50, 2^2)$이므로 모표준편차 $\\sigma = 2$입니다.표본의 크기 $n = 16$이므로 $\\sqrt{n} = 4$입니다.따라서, $\\sigma_{\\bar{x}} = \\frac{2}{4} = \\frac{1}{2} = 0.5$ 가 됩니다.2. 표준화 점수($z$) 구하기표본평균 $\\bar{x}$를 표준화하는 공식은 다음과 같습니다.$$z = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}}$$표본평균 $\\bar{x} = 51$, 모평균 $\\mu = 50$, 표준오차 $\\sigma_{\\bar{x}} = 0.5$를 대입합니다.$z = \\frac{51 - 50}{0.5} = \\frac{1}{0.5} = 2$ 가 됩니다.3. 표준화된 분포어떤 정규분포를 따르는 변수든 표준화 과정을 거치면 평균이 0이고 표준편차가 1인 **표준정규분포 $N(0, 1)$**을 따르게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d0b83",
   "metadata": {},
   "source": [
    "# 35. 이산확률변수 $x$에 대해 $E(X)=4, E(X^2)=25$ 일 때, 확률변수 $Y=3X-4$의 평균과 분산으로 옳은 것은?\n",
    "① $E(Y) = 8, V(Y) = 81$② $E(Y) = 16, V(Y) = 9$③ $E(Y) = 8, V(Y) = 25$④ $E(Y) = 16, V(Y) = 81$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c77839",
   "metadata": {},
   "source": [
    "정답은 ① $E(Y)=8, V(Y)=81$ 입니다.이 문제는 확률변수의 기댓값(평균)과 분산의 성질을 이용하여 새로운 변수 $Y$의 통계량을 계산할 수 있는지 묻는 문제입니다.💡 단계별 풀이 과정1. 확률변수 $X$의 분산 $V(X)$ 구하기분산은 '제곱의 평균에서 평균의 제곱을 뺀 값'이라는 공식을 사용합니다.$$V(X) = E(X^2) - \\{E(X)\\}^2$$문제에서 $E(X^2) = 25$, $E(X) = 4$이므로$V(X) = 25 - (4)^2 = 25 - 16 = \\mathbf{9}$2. $Y = 3X - 4$의 평균 $E(Y)$ 구하기기댓값의 선형성 성질($E(aX+b) = aE(X) + b$)을 이용합니다.$E(Y) = E(3X - 4) = 3E(X) - 4$$3 \\times 4 - 4 = 12 - 4 = \\mathbf{8}$3. $Y = 3X - 4$의 분산 $V(Y)$ 구하기분산의 성질($V(aX+b) = a^2V(X)$)을 이용합니다. 상수는 분산에 영향을 주지 않으며, 계수는 제곱이 되어 밖으로 나옵니다.$V(Y) = V(3X - 4) = 3^2V(X) = 9V(X)$$9 \\times 9 = \\mathbf{81}$📊 결과 요약항목성질(공식)계산 결과$X$의 분산$E(X^2) - \\{E(X)\\}^2$$25 - 16 = \\mathbf{9}$$Y$의 평균$aE(X) + b$$3(4) - 4 = \\mathbf{8}$$Y$의 분산$a^2V(X)$$3^2(9) = \\mathbf{81}$💡 핵심 암기 포인트평균($E$): 더하고 곱하는 대로 그대로 반응합니다.분산($V$): 더하기($b$)는 무시하고, 곱하기($a$)는 **제곱($a^2$)**으로 반응합니다.표준편차($\\sigma$): 더하기는 무시하고, 곱하기는 **절댓값($|a|$)**으로 반응합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3316ef1",
   "metadata": {},
   "source": [
    "# 36. 정규분포의 설명으로 옳지 않은 것은?\n",
    "① 왜도가 3, 첨도가 0이다.② 직선 $x = \\mu$(평균)에 대하여 대칭인 종 모양의 곡선이다.③ 곡선과 $x$축으로 둘러싸인 영역의 넓이는 1이다(확률의 총합은 100%이다).④ 정규분포의 모양은 평균이 동일할 때 중심축도 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87ce40",
   "metadata": {},
   "source": [
    "정답은 ① 왜도가 3, 첨도가 0이다. 입니다.이 문제는 통계학에서 가장 중요한 **정규분포(Normal Distribution)**의 기하학적, 통계적 특성을 정확히 알고 있는지 묻고 있습니다.💡 각 선택지 상세 해설① 왜도와 첨도 (오답): * 정규분포는 좌우가 완벽하게 대칭인 분포이므로 왜도(Skewness)는 0입니다.정규분포의 첨도(Kurtosis)는 3입니다. (단, 분석 소프트웨어에 따라 정규분포의 첨도를 0으로 맞추기 위해 3을 빼서 계산하는 '초과 첨도'를 사용하기도 하지만, 기본 정의상 왜도는 0입니다.) 1번 지문은 왜도와 첨도의 수치가 바뀌어 설명되었습니다.② 대칭성 (정답 설명): 정규분포는 평균($\\mu$)을 중심으로 좌우가 정확히 대칭인 **종 모양(Bell-shape)**의 곡선입니다. 이 때문에 평균, 중앙값, 최빈값이 모두 일치합니다.③ 확률의 총합 (정답 설명): 확률밀도함수의 곡선 아래 전체 면적은 해당 확률변수가 가질 수 있는 모든 경우의 수의 합이므로 항상 **1(100%)**이 됩니다.④ 중심축과 모양 (정답 설명): 정규분포의 위치(중심축)는 **평균($\\mu$)**에 의해 결정되고, 그래프의 높낮이와 넓은 정도는 **표준편차($\\sigma$)**에 의해 결정됩니다. 따라서 평균이 같다면 그래프의 중심축 위치는 동일합니다.📊 정규분포의 주요 요약특성내용모양평균($\\mu$) 기준 좌우 대칭인 종 모양매개변수평균($\\mu$)과 표준편차($\\sigma$)왜도 (Skewness)0 (완전 대칭)첨도 (Kurtosis)3 (기준값)68-95-99.7 법칙$\\pm 1\\sigma$ 내 68.2%, $\\pm 2\\sigma$ 내 95.4%, $\\pm 3\\sigma$ 내 99.7% 데이터 존재💡 추가 팁: 표준정규분포평균이 0이고 표준편차가 1인 정규분포($N(0, 1)$)를 특별히 표준정규분포라고 부르며, 서로 다른 두 집단을 비교할 때 데이터를 이 분포로 변환(표준화)하여 분석합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16acf0a",
   "metadata": {},
   "source": [
    "# 37. 포아송분포가 맞는지 적합도 검정을 한다. 보기 중 맞는 설명을 고르시오.\n",
    "(지문 가, 나, 다 생략 - 이미지 하단 박스 영역)\n",
    "\n",
    "① 가, 나 ② 가, 다 ③ 나, 다 ④ 가, 나, 다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d1024",
   "metadata": {},
   "source": [
    "지문 이미지가 제공되지 않았지만, 일반적으로 포아송 분포(Poisson Distribution)의 적합도 검정(Goodness of Fit Test) 문제에서 다루는 핵심 이론들을 바탕으로 가장 적절한 설명을 구성해 드립니다.보통 이러한 문제에서 제시되는 가, 나, 다의 내용은 다음과 같습니다.[예상 지문 내용]가: 관측값이 특정 확률분포(포아송 분포)를 따르는지 검정하기 위해 카이제곱($\\chi^2$) 검정 통계량을 사용한다.나: 귀무가설($H_0$)은 \"데이터가 포아송 분포를 따른다\"로 설정한다.다: 자유도는 \"범주의 개수($k$) - 1 - (추정된 모수의 개수)\"로 계산한다.💡 상세 해설 및 정답 추론정답은 ④ 가, 나, 다일 가능성이 가장 높습니다.검정 방법 (가): 적합도 검정은 실제 관측된 빈도와 포아송 분포에서 기대되는 빈도의 차이를 측정하는 카이제곱 적합도 검정을 수행합니다.가설 설정 (나): 모든 적합도 검정에서 **귀무가설($H_0$)은 \"현재 데이터는 해당 분포와 일치한다(따른다)\"**입니다. 대립가설($H_1$)은 \"따르지 않는다\"가 됩니다.자유도 계산 (다): 카이제곱 적합도 검정의 자유도는 기본적으로 $k-1$입니다. 하지만 포아송 분포의 모수($\\lambda$, 평균)를 모르고 표본으로부터 추정했다면, 자유도는 **$k - 1 - 1$**이 됩니다. (포아송 분포는 모수가 1개이므로)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4fc795",
   "metadata": {},
   "source": [
    "# 38. 표준화와 표준정규분포에 관한 설명으로 적절한 것은?\n",
    "① 표준화는 각 요소에서 평균을 뺀 값을 분산으로 나눈다. ② 표준화의 최대값은 1이다. ③ 표준화의 표준편차는 0이다. ④ 정규분포를 표준화하면 표준정규분포가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d9a2b",
   "metadata": {},
   "source": [
    "정답은 ④ 정규분포를 표준화하면 표준정규분포가 된다. 입니다.이 문제는 데이터의 척도를 맞추는 **표준화(Standardization)**의 정의와 그 결과로 생성되는 표준정규분포의 성질을 묻고 있습니다.💡 각 선택지 상세 해설④ 정규분포의 표준화 (정답): 어떤 정규분포 $N(\\mu, \\sigma^2)$를 따르는 확률변수 $X$라도 표준화 공식($Z = \\frac{X-\\mu}{\\sigma}$)을 거치면 평균이 0, 표준편차가 1인 **표준정규분포 $N(0, 1)$**로 변환됩니다.① 표준화 공식의 오류 (오답): 표준화는 각 요소에서 평균을 뺀 값을 분산이 아니라 표준편차로 나누어야 합니다. (분산으로 나누면 단위가 맞지 않습니다.)$$Z = \\frac{X - \\mu}{\\sigma}$$② 표준화의 범위 (오답): 표준화된 값($Z$값)은 이론적으로 $-\\infty$에서 $+\\infty$ 사이의 모든 값을 가질 수 있습니다. 최댓값이 1로 제한되는 것은 0~1 사이로 압축하는 'Min-Max 정규화'와 혼동한 설명입니다.③ 표준화의 통계량 (오답): 표준화를 거친 데이터의 평균은 항상 0이며, 표준편차는 항상 1이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd069a",
   "metadata": {},
   "source": [
    "# 39. 초기하분포의 설명으로 적절하지 않은 것은?\n",
    "① 확률변수값으로서 일정횟수의 베르누이 시행에서 성공횟수를 가진다. ② 성공확률은 일정하지 않다. ③ 각 시행은 독립적이다. ④ 이산확률분포를 따른다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b58b79d",
   "metadata": {},
   "source": [
    "정답은 ③ 각 시행은 독립적이다. 입니다.이 문제는 **초기하분포(Hypergeometric Distribution)**와 **이항분포(Binomial Distribution)**의 핵심 차이점인 '독립성' 여부를 묻고 있습니다.💡 각 선택지 상세 해설③ 각 시행의 독립성 (오답): 초기하분포는 **비복원 추출(Sampling without replacement)**을 전제로 합니다. 즉, 앞에서 뽑힌 데이터가 다음 추출에 영향을 주므로 각 시행은 종속적입니다. 반면, 뽑은 것을 다시 넣고 뽑는 '이항분포'는 각 시행이 독립적입니다.① 베르누이 시행과 성공 횟수 (적절): 초기하분포는 '성공' 혹은 '실패'라는 두 가지 결과만 가지는 베르누이 시행을 기반으로 하며, 특정 횟수의 추출 중 성공한 횟수를 확률변수 $X$로 가집니다.② 성공 확률의 가변성 (적절): 비복원 추출이기 때문에 시행을 반복할 때마다 주머니 속의 전체 개수와 성공 개수가 변하게 되어, 매 시행 시 성공 확률이 계속해서 변합니다.④ 이산확률분포 (적절): 성공 횟수는 0회, 1회, 2회처럼 셀 수 있는 정수 값으로 나타나므로 대표적인 이산확률분포에 해당합니다.📊 이항분포 vs 초기하분포 비교구분이항분포 (Binomial)초기하분포 (Hypergeometric)추출 방식복원 추출 (다시 넣음)비복원 추출 (다시 안 넣음)시행의 관계독립 시행종속 시행성공 확률 ($p$)매회 일정함매회 변함모수시행 횟수($n$), 성공 확률($p$)모집단 크기($N$), 성공 수($M$), 추출 수($n$)💡 추가 팁: 초기하분포가 이항분포가 되는 경우만약 모집단의 크기($N$)가 추출하는 횟수($n$)에 비해 압도적으로 크다면(보통 $N > 10n$), 비복원 추출이라 하더라도 확률 변화가 미미하기 때문에 이항분포로 근사하여 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab17653",
   "metadata": {},
   "source": [
    "# 40. 다음 사례의 귀무가설 검정으로 옳은 것은?\n",
    "사람의 평균수명을 알아보기 위해 사망자 100명을 표본으로 추출하여 조사하였더니 평균 72.4년으로 나타났다. 모표준편차를 12년으로 가정할 때, 현재의 평균수명은 70년보다 길다고 할 수 있는가를 검정하라(유의수준 $\\alpha=0.05$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1302",
   "metadata": {},
   "source": [
    "① 표준정규확률변수 $z=2$, 귀무가설 채택\n",
    "② 표준정규확률변수 $z=2$, 귀무가설 기각\n",
    "③ 표준정규확률변수 $z=3$, 귀무가설 채택\n",
    "④ 표준정규확률변수 $z=3$, 귀무가설 기각"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10dd10",
   "metadata": {},
   "source": [
    "정답은 ② 표준정규확률변수 $z=2$, 귀무가설 기각입니다.이 문제는 가설 검정의 절차를 이해하고, 검정 통계량을 계산하여 결론(채택/기각)을 내릴 수 있는지 묻는 종합적인 문제입니다.💡 단계별 풀이 과정1. 가설 설정문제에서 \"평균수명이 70년보다 길다고 할 수 있는가\"를 묻고 있으므로 **단측 검정(우측 검정)**을 수행합니다.귀무가설($H_0$): $\\mu = 70$ (평균수명은 70년이다.)대립가설($H_1$): $\\mu > 70$ (평균수명은 70년보다 길다.)2. 검정 통계량 ($z$) 계산모표준편차($\\sigma$)를 알고 있고 표본의 크기($n$)가 충분히 크므로($n \\ge 30$), 표준정규분포를 이용한 Z-검정을 실시합니다.$$z = \\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}}$$표본평균 $\\bar{x} = 72.4$귀무가설의 모평균 $\\mu = 70$모표준편차 $\\sigma = 12$표본 크기 $n = 100 \\rightarrow \\sqrt{n} = 10$$$z = \\frac{72.4 - 70}{12 / 10} = \\frac{2.4}{1.2} = \\mathbf{2}$$3. 임계값 비교 및 결론유의수준($\\alpha$): 0.05임계값 ($z_{0.05}$): 표준정규분포표에서 우측 단측 검정 시 $\\alpha=0.05$에 해당하는 $z$값은 1.645입니다.판정: 계산된 검정 통계량($z=2$)이 임계값(1.645)보다 큽니다. ($2 > 1.645$)따라서 검정 통계량이 **기각역(Rejection Region)**에 속하므로, 귀무가설($H_0$)을 기각합니다. 즉, 유의수준 5%에서 사람의 평균수명은 70년보다 길다고 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f47195",
   "metadata": {},
   "source": [
    "# 41. 범주형 자료의 분석방법이 아닌 것은?\n",
    "① 인공신경망 ② 선형회귀분석 ③ 서포트벡터머신 ④ 의사결정나무"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d492e",
   "metadata": {},
   "source": [
    "정답은 ② 선형회귀분석입니다.이 문제는 분석 대상 데이터의 형태(범주형 vs 연속형)에 따른 적절한 알고리즘을 구분할 수 있는지 묻고 있습니다.💡 각 선택지 상세 해설② 선형회귀분석 (오답): 선형회귀분석은 독립변수와 종속변수 간의 선형적 상관관계를 모델링하여 연속형 수치를 예측하는 알고리즘입니다. 범주형(A, B, C 또는 합격/불합격 등) 자료를 직접적으로 예측하는 데는 적합하지 않습니다.① 인공신경망 (정답 설명): 활성화 함수(Softmax 등)를 사용하여 데이터를 여러 범주로 분류하는 분류(Classification) 문제에 매우 널리 사용됩니다.③ 서포트벡터머신 (정답 설명): 데이터 간의 경계(결정 경계)를 찾아 범주를 구분하는 대표적인 분류 알고리즘입니다. (회귀용 SVR도 있으나 일반적으로 분류에 많이 쓰입니다.)④ 의사결정나무 (정답 설명): 나무 구조를 통해 데이터를 하위 범주로 계속해서 나누어가는 방식으로, **분류(Classification Tree)**와 회귀 모두에 사용될 수 있는 강력한 기법입니다.📊 데이터 형태에 따른 분석 방법 요약구분분류 (Classification)회귀 (Regression)대상 데이터범주형 (Discrete)연속형 (Continuous)결과 예시스팸 여부, 등급, 생존 여부주가 예측, 온도 변화, 매출액주요 기법로지스틱 회귀, SVM, KNN, 의사결정나무, 랜덤포레스트, 신경망 등선형회귀, 다항회귀, 의사결정나무(회귀), 신경망 등✍️ 핵심 포인트: 로지스틱 회귀(Logistic Regression)이름에 '회귀'가 들어가서 헷갈리기 쉽지만, 로지스틱 회귀는 대표적인 범주형(이진 분류) 분석 방법입니다. 시험에 단골로 나오니 선형회귀와 꼭 구분해서 기억하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03b5eaa",
   "metadata": {},
   "source": [
    "# 42. 비지도학습 알고리즘 유형으로 알맞은 것은?\n",
    "① 회귀분석 ② 로지스틱회귀분석 ③ 서포트벡터머신 ④ 군집분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a529712",
   "metadata": {},
   "source": [
    "정답은 **④ 군집분석(Clustering)**입니다.\n",
    "\n",
    "이 문제는 머신러닝의 학습 방식에 따른 분류를 묻고 있습니다. 머신러닝은 크게 정답(Label)의 유무에 따라 지도학습과 비지도학습으로 나뉩니다.\n",
    "\n",
    "💡 각 선택지 상세 해설\n",
    "④ 군집분석 (비지도학습): 정답(Label)이 없는 상태에서 데이터 간의 유사성을 측정하여 비슷한 특성을 가진 데이터끼리 그룹화하는 대표적인 비지도학습 알고리즘입니다. (예: K-평균 군집화, 계층적 군집화)\n",
    "\n",
    "① 회귀분석 (지도학습): 입력값(X)에 대응하는 연속적인 수치 정답(Y)을 맞히는 학습 방식입니다.\n",
    "\n",
    "② 로지스틱회귀분석 (지도학습): 범주형 정답(예: 스팸이다/아니다)을 분류하기 위해 확률을 예측하는 지도학습 알고리즘입니다.\n",
    "\n",
    "③ 서포트벡터머신 (지도학습): 데이터를 분류하거나 회귀하기 위해 최적의 결정 경계(Hyperplane)를 찾는 지도학습 알고리즘입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce36d6",
   "metadata": {},
   "source": [
    "# 43. 하이퍼파라미터의 최적화 기법으로 옳지 않은 것은?\n",
    "① 무작위탐색(Random Search) ② 격자탐색(Grid Search) ③ 베이지안 최적화(Bayesian Optimization) ④ 경사하강법(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b917abd",
   "metadata": {},
   "source": [
    "정답은 **④ 경사하강법(Gradient Descent)**입니다.이 문제는 모델의 내부 가중치를 학습시키는 파라미터(Parameter) 최적화와 사용자가 직접 설정하는 하이퍼파라미터(Hyperparameter) 최적화의 차이를 묻고 있습니다.💡 각 선택지 상세 해설④ 경사하강법 (오답): 경사하강법은 모델의 손실 함수(Loss Function)를 최소화하기 위해 **가중치(Weight)와 편향(Bias)**이라는 '파라미터'를 자동으로 업데이트하는 알고리즘입니다. 즉, 모델 내부에서 학습을 통해 결정되는 수치를 찾는 기법입니다.① 무작위 탐색 (정답 설명): 하이퍼파라미터의 범위를 정해놓고 그 안에서 무작위로 조합을 골라 테스트하는 기법입니다. 격자 탐색보다 속도가 빠르고 효율적일 때가 많습니다.② 격자 탐색 (정답 설명): 가능한 하이퍼파라미터 후보들을 미리 격자(Grid) 형태로 정해놓고, 모든 조합을 하나씩 차례대로 검증하여 가장 좋은 성능을 내는 조합을 찾는 기법입니다.③ 베이지안 최적화 (정답 설명): 이전 테스트 결과를 바탕으로 다음 후보를 전략적으로 선정하는 확률 모델 기반 기법입니다. 무작위나 격자 탐색보다 더 적은 횟수로 최적의 값을 찾을 수 있습니다.📊 파라미터 vs 하이퍼파라미터 비교구분파라미터 (Parameter)하이퍼파라미터 (Hyperparameter)정의모델 내부에서 학습으로 결정되는 값모델 학습 전 사용자가 직접 설정하는 값예시가중치(W), 편향(b)학습률(Learning Rate), K-NN의 K값, 은닉층 개수최적화 기법경사하강법(Gradient Descent)Grid Search, Random Search, Bayesian💡 추가 팁: 최적화 기법의 흐름최근에는 사람이 일일이 하이퍼파라미터를 조정하지 않고, 알고리즘이 스스로 최적의 모델 구조와 하이퍼파라미터를 찾는 AutoML 기술이 활발하게 연구되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9016df45",
   "metadata": {},
   "source": [
    "# 44. 선형회귀분석의 오차항의 특성이 아닌 것은?\n",
    "① 선형성 ② 독립성 ③ 정규성 ④ 등분산성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3febca9",
   "metadata": {},
   "source": [
    "정답은 ① 선형성입니다.이 문제는 선형회귀 모델이 통계적으로 유효하고 신뢰할 수 있는 결과를 내기 위해 반드시 전제되어야 하는 오차항(Error term)의 기본 가정을 묻고 있습니다.💡 상세 해설선형회귀분석의 4대 기본 가정을 흔히 **'선독등정'**이라고 외우기도 합니다. 하지만 이 중 '선형성'은 오차항의 특성이 아니라 모델 구조에 대한 가정입니다.① 선형성 (Linearity): 독립변수($X$)와 종속변수($Y$)의 관계가 직선 형태(선형 결합)여야 한다는 가정입니다. 즉, $Y = \\beta_0 + \\beta_1X + \\epsilon$의 형태를 만족해야 함을 의미하며, 이는 변수 간의 관계에 대한 설명입니다.② 독립성 (Independence): 오차항들끼리는 서로 상관관계가 없어야 합니다. 특히 시계열 데이터에서 이전 시점의 오차가 다음 시점의 오차에 영향을 주는 '자기상관'이 없어야 함을 의미합니다. (더빈-왓슨 검정으로 확인)③ 정규성 (Normality): 오차항의 분포가 평균이 0인 정규분포를 따라야 합니다. 이 가정이 깨지면 모델의 계수에 대한 신뢰구간이나 가설 검정(p-value)의 결과를 믿기 어려워집니다. (Q-Q Plot으로 확인)④ 등분산성 (Homoscedasticity): 독립변수의 모든 값에 대하여 오차항의 분산이 일정해야 합니다. 데이터의 크기에 따라 오차의 변동폭이 커지거나 작아지지 않고 일정해야 함을 뜻합니다. (잔차 산점도로 확인)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7973e",
   "metadata": {},
   "source": [
    "# 45. 아래의 수식이 나타내는 회귀분석은?\n",
    "$$MSE(\\theta) + \\alpha \\sum_{i=1}^{p} \\theta_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a276eaae",
   "metadata": {},
   "source": [
    "① 라쏘회귀② 릿지회귀③ 엘라스틱넷④ 단순회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd6718",
   "metadata": {},
   "source": [
    "정답은 **② 릿지회귀(Ridge Regression)**입니다.이 문제는 회귀 모델의 과적합(Overfitting)을 방지하기 위해 사용되는 규제(Regularization) 기법의 수식을 구분할 수 있는지 묻고 있습니다.💡 상세 해설제시된 수식 $MSE(\\theta) + \\alpha \\sum_{i=1}^{p} \\theta_i^2$는 기존의 오차 제곱합(MSE)에 가중치들의 제곱 합을 더한 형태입니다.② 릿지회귀 (Ridge): 가중치(회귀 계수)의 **제곱 합(L2 규제)**을 손실 함수에 더합니다. 가중치 값을 완전히 0으로 만들지는 않지만, 전체적으로 작게 만들어 모델의 복잡도를 줄이고 분산을 감소시킵니다.① 라쏘회귀 (Lasso): 가중치 **절댓값의 합(L1 규제)**을 더합니다. 수식으로는 $\\alpha \\sum |\\theta_i|$가 됩니다. 중요한 특징은 불필요한 변수의 가중치를 완전히 0으로 만들어 변수 선택(Feature Selection) 효과가 있다는 점입니다.③ 엘라스틱넷 (Elastic Net): L1 규제와 L2 규제를 결합한 형태입니다. 두 가지 규제의 장점을 모두 취하며, 수식에는 절댓값 항과 제곱 항이 모두 포함됩니다.④ 단순회귀 (Simple Linear Regression): 규제 항이 없는 가장 기본적인 회귀 모델로, 오직 $MSE$만을 최소화하는 방향으로 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfecb351",
   "metadata": {},
   "source": [
    "# 46. 로지스틱회귀분석에 대한 설명으로 잘못된 것은?\n",
    "① 분류에 주로 사용한다.② 종속변수가 범주형을 갖는 경우 사용하는 분석기법이다.③ $Y$값은 0과 1 사이이다.④ 대표적인 비지도학습 알고리즘이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753c4f8",
   "metadata": {},
   "source": [
    "정답은 ④ 대표적인 비지도학습 알고리즘이다. 입니다.로지스틱 회귀분석(Logistic Regression)은 데이터 분석 시험에서 매우 자주 출제되는 주제입니다.💡 각 선택지 상세 해설④ 지도학습 vs 비지도학습 (오답): 로지스틱 회귀분석은 입력 데이터($X$)와 그에 따른 정답 라벨($Y$, 예: 성공/실패, 스팸 여부)이 있는 상태에서 모델을 학습시키는 대표적인 지도학습(Supervised Learning) 알고리즘입니다. 비지도학습은 정답이 없는 상태에서 군집화 등을 수행하는 기법입니다.① 분류 분석 (정답 설명): 이름에는 '회귀'가 들어가지만, 실제로는 특정 범주에 속할 확률을 계산하여 데이터를 나누는 분류(Classification) 목적으로 주로 사용됩니다.② 범주형 종속변수 (정답 설명): 선형 회귀와 달리 종속변수($Y$)가 '예/아니오', '합격/불합격'과 같이 **범주형(이진형)**인 경우에 적합한 분석 기법입니다.③ $Y$값의 범위 (정답 설명): 로지스틱 함수(시그모이드 함수)를 사용하기 때문에 출력되는 결과값(확률)은 항상 0과 1 사이의 값을 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f53a6a",
   "metadata": {},
   "source": [
    "# 47. 회귀분석 $log(odds) = b + ax$ 설명으로 가장 거리가 먼 것은?\n",
    "① $a, b$ 둘 다 0이면 $y$ 확률은 0이다.② Log 연산을 통해 0에서 1사이의 Logit을 획득한다.③ 오즈(Odds)는 클래스 0에 속하는 확률에 대한 클래스 1에 속하는 확률의 비이다.④ 승산비(Odd Ratio) 사건이 발생할 확률과 발생하지 않을 확률 간의 비율이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9479201",
   "metadata": {},
   "source": [
    "정답은 ① $a, b$ 둘 다 0이면 $y$ 확률은 0이다. 입니다.이 문제는 로지스틱 회귀분석의 핵심인 로짓(Logit) 변환과 **오즈(Odds)**의 수학적 성질을 정확히 이해하고 있는지 묻고 있습니다.💡 각 선택지 상세 해설① $a, b$가 0일 때의 확률 (오답):수식 $\\log(\\text{odds}) = b + ax$에서 $a=0, b=0$을 대입하면 $\\log(\\text{odds}) = 0$이 됩니다.로그 값이 0이 되려면 진수인 오즈($\\text{odds}$)는 1이어야 합니다 ($\\text{odds} = e^0 = 1$).오즈가 1이라는 것은 $\\frac{p}{1-p} = 1$이므로, 성공 확률 **$p = 0.5$**가 됨을 의미합니다. 따라서 확률이 0이라는 설명은 틀렸습니다.② 로짓(Logit) 획득 (정답 설명):확률 $p$는 0과 1 사이의 값을 가지는데, 이를 오즈비로 바꾸고 로그를 취하는 과정을 '로짓 변환'이라고 합니다. 이 과정을 통해 비선형적인 확률 결정을 선형 회귀 형태로 분석할 수 있게 됩니다.③ 오즈(Odds)의 정의 (정답 설명):오즈는 특정 사건이 발생할 확률($p$)을 발생하지 않을 확률($1-p$)로 나눈 값입니다. 즉, $\\text{Odds} = \\frac{P(\\text{Class 1})}{P(\\text{Class 0})}$ 입니다.④ 승산비(Odds Ratio) (정답 설명):승산비는 보통 독립변수 $x$가 1단위 증가할 때 오즈가 몇 배 변하는지를 나타내는 비율입니다. 본문 설명처럼 사건 발생 확률과 발생하지 않을 확률 간의 비율(오즈 자체)을 의미하거나, 두 집단의 오즈 비교를 위해 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7a671",
   "metadata": {},
   "source": [
    "# 48. 의사결정나무에 대한 설명 중 틀린 것은?\n",
    "① 가지에 하나가 남는 끝까지 진행한다. ② 불순도란 복잡성을 의미하며, 해당 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지 뜻한다. ③ 분류(Classification)와 회귀(Regression)에서 모두 사용할 수 있다. ④ 지도학습으로 알려져 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde921c6",
   "metadata": {},
   "source": [
    "정답은 ① 가지에 하나가 남는 끝까지 진행한다. 입니다.이 문제는 **의사결정나무(Decision Tree)**의 구조와 학습 과정에서 발생하는 과적합(Overfitting) 방지 전략을 이해하고 있는지 묻고 있습니다.💡 각 선택지 상세 해설① 끝까지 진행 여부 (오답): 의사결정나무를 가지에 데이터가 하나만 남을 때까지(완벽하게 순수해질 때까지) 계속 키우게 되면, 훈련 데이터에 너무 과하게 최적화되는 과적합 문제가 발생합니다. 따라서 실무에서는 적절한 수준에서 성장을 멈추는 **정지 규칙(Stopping Rule)**을 설정하거나, 일단 다 키운 뒤 불필요한 가지를 쳐내는 **가지치기(Pruning)**를 수행합니다.② 불순도(Impurity)의 정의 (정답 설명): 불순도는 해당 노드 안에 서로 다른 범주들이 얼마나 섞여 있는지를 수치화한 것입니다. **지니 지수(Gini Index)**나 **엔트로피(Entropy)**가 낮을수록 해당 노드의 순도가 높고 정보 전달력이 좋다고 판단합니다.③ 분류와 회귀 모두 사용 (정답 설명): 의사결정나무는 종속변수의 형태에 따라 **분류 나무(Classification Tree)**와 **회귀 나무(Regression Tree)**로 나뉩니다. 분류일 때는 다수결로, 회귀일 때는 평균값으로 예측값을 결정합니다.④ 지도학습 (정답 설명): 입력 데이터와 정답(라벨)을 함께 학습시켜 규칙을 찾아내는 전형적인 지도학습(Supervised Learning) 알고리즘입니다.📊 의사결정나무의 주요 구성 요소용어설명뿌리 노드 (Root Node)전체 데이터가 시작되는 맨 위의 노드중간 노드 (Internal Node)의사결정 규칙(분리 기준)이 포함된 노드끝 노드 (Terminal/Leaf Node)더 이상 분리되지 않고 최종 결과가 담긴 노드가지치기 (Pruning)과적합 방지를 위해 불필요한 마디를 제거하는 과정💡 추가 팁: 의사결정나무의 장점의사결정나무는 '왜 이런 결과가 나왔는지' 사람이 눈으로 보고 이해할 수 있는 **해석력(Explainability)**이 매우 뛰어난 모델입니다. 그래서 의료 진단이나 신용 평가 등 설명이 중요한 분야에서 많이 쓰입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594737b",
   "metadata": {},
   "source": [
    "# 49. 인공지능에 대한 설명으로 가장 거리가 먼 것은?\n",
    "① 모델 예측값과 실제값의 오차인 손실 함수(Loss Function, 비용 함수)는 인공지능 학습에서, 최적화된 비용에 관련된 모든 변량에 대하여 어떤 관계를 나타내는 함수이다. ② 일반적으로 여러 개의 은닉층을 가진 신경망을 통해 데이터를 학습하는 것을 딥러닝이라 한다. ③ 딥러닝은 인공신경망으로 발전했다. ④ 인공지능의 기울기 소멸 문제로 인해 암흑기가 발생한 적이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ce9c3",
   "metadata": {},
   "source": [
    "정답은 ③ 딥러닝은 인공신경망으로 발전했다. 입니다.이 문제는 인공지능(AI), 머신러닝(ML), 딥러닝(DL)의 역사적 선후 관계와 포함 관계를 정확히 이해하고 있는지 묻고 있습니다.💡 각 선택지 상세 해설③ 발전 방향의 오류 (오답): 인공신경망(ANN)은 1950년대부터 시작된 초기 인공지능 모델 중 하나입니다. 이 인공신경망을 바탕으로 컴퓨터 성능의 발전과 알고리즘 개선을 통해 층을 깊게 쌓은 것이 딥러닝입니다. 즉, **'인공신경망이 딥러닝으로 발전했다'**가 올바른 설명입니다.① 손실 함수와 최적화 (정답 설명): 손실 함수는 모델의 예측값($\\hat{y}$)과 실제 정답($y$) 사이의 차이를 수치화한 것입니다. 인공지능 학습의 핵심은 이 손실 함수의 값을 최소화하는 가중치($w$)를 찾는 최적화(Optimization) 과정입니다.② 딥러닝의 정의 (정답 설명): 전통적인 신경망보다 더 많은 **은닉층(Hidden Layer)**을 가진 다층 신경망 구조를 통해 대량의 데이터를 학습시키는 기법을 딥러닝(심층 학습)이라고 정의합니다.④ 기울기 소멸 문제와 암흑기 (정답 설명): 신경망의 층이 깊어질수록 역전파 과정에서 오차가 앞쪽 층으로 전달되지 않는 기울기 소멸(Vanishing Gradient) 문제가 발생했습니다. 이로 인해 한동안 인공신경망 연구가 정체되며 인공지능의 암흑기(Second AI Winter)를 겪기도 했습니다.📊 인공지능 역사 및 개념 요약구분설명인공지능 (AI)인간의 지능을 기계로 구현하려는 포괄적 범주머신러닝 (ML)데이터를 통해 스스로 학습하고 성능을 개선하는 알고리즘 (AI의 하위 분야)인공신경망 (ANN)생물학적 뇌의 뉴런 구조를 모방한 모델딥러닝 (DL)다층 신경망을 사용하여 복잡한 패턴을 학습하는 ML의 하위 분야💡 추가 팁: 암흑기를 끝낸 핵심 요소들기울기 소멸 문제는 ReLU 활성화 함수, 가중치 초기화 기법, Dropout 등 다양한 기술적 해결책이 나오면서 극복되었고, 이후 GPU의 연산 능력 발전과 빅데이터의 폭증이 더해져 지금의 딥러닝 전성시대가 열렸습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77655da0",
   "metadata": {},
   "source": [
    "# 50. 인공신경망의 단층퍼셉트론 문제로 표현이 불가능한 논리회로는?\n",
    "① AND ② OR ③ NOR ④ XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de711b2b",
   "metadata": {},
   "source": [
    "정답은 ④ XOR입니다.\n",
    "\n",
    "이 문제는 인공신경망 역사의 아주 중요한 분기점인 단층 퍼셉트론(Single-Layer Perceptron)의 한계를 묻고 있습니다.\n",
    "\n",
    "💡 상세 해설\n",
    "④ XOR (오답 - 표현 불가능): XOR(배타적 논리합) 회구는 입력값 두 개가 서로 다를 때만 1을 출력합니다. 이를 평면에 점으로 찍어보면, 하나의 직선(선형 결정 경계)으로는 0과 1의 영역을 완벽하게 분리할 수 없는 비선형 모델입니다. 이 문제 때문에 인공지능 연구의 첫 번째 암흑기가 오기도 했습니다.\n",
    "\n",
    "①, ②, ③ AND, OR, NOR (정답 설명): 이 논리 회로들은 데이터가 평면상에서 하나의 직선으로 충분히 분류가 가능한 선형 분리 가능(Linearly Separable) 데이터입니다. 따라서 단층 퍼셉트론으로 구현이 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07f51f8",
   "metadata": {},
   "source": [
    "# 51. 다음 중 연관분석 기법으로 알맞은 것은?\n",
    "① 회귀분석\n",
    "\n",
    "② 선험적 규칙(Apriori)\n",
    "\n",
    "③ 군집분석\n",
    "\n",
    "④ 윌콕슨 순위합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3ffd5",
   "metadata": {},
   "source": [
    "정답은 **② 선험적 규칙(Apriori)**입니다.\n",
    "\n",
    "이 문제는 데이터 간의 '함께 발생하는' 패턴을 찾아내는 **연관성 분석(Association Analysis)**의 대표적인 알고리즘을 묻고 있습니다.\n",
    "\n",
    "💡 각 선택지 상세 해설\n",
    "② 선험적 규칙 (Apriori) (정답): \"기저귀를 사는 사람은 맥주를 함께 산다\"와 같은 장바구니 분석으로 유명한 알고리즘입니다. 데이터들 간의 **연관 규칙(Association Rule)**을 찾기 위해 빈번하게 발생하는 아이템 세트를 먼저 식별하는 방식을 사용합니다.\n",
    "\n",
    "① 회귀분석 (지도학습): 원인(독립변수)과 결과(종속변수) 사이의 상관관계를 수치로 예측하는 기법입니다.\n",
    "\n",
    "③ 군집분석 (비지도학습): 데이터들 간의 유사성을 측정하여 비슷한 성격의 데이터끼리 그룹화(Clustering)하는 기법입니다.\n",
    "\n",
    "④ 윌콕슨 순위합 (비모수 검정): 두 집단의 중앙값이 차이가 있는지 검정할 때 사용하는 비모수 통계 방법입니다. 정규분포를 따르지 않는 데이터에 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee859147",
   "metadata": {},
   "source": [
    "# 52. 다음 중 비지도학습에 대한 설명으로 알맞은 것은?\n",
    "① 정답을 가르쳐주지 않고, 회귀분석\n",
    "\n",
    "② 정답을 가르쳐주고, 회귀분석\n",
    "\n",
    "③ 정답을 가르쳐주지 않고, 군집분석\n",
    "\n",
    "④ 정답을 가르쳐주고, 군집분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792f932",
   "metadata": {},
   "source": [
    "정답은 **② 선험적 규칙(Apriori)**입니다.\n",
    "\n",
    "이 문제는 데이터 간의 '함께 발생하는' 패턴을 찾아내는 **연관성 분석(Association Analysis)**의 대표적인 알고리즘을 묻고 있습니다.\n",
    "\n",
    "💡 각 선택지 상세 해설\n",
    "② 선험적 규칙 (Apriori) (정답): \"기저귀를 사는 사람은 맥주를 함께 산다\"와 같은 장바구니 분석으로 유명한 알고리즘입니다. 데이터들 간의 **연관 규칙(Association Rule)**을 찾기 위해 빈번하게 발생하는 아이템 세트를 먼저 식별하는 방식을 사용합니다.\n",
    "\n",
    "① 회귀분석 (지도학습): 원인(독립변수)과 결과(종속변수) 사이의 상관관계를 수치로 예측하는 기법입니다.\n",
    "\n",
    "③ 군집분석 (비지도학습): 데이터들 간의 유사성을 측정하여 비슷한 성격의 데이터끼리 그룹화(Clustering)하는 기법입니다.\n",
    "\n",
    "④ 윌콕슨 순위합 (비모수 검정): 두 집단의 중앙값이 차이가 있는지 검정할 때 사용하는 비모수 통계 방법입니다. 정규분포를 따르지 않는 데이터에 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8dd3d7",
   "metadata": {},
   "source": [
    "# 53. 맨해튼 거리를 계산하시오. 점 A에서 2번째로 가까운 점의 거리는?\n",
    "(좌표 보기: $A(1,1), B(1,2), C(2,2), D(4,1)$ / 그래프 포함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c75f5a9",
   "metadata": {},
   "source": [
    "① 1② 2③ 3④ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c0249",
   "metadata": {},
   "source": [
    "주어진 좌표들을 바탕으로 점 A(1, 1)로부터 다른 점들(B, C, D)까지의 맨해튼 거리(Manhattan Distance)를 계산하면 다음과 같습니다.맨해튼 거리 공식: $d(P, Q) = |x_1 - x_2| + |y_1 - y_2|$A(1, 1)와 B(1, 2) 사이의 거리:$|1 - 1| + |1 - 2| = 0 + 1 = \\mathbf{1}$A(1, 1)와 C(2, 2) 사이의 거리:$|1 - 2| + |1 - 2| = 1 + 1 = \\mathbf{2}$A(1, 1)와 D(4, 1) 사이의 거리:$|1 - 4| + |1 - 1| = 3 + 0 = \\mathbf{3}$거리를 가까운 순서대로 나열하면 **B(거리 1) < C(거리 2) < D(거리 3)**가 됩니다.따라서 점 A에서 2번째로 가까운 점인 C까지의 거리는 2입니다.정답: ② 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186200f5",
   "metadata": {},
   "source": [
    "# 54. 통계에서 평균에 대한 차이검정으로 모집단 3개 이상 시 사용하는 분석방법으로 가장 알맞은 것은?\n",
    "① t-검정\n",
    "\n",
    "② z-검정\n",
    "\n",
    "③ 분산분석\n",
    "\n",
    "④ 상관분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f5d5d",
   "metadata": {},
   "source": [
    "모집단이 3개 이상일 때 각 집단의 평균 차이를 검정하는 가장 적절한 분석 방법은 **③ 분산분석(ANOVA)**입니다.\n",
    "\n",
    "💡 왜 분산분석인가요?\n",
    "t-검정(t-test): 주로 2개의 집단 간 평균 차이를 비교할 때 사용합니다.\n",
    "\n",
    "z-검정(z-test): 모집단의 분산을 알고 있거나 표본의 크기가 클 때 2개 이하의 집단 평균을 비교합니다.\n",
    "\n",
    "분산분석(ANOVA): 3개 이상의 집단 간 평균 차이가 통계적으로 유의미한지 확인합니다. \"분산\"을 이용하여 집단 간의 차이가 실제 차이인지, 아니면 우연한 변동인지를 분석합니다.\n",
    "\n",
    "상관분석(Correlation): 두 변수 간의 **선형적 관계(연관성)**를 분석하는 방법으로, 평균의 차이를 검정하는 도구가 아닙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336ed78",
   "metadata": {},
   "source": [
    "# 55. 다음이 설명하는 시계열의 특성은 무엇인가?\n",
    "종/장기적, 빈번한 발생빈도가 없는 패턴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185946b",
   "metadata": {},
   "source": [
    "① 추세요인\n",
    "\n",
    "② 주기요인\n",
    "\n",
    "③ 계절요인\n",
    "\n",
    "④ 불규칙 요인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd002f2",
   "metadata": {},
   "source": [
    "주어진 설명이 가리키는 시계열의 특성은 **② 주기요인(Cyclical factor)**입니다.시계열 분석에서 데이터를 구성하는 네 가지 주요 요인은 다음과 같이 구분됩니다.시계열 구성 요인 비교요인특징발생 빈도 및 기간추세요인 (Trend)데이터가 장기적으로 상승하거나 하락하는 경향수년 이상의 장기적 변화주기요인 (Cyclical)장기적이며, 일정한 주기가 있지만 발생 빈도가 고정적이지 않은 패턴경제 순환(경기 변동) 등 2~10년 주기계절요인 (Seasonal)일, 주, 월, 분기 등 일정한 기간을 주기로 반복되는 패턴1년 이내의 일정한 빈도 (예: 냉면 매출)불규칙요인 (Irregular)예측할 수 없는 우발적인 원인에 의해 발생하는 변동돌발 상황 (예: 천재지변, 전쟁)💡 힌트 정리문제에서 **\"빈번한 발생빈도가 없는 패턴\"**이라는 표현은 계절요인처럼 매년 딱딱 맞춰 발생하는 것이 아니라, 경기 변동처럼 주기가 길고 불규칙하게(빈번하지 않게) 나타나는 주기성을 강조한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b57bb1d",
   "metadata": {},
   "source": [
    "# 56. 다음 그림에서 불 함수(Boolean Function)로 표현할 수 있는 나이브 베이지안 함수로 잘못된 것은?\n",
    "(그래프: $A, B$가 $C$를 가리키고, $C$가 $D, E$를 가리키는 베이지안 네트워크 구조)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d957b3",
   "metadata": {},
   "source": [
    "① $P(A,B,C) = P(A) \\times P(B|C)$② $P(A,B,C|E) = P(A|C) \\times P(B|C) \\times P(C|C)$③ $P(A,B|C) = P(A|C) \\times P(B|C)$④ $P(A,E|C) = P(A|C) \\times P(E|C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bec9ba",
   "metadata": {},
   "source": [
    "주어진 베이지안 네트워크의 구조는 다음과 같습니다:$A \\rightarrow C$ (A가 C의 부모)$B \\rightarrow C$ (B가 C의 부모)$C \\rightarrow D$ (C가 D의 부모)$C \\rightarrow E$ (C가 E의 부모)이 구조에서 핵심적인 특징은 **조건부 독립(Conditional Independence)**입니다. 특정 노드($C$)가 주어졌을 때, 그 부모들($A, B$)과 자식들($D, E$) 사이의 관계를 파악하는 것이 중요합니다.잘못된 설명은 ①번입니다.💡 상세 분석① $P(A, B, C) = P(A) \\times P(B|C)$ (오답)베이지안 네트워크의 전체 확률 법칙에 따르면 $P(A, B, C) = P(A) \\times P(B) \\times P(C|A, B)$가 되어야 합니다.$A$와 $B$는 서로 독립적인 부모 노드이므로 $P(A, B) = P(A)P(B)$이지만, $C$는 $A$와 $B$에 의존하므로 $P(B|C)$라는 표현은 구조와 맞지 않습니다.② $P(A, B, C|E) = P(A|C) \\times P(B|C) \\times P(C|C)$ (옳음 - 특수 케이스)$C$가 주어지면 $A, B, E$ 사이의 관계를 따지는 식입니다. 사실 $P(C|C)$는 1이므로 수식적으로는 다소 어색해 보일 수 있으나, $C$라는 조건이 주어졌을 때 $A$와 $B$가 조건부 독립이 된다는 논리 구조상에서는 성립 가능한 형태입니다.③ $P(A, B|C) = P(A|C) \\times P(B|C)$ (옳음)이것은 베이지안 네트워크의 **'V-구조(Collider)'**와 관련이 있습니다. 일반적으로 $A$와 $B$는 독립이지만, 그들의 자식인 $C$가 주어지면(결과를 알게 되면) $A$와 $B$는 서로 상관을 가지게 됩니다(Explaining away). 하지만 문제 맥락상 나이브 베이지안의 조건부 독립성 가정(결과 $C$가 주어졌을 때 원인 $A, B$는 독립이다)을 표현한 식으로 보아 옳다고 판단합니다.④ $P(A, E|C) = P(A|C) \\times P(E|C)$ (옳음)$C$는 $A$와 $E$ 사이의 경로를 막고 있는 노드입니다. 즉, $C$를 알고 있다면(조건부) $A$와 $E$는 서로 독립입니다. 전형적인 조건부 독립의 정의입니다.정답: ①가장 명백하게 확률의 기본 결합 법칙(Chain Rule)과 네트워크 구조를 무시한 식은 1번입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fc228c",
   "metadata": {},
   "source": [
    "# 57. 오토인코더에 대한 설명으로 가장 잘못된 것은?\n",
    "① 비지도학습이다.\n",
    "\n",
    "② 사전학습으로 사용된다.\n",
    "\n",
    "③ 입력 수는 은닉층 수보다 항상 작다.\n",
    "\n",
    "④ 인코드 입력 수와 디코드 출력 수는 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aed222",
   "metadata": {},
   "source": [
    "오토인코더(Autoencoder)의 구조와 원리를 고려할 때, 가장 잘못된 설명은 ③번입니다.\n",
    "\n",
    "💡 상세 해설\n",
    "① 비지도학습이다. (옳음) 별도의 레이블(정답) 없이 입력 데이터 자체를 타겟으로 삼아 학습하므로 비지도학습의 대표적인 모델입니다.\n",
    "\n",
    "② 사전학습으로 사용된다. (옳음) 딥러닝 초기에 가중치 초기화 문제를 해결하기 위해 층별로 오토인코더를 학습시키는 'Greedy Layer-wise Pre-training' 방식으로 널리 사용되었습니다.\n",
    "\n",
    "③ 입력 수는 은닉층 수보다 항상 작다. (틀림) 오토인코더의 핵심은 데이터를 압축하는 것입니다. 따라서 일반적으로 입력층의 노드 수보다 은닉층(Bottleneck)의 노드 수가 더 적은 구조(Undercomplete)를 가집니다. 입력 수가 은닉층 수보다 더 큰 것이 일반적이며, '항상 작다'는 설명은 반대로 표현된 것입니다.\n",
    "\n",
    "④ 인코드 입력 수와 디코드 출력 수는 동일하다. (옳음) 오토인코더는 입력을 복원(Reconstruction)하는 것이 목적이므로, 입력층의 차원과 최종 출력층의 차원은 반드시 같아야 합니다.\n",
    "\n",
    "🔍 오토인코더의 핵심 구조\n",
    "오토인코더는 크게 두 부분으로 나뉩니다.\n",
    "\n",
    "인코더(Encoder): 고차원 입력 데이터를 저차원의 핵심 특징(Latent Vector)으로 압축합니다.\n",
    "\n",
    "디코더(Decoder): 압축된 특징을 다시 원래의 고차원 데이터로 복원합니다.\n",
    "\n",
    "이 과정에서 은닉층은 데이터의 불필요한 노이즈를 제거하고 가장 중요한 정보만을 남기는 역할을 합니다.\n",
    "\n",
    "정답: ③"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9c1a7",
   "metadata": {},
   "source": [
    "# 58. 텍스트 문맥 파악을 위해서 단어 단위로 끊어서 판별하는 기법은?\n",
    "① 토픽모델링\n",
    "\n",
    "② 워드클라우드\n",
    "\n",
    "③ 엔그램(N-gram)\n",
    "\n",
    "④ TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a2a4e",
   "metadata": {},
   "source": [
    "텍스트 데이터에서 문맥을 파악하기 위해 인접한 단어들을 일정한 개수(단위)로 묶어 처리하는 기법은 **③ 엔그램(N-gram)**입니다.💡 주요 기법 설명엔그램(N-gram): 연속된 $n$개의 단어(또는 문자)를 하나의 단위로 묶어 분석하는 방법입니다. 단어가 단독으로 쓰일 때보다 인접한 단어와 함께 묶였을 때 문맥(Context)을 더 잘 파악할 수 있다는 점을 이용합니다.Unigram ($n=1$): \"사과\", \"가\", \"맛있다\"Bigram ($n=2$): \"사과 가\", \"가 맛있다\"Trigram ($n=3$): \"사과 가 맛있다\"토픽모델링(Topic Modeling): 문서 집합에서 추상적인 '주제(Topic)'를 발견하기 위한 통계적 모델입니다. (예: LDA)워드클라우드(Word Cloud): 텍스트 내에서 언급된 빈도수가 높은 단어를 시각적으로 크게 배열하여 한눈에 파악하게 하는 기법입니다.TF-IDF: 특정 문서 내에서 단어의 중요도를 평가하는 가중치 계산법입니다. (단어 빈도와 역문서 빈도의 곱)🔍 왜 N-gram인가요?예를 들어 \"가방에 들어간다\"라는 문장에서 단순히 단어만 추출하면 \"가방\", \"들어간다\"가 되지만, N-gram(Bigram)을 사용하면 두 단어의 연속성을 유지하여 문장의 흐름이나 문맥적 의미를 보존할 수 있습니다.정답: ③"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0fb21a",
   "metadata": {},
   "source": [
    "# 59. 다음 설명 중 배경에 대한 내용으로 가장 옳은 것은?\n",
    "① 편향이 낮은 과소적합 모델을 사용\n",
    "\n",
    "② 편향이 높은 과적합 모델을 사용\n",
    "\n",
    "③ 부트스트랩 자료를 생성하고 각 부트스트랩 자료를 결합하여 최종 예측 모델 산출\n",
    "\n",
    "④ 가중치를 활용하여 약 분류기를 강 분류기로 만드는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e8402",
   "metadata": {},
   "source": [
    "문제에서 '배경'은 오타로 보이며, 맥락상 **배깅(Bagging, Bootstrap Aggregating)**에 대한 설명을 묻는 문제입니다. 배깅에 대한 설명으로 가장 옳은 것은 ③번입니다.\n",
    "\n",
    "💡 상세 해설\n",
    "①, ② 편향과 분산 (오답): 배깅은 보통 분산(Variance)이 높은 과적합된 모델(주로 깊은 의사결정나무)들을 결합하여 전체적인 분산을 낮추는 데 효과적입니다. 편향이 높은 모델을 쓰는 것은 적절하지 않습니다.\n",
    "\n",
    "③ 부트스트랩 자료 생성 및 결합 (정답): 배깅의 핵심 정의입니다. 원래 데이터에서 중복을 허용하여 무작위로 추출한 표본인 부트스트랩(Bootstrap) 자료를 여러 개 만들고, 각각 모델을 학습시킨 뒤 그 결과를 투표(Voting)나 평균(Averaging)을 통해 **결합(Aggregating)**합니다.\n",
    "\n",
    "④ 가중치를 활용한 방법 (오답): 이는 **부스팅(Boosting)**에 대한 설명입니다. 부스팅은 이전 모델이 틀린 데이터에 가중치를 부여하여 순차적으로 학습하는 방식입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3672572",
   "metadata": {},
   "source": [
    "# 60. 비모수 통계 분석기법인 윌콕슨 부호순위(Wilcoxon Signed Rank)와 윌콕슨 순위합(Wilcoxon Rank Sum)에 대한 설명 중 가장 옳지 않은 것은?\n",
    "① 윌콕슨 부호순위는 일표본 검정이다.\n",
    "\n",
    "② 윌콕슨 순위합은 이표본 검정이다.\n",
    "\n",
    "③ 주로 30개 이하의 작은 샘플일 때 사용한다.\n",
    "\n",
    "④ 윌콕슨 부호순위 검정은 검정 결과가 대칭이어야만 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de51f778",
   "metadata": {},
   "source": [
    "비모수 통계 분석 기법인 윌콕슨 검정들에 대한 설명 중 가장 옳지 않은 것은 ④번입니다.💡 상세 해설① 윌콕슨 부호순위(Wilcoxon Signed Rank)는 일표본 검정이다. (옳음)윌콕슨 부호순위 검정은 단일 표본의 중앙값이 특정 값과 같은지 테스트하는 일표본(One-sample) 검정으로 사용될 뿐만 아니라, 동일한 집단의 전/후 차이를 비교하는 대응표본(Paired-sample) 검정으로도 널리 쓰입니다.② 윌콕슨 순위합(Wilcoxon Rank Sum)은 이표본 검정이다. (옳음)이 검정은 서로 독립적인 두 집단 간의 중앙값 차이를 비교하는 방법으로, 모수적 방법의 '독립표본 t-검정'에 대응하는 비모수 기법입니다. (맨-휘트니 U 검정과 사실상 동일합니다.)③ 주로 30개 이하의 작은 샘플일 때 사용한다. (옳음)비모수 검정은 표본의 크기가 작아 정규성 가정을 만족하기 어려운 경우(보통 $n < 30$)에 주로 사용합니다. 표본이 충분히 크면 중심극한정리에 의해 모수적 검정을 사용할 수 있기 때문입니다.④ 윌콕슨 부호순위 검정은 검정 결과가 대칭이어야만 한다. (틀림)**'대칭성(Symmetry)'**은 검정 결과에 대한 요구사항이 아니라, 분포에 대한 가정입니다. 윌콕슨 부호순위 검정은 자료의 분포(또는 차이의 분포)가 중앙값을 중심으로 대칭이라는 가정을 전제로 합니다. 따라서 '검정 결과가 대칭이어야 한다'는 표현은 분석 기법의 성격과 맞지 않는 잘못된 설명입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7df487a",
   "metadata": {},
   "source": [
    "# 61. ROC 그래프의 설명으로 가장 옳은 것은?\n",
    "① 민감도가 1, 특이도가 0인 점을 지난다.\n",
    "\n",
    "② 민감도가 0, 특이도가 1인 점을 지난다.\n",
    "\n",
    "③ 특이도가 증가하는 그래프이다.\n",
    "\n",
    "④ 가장 이상적인 그래프는 민감도가 1, 특이도가 1인 점을 지난다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c625d52",
   "metadata": {},
   "source": [
    "ROC(Receiver Operating Characteristic) 곡선은 이진 분류 모델의 성능을 평가하는 대표적인 지표입니다. 각 보기의 내용을 분석하면 정답은 ④번입니다.\n",
    "\n",
    "💡 상세 해설\n",
    "①, ②번 설명: ROC 곡선은 항상 **(0,0)**과 **(1,1)**을 지납니다.\n",
    "\n",
    "$(0,0)$은 민감도 0, 1-특이도 0인 지점입니다.\n",
    "\n",
    "$(1,1)$은 민감도 1, 1-특이도 1(즉, 특이도 0)인 지점입니다. 따라서 특정 수치 하나만을 고정해서 설명한 ①, ②번은 ROC 곡선의 특징을 온전히 설명하지 못합니다.\n",
    "\n",
    "③ 특이도가 증가하는 그래프이다. (틀림): ROC 곡선의 X축은 **'1-특이도(False Positive Rate)'**이고, Y축은 **'민감도(True Positive Rate)'**입니다. 그래프가 오른쪽으로 갈수록 '1-특이도'가 증가하므로, 실제로는 특이도가 감소하는 방향으로 그려집니다.\n",
    "\n",
    "④ 가장 이상적인 그래프는 민감도가 1, 특이도가 1인 점을 지난다. (정답): 모델이 완벽하다면 거짓 긍정(FPR)은 0이고 진짜 긍정(TPR)은 1이어야 합니다. 즉, **좌측 상단 꼭짓점$(0, 1)$**을 지나는 그래프가 가장 이상적입니다. 이 지점은 **특이도가 1(FPR=0)**이고 민감도가 1인 상태를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d6445",
   "metadata": {},
   "source": [
    "# 62. $y=0$ 혹은 $y=1$ 값을 가지는 이진분류 분석에서 $y=1$ 의 값이 $y=0$ 값의 2배일 때, 민감도, 특이도, 정확도에 대한 설명으로 적절한 것은?\n",
    "① 민감도와 특이도 둘 다 1일 때 정확도는 1이다.② 특이도가 1일 때 정확도는 1/2이다.③ 민감도가 1/2일 때 정확도는 1/2이다.④ 민감도와 특이도가 같을 때 정확도도 특이도와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307909f2",
   "metadata": {},
   "source": [
    "이 문제는 혼동 행렬(Confusion Matrix)의 평가지표 간의 관계를 묻는 문제입니다. 주어진 조건($y=1$의 빈도가 $y=0$의 2배)을 수식으로 정리하여 각 보기를 검증해 보겠습니다.💡 문제 상황 정의$y=1$인 실제 양성(Positive)의 개수: $2k$$y=0$인 실제 음성(Negative)의 개수: $k$전체 데이터 수 ($N$): $3k$평가지표 공식:민감도(Sensitivity, TPR): $\\frac{TP}{2k}$특이도(Specificity, TNR): $\\frac{TN}{k}$정확도(Accuracy): $\\frac{TP + TN}{3k}$🔍 보기 분석① 민감도와 특이도가 둘 다 1일 때 정확도는 1이다. (정답)민감도가 1이면 $TP = 2k$, 특이도가 1이면 $TN = k$입니다.정확도 = $\\frac{2k + k}{3k} = 1$이 됩니다. 어떤 데이터 분포에서도 두 지표가 모두 1이면 모델은 완벽하며 정확도는 1입니다.② 특이도가 1일 때 정확도는 1/2이다. (오답)특이도가 1이면 $TN = k$입니다. 이때 $TP$의 값에 따라 정확도는 달라집니다.만약 모델이 전부 0으로 예측했다면 $TP=0$이 되어 정확도는 $k/3k = 1/3$이 됩니다.③ 민감도가 1/2일 때 정확도는 1/2이다. (오답)민감도가 1/2이면 $TP = k$입니다.정확도 = $\\frac{k + TN}{3k}$이므로, $TN$의 값에 따라 결과가 달라집니다.④ 민감도와 특이도가 같을 때 정확도도 특이도와 같다. (옳음 - 하지만 일반적으로 ①번이 더 명확한 정의임)민감도($S$) = 특이도($T$) = $x$라고 가정하면:$TP = 2k \\cdot x$, $TN = k \\cdot x$정확도 = $\\frac{2kx + kx}{3k} = \\frac{3kx}{3k} = x$따라서 정확도 역시 $x$가 되어 특이도(혹은 민감도)와 같아집니다.참고: 이 문제의 의도는 클래스 불균형이 있더라도 **민감도와 특이도가 모두 완벽하면 정확도도 완벽하다(1이다)**는 가장 기본적이고 확실한 원리를 묻는 것입니다. 따라서 ①번이 가장 적절한 답안으로 채택됩니다. (④번 또한 수학적으로는 성립하지만, ①번이 분류 모델의 핵심 성능을 가장 잘 대변합니다.)💡 요약민감도(1) + 특이도(1) = 정확도(1) 관계는 데이터의 비율과 상관없이 성립하는 절대적인 법칙입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7cf37",
   "metadata": {},
   "source": [
    "# 63. 실루엣계수를 이용한 최적의 군집분석 개수는?\n",
    "(그래프: 군집 개수 1~5에 따른 실루엣 계수 변화 그래프 포함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f0696",
   "metadata": {},
   "source": [
    "① 2\n",
    "\n",
    "② 3\n",
    "\n",
    "③ 4\n",
    "\n",
    "④ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec7549e",
   "metadata": {},
   "source": [
    "실루엣 계수(Silhouette Coefficient)를 이용하여 최적의 군집 개수($k$)를 결정할 때는 실루엣 계수 값이 가장 큰(높은) 지점을 선택합니다.💡 실루엣 계수의 의미정의: 군집 내 데이터 간의 거리는 가깝고($a$), 군집 간 거리는 멀게($b$) 형성되었는지를 나타내는 지표입니다.범위: $-1$에서 $1$ 사이의 값을 가지며, 1에 가까울수록 군집화가 아주 잘 된 상태를 의미합니다.해석: * $1$ 근처: 군집이 서로 잘 분리되어 있음.$0$ 근처: 군집 간 경계가 모호함.음수: 데이터가 잘못된 군집에 할당됨.🔍 문제 풀이 가이드일반적으로 시험 문제에 포함되는 그래프는 특정 $k$ 값에서 정점을 찍는 형태를 보입니다.그래프의 **세로축(Silhouette Score)**이 가장 높은 곳을 찾습니다.해당 지점의 가로축(Number of Clusters, $k$) 값을 확인합니다.보통 실루엣 분석 예시에서는 $k=2$ 또는 $k=3$에서 가장 높은 계수를 보이는 경우가 많습니다. 제시된 그래프에서 가장 높은 점에 위치한 가로축 숫자가 정답입니다. (그래프 이미지가 직접 보이지 않지만, 수치상 가장 높은 곳을 고르시면 됩니다.)일반적인 정답 유형: 그래프상 피크(Peak) 지점이 2라면 ① 2가 답이 됩니다.💡 추가 팁: 엘보우(Elbow) 기법과 차이점엘보우 기법: SSE(오차제곱합)를 사용하여, 기울기가 급격하게 완만해지는(꺾이는) 지점을 찾습니다.실루엣 기법: 계수값이 **최대(Maximum)**가 되는 지점을 찾습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165381da",
   "metadata": {},
   "source": [
    "# 64. 혼동행렬에서의 FN 해석에 대한 것으로 알맞은 것은?\n",
    "① 예측값 False, 실제값 False\n",
    "\n",
    "② 예측값 False, 실제값 True\n",
    "\n",
    "③ 예측값 True, 실제값 False\n",
    "\n",
    "④ 예측값 True, 실제값 True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16472546",
   "metadata": {},
   "source": [
    "혼동행렬(Confusion Matrix)의 약어는 **[예측의 성공 여부][예측한 값]**의 순서로 구성됩니다. 따라서 FN을 풀어서 해석하면 다음과 같습니다.\n",
    "\n",
    "F (False): 예측이 틀렸음 (실제값과 예측값이 다름)\n",
    "\n",
    "N (Negative): 모델이 **False(0)**라고 예측했음\n",
    "\n",
    "즉, 모델은 False라고 예측했는데 그 예측이 틀렸으므로, **실제값은 True(1)**였다는 뜻입니다.\n",
    "\n",
    "💡 상세 분석\n",
    "① 예측값 False, 실제값 False: 예측이 맞았으므로 TN (True Negative)\n",
    "\n",
    "② 예측값 False, 실제값 True: 예측이 틀렸고(F) 예측치는 Negative(N)이므로 FN (False Negative) → 정답!\n",
    "\n",
    "③ 예측값 True, 실제값 False: 예측이 틀렸고(F) 예측치는 Positive(P)이므로 FP (False Positive)\n",
    "\n",
    "④ 예측값 True, 실제값 True: 예측이 맞았으므로 TP (True Positive)\n",
    "\n",
    "🔍 암기 팁!\n",
    "혼동행렬이 헷갈릴 때는 뒤의 글자부터 보세요.\n",
    "\n",
    "뒤의 글자(P/N): 내가 뭐라고 대답했는가? (P면 True라고 대답, N이면 False라고 대답)\n",
    "\n",
    "앞의 글자(T/F): 내 대답이 맞았는가? (T면 정답, F면 오답)\n",
    "\n",
    "정답: ② 예측값 False, 실제값 True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423b779",
   "metadata": {},
   "source": [
    "# 65. 데이터 불균형이 있을 경우 사용하는 평가지표로 옳지 않은 것은?\n",
    "① 민감도\n",
    "\n",
    "② 정확도\n",
    "\n",
    "③ 오분류율\n",
    "\n",
    "④ ROC 곡선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17399aff",
   "metadata": {},
   "source": [
    "데이터 불균형(Class Imbalance)이 심한 경우(예: 암 환자가 1%, 정상인이 99%인 데이터)에 사용하기에 가장 적절하지 않은 평가지표는 ② 정확도와 ③ 오분류율입니다.\n",
    "\n",
    "하지만 이 중 하나를 골라야 한다면, 일반적으로 데이터 분석 시험에서 '데이터 불균형 시 가장 믿을 수 없는 지표'로 꼽히는 **② 정확도(Accuracy)**가 가장 대표적인 오답 후보입니다. (오분류율은 정확도와 동전의 양면 같은 지표라 사실상 같은 문제를 공유합니다.)\n",
    "\n",
    "💡 왜 정확도가 위험한가요?\n",
    "데이터 100개 중 99개가 '정상'이고 1개가 '암'인 경우, 모델이 무조건 \"정상\"이라고만 대답해도 **정확도는 99%**가 나옵니다. 정작 중요한 1명의 암 환자를 한 명도 찾아내지 못했는데도 성능이 매우 우수해 보이는 **'정확도의 역설'**이 발생하기 때문입니다.\n",
    "\n",
    "🔍 선택지 분석\n",
    "① 민감도(Sensitivity): 실제 양성 중 얼마나 맞췄는지를 보므로 불균형 데이터에서 매우 중요합니다.\n",
    "\n",
    "② 정확도: 앞서 설명한 대로 다수 클래스에 편향된 결과를 보여주므로 부적절합니다.\n",
    "\n",
    "③ 오분류율(Error Rate): $1 - \\text{정확도}$입니다. 정확도가 신뢰할 수 없다면 오분류율 역시 데이터 불균형 상황에서는 의미가 퇴색됩니다.\n",
    "\n",
    "④ ROC 곡선: 임계값(Threshold)을 변화시키며 민감도와 특이도의 관계를 보기 때문에 불균형 데이터에서도 모델의 변별력을 파악하기 좋은 지표입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c03010",
   "metadata": {},
   "source": [
    "# 66. 홀드아웃 관련 데이터가 아닌 것은?\n",
    "① 중간 데이터\n",
    "\n",
    "② 학습 데이터\n",
    "\n",
    "③ 검증 데이터\n",
    "\n",
    "④ 테스트 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77e8122",
   "metadata": {},
   "source": [
    "홀드아웃(Hold-out) 방법은 전체 데이터를 서로 겹치지 않는 여러 세트로 나누어 모델을 생성하고 평가하는 가장 기본적인 데이터 분할 기표입니다.\n",
    "\n",
    "이 중 홀드아웃의 일반적인 구성 요소가 아닌 것은 ① 중간 데이터입니다.\n",
    "\n",
    "💡 홀드아웃의 데이터 분할 구조\n",
    "일반적으로 전체 데이터는 목적에 따라 다음과 같이 세 가지로 나뉩니다.\n",
    "\n",
    "② 학습 데이터 (Training Data): 모델을 학습시키는 데 직접적으로 사용되는 데이터입니다.\n",
    "\n",
    "③ 검증 데이터 (Validation Data): 학습된 모델들의 성능을 비교하고, 하이퍼파라미터를 튜닝하며 최적의 모델을 선택하기 위해 사용됩니다.\n",
    "\n",
    "④ 테스트 데이터 (Test Data): 최종 선택된 모델의 일반화 성능을 측정하기 위해 마지막에 딱 한 번 사용하는 데이터입니다.\n",
    "\n",
    "🔍 왜 '중간 데이터'는 아닌가요?\n",
    "'중간 데이터'라는 용어는 분석 과정에서 생성되는 임시 파일이나 처리 과정의 결과물을 지칭할 때 사용될 수는 있지만, 머신러닝 모델의 성능 평가를 위한 데이터 분할(Hold-out) 체계에서는 공식적으로 사용되는 명칭이 아닙니다.\n",
    "\n",
    "정답: ①\n",
    "\n",
    "홀드아웃 방식은 데이터가 충분히 많을 때 유용하며, 데이터가 적을 경우에는 이 방식을 보완한 K-폴드 교차 검증(K-Fold Cross Validation) 등을 사용하기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ba65a",
   "metadata": {},
   "source": [
    "# 67. 케이폴드 교차검증(K-fold CV)에 대한 설명 중 옳지 않은 것은?\n",
    "① 학습 데이터(훈련 데이터)와 검증 데이터 혹은 테스트 데이터로 분할② $k=3$ 이상만 가능③ $k$개의 균일한 서브셋④ $k-1$ 개의 부분집합을 학습 데이터로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7efe74",
   "metadata": {},
   "source": [
    "K-폴드 교차검증(K-fold Cross Validation)에 대한 설명 중 옳지 않은 것은 ② $k=3$ 이상만 가능입니다.💡 상세 해설① 학습 데이터와 검증 데이터 혹은 테스트 데이터로 분할 (옳음)전체 데이터를 $k$개의 그룹으로 나눈 뒤, 그중 하나를 검증(Validation)용으로 사용하고 나머지를 학습(Training)용으로 사용하는 방식입니다.② $k=3$ 이상만 가능 (틀림)$k$값은 이론적으로 2 이상이면 가능합니다. 예를 들어 $k=2$인 경우 데이터를 반으로 나누어 한 번씩 번갈아 가며 학습과 검증을 수행할 수 있습니다. 다만, 실제 분석에서는 데이터의 편향을 줄이기 위해 보통 $k=5$ 또는 $k=10$을 가장 많이 사용합니다.③ $k$개의 균일한 서브셋 (옳음)전체 데이터를 최대한 동일한 크기를 가진 $k$개의 부분집합(Subset, Fold)으로 나눕니다.④ $k-1$ 개의 부분집합을 학습 데이터로 사용 (옳음)$k$개의 폴드 중 1개는 평가(검증)를 위해 남겨두고, 나머지 $k-1$개를 모델 학습에 사용합니다. 이 과정을 $k$번 반복하여 평균 성능을 산출합니다.🔍 K-폴드 교차검증의 특징데이터 활용도: 모든 데이터를 한 번씩 검증에 사용할 수 있어 데이터 낭비가 적습니다.일반화 성능: 특정 데이터셋에만 과적합되는 문제를 방지하고 모델의 일반화 성능을 신뢰성 있게 평가할 수 있습니다.시간 복잡도: 모델을 $k$번 학습시켜야 하므로 학습 시간이 오래 걸린다는 단점이 있습니다.정답: ②"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0964304b",
   "metadata": {},
   "source": [
    "# 68. A상품에 대한 인지도 조사결과가 아래와 같을 때, 이에 대한 설명으로 옳지 않은 것은?\n",
    "(표: 아이가 있는 남성/없는 남성별 알고 있다/모른다 빈도수 총계 1000명)\n",
    "\n",
    "① A제품을 알고 있을 확률은 0.90이다.\n",
    "\n",
    "② 아이가 있는 남자이면서 A제품을 모르고 있을 확률은 0.04이다.\n",
    "\n",
    "③ 아이가 없는 남자이면서 A제품을 모르고 있을 확률은 0.06이다.\n",
    "\n",
    "④ 아이가 없는 남자 중에서 A제품을 알고 있을 확률은 약 0.96이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbde706",
   "metadata": {},
   "source": [
    "68. A상품에 대한 인지도 조사결과가 아래와 같을 때, 이에 대한 설명으로 옳지 않은 것은?\n",
    "(표: 아이가 있는 남성/없는 남성별 알고 있다/모른다 빈도수 총계 1000명)\n",
    "\n",
    "① A제품을 알고 있을 확률은 0.90이다.\n",
    "\n",
    "② 아이가 있는 남자이면서 A제품을 모르고 있을 확률은 0.04이다.\n",
    "\n",
    "③ 아이가 없는 남자이면서 A제품을 모르고 있을 확률은 0.06이다.\n",
    "\n",
    "④ 아이가 없는 남자 중에서 A제품을 알고 있을 확률은 약 0.96이다.\n",
    "\n",
    "(※ 위 수치는 보기의 확률값인 0.9, 0.04, 0.06 등을 만족하는 추정치입니다.)\n",
    "\n",
    "💡 각 보기 분석① A제품을 알고 있을 확률은 0.90이다.전체 1,000명 중 알고 있는 사람이 900명이라면 \n",
    "\n",
    "$\\frac{900}{1,000} = 0.9$로 옳은 설명입니다.② 아이가 있는 남자이면서 A제품을 모르고 있을 확률은 0.04이다.\n",
    "\n",
    "전체 1,000명 중 해당 조건(교집합)이 40명이라면 $\\frac{40}{1,000} = 0.04$로 옳은 설명입니다.\n",
    "\n",
    "③ 아이가 없는 남자이면서 A제품을 모르고 있을 확률은 0.06이다.전체 1,000명 중 해당 조건(교집합)이 60명이라면\n",
    "\n",
    "$\\frac{60}{1,000} = 0.06$로 옳은 설명입니다.\n",
    "\n",
    "④ 아이가 없는 남자 중에서 A제품을 알고 있을 확률은 약 0.96이다. (오답)이 보기는 조건부 확률입니다. 전체(1,000명)가 아닌 '아이가 없는 남자(600명)'를 분모로 써야 합니다.계산: $\\frac{\\text{아이가 없는 남성 중 알고 있는 사람}}{\\text{아이가 없는 남성 전체}} = \\frac{540}{600} = \\mathbf{0.9}$따라서 0.96이 아니라 **0.9(90%)**가 되어야 하므로 옳지 않습니다.\n",
    "\n",
    "🔍 핵심 개념: 결합 확률 vs 조건부 확률결합 확률 (Intersection): 전체 인원 대비 해당 칸의 비율 (예: ②, ③번)조건부 확률 (Conditional): 특정 행이나 열의 합계 대비 해당 칸의 비율 (예: ④번)정답: ④표의 실제 수치가 조금 다르더라도, 보통 **'전체 대비 확률'**과 **'특정 집단 내에서의 확률'**을 혼동하게 만드는 것이 이 유형의 함정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63ba74",
   "metadata": {},
   "source": [
    "# 69. 다음 관측값에 대한 설명으로 옳지 않은 것은?\n",
    "(관측값: 34, 46, 60, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80499c77",
   "metadata": {},
   "source": [
    "① 기대빈도 50\n",
    "\n",
    "② 모비율 $P(54)$는 1/4\n",
    "\n",
    "③ 카이제곱값 4.64\n",
    "\n",
    "④ 카이제곱(3) = 7.8이라면, 귀무가설을 기각한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ca728",
   "metadata": {},
   "source": [
    "주어진 관측값 $34, 46, 60, 40$을 바탕으로 각 보기를 검증해 보겠습니다. 우선 기본 통계량을 계산합니다.관측값의 총합: $34 + 46 + 60 + 40 = 180$표본의 개수($n$): $4$💡 각 보기 상세 분석① 기대빈도 50 (오답)적합도 검정(Goodness of Fit)에서 각 범주의 발생 확률이 동일하다고 가정할 경우, 기대빈도(Expected Frequency)는 $\\frac{\\text{총합}}{\\text{범주 수}}$입니다.$$\\text{기대빈도} = \\frac{180}{4} = 45$$따라서 50이라는 설명은 옳지 않습니다.② 모비율 $P(54)$는 $1/4$ (오답 가능성 높음)관측값 중 '54'라는 값은 존재하지 않습니다. 만약 특정 값의 발생 확률을 묻는 것이라면 관측값 $4$개 중 하나가 $54$여야 하는데 그렇지 않습니다. 다만, 문제의 오타로 $P(60)$이나 다른 존재하는 값을 묻는다면 전체 4개 중 1개이므로 $1/4$이 될 수 있습니다. 하지만 ①번이 수치상 명확한 틀린 답입니다.③ 카이제곱값 4.64 (검증)카이제곱 통계량 공식은 $\\chi^2 = \\sum \\frac{(O - E)^2}{E}$ 입니다 ($O$: 관측값, $E$: 기대빈도 45).$(34-45)^2 / 45 = (-11)^2 / 45 = 121 / 45 \\approx 2.689$$(46-45)^2 / 45 = (1)^2 / 45 = 1 / 45 \\approx 0.022$$(60-45)^2 / 45 = (15)^2 / 45 = 225 / 45 = 5.0$$(40-45)^2 / 45 = (-5)^2 / 45 = 25 / 45 \\approx 0.556$합계: $2.689 + 0.022 + 5.0 + 0.556 = \\mathbf{8.267}$(※ 문제에 제시된 4.64와 차이가 있으나, 이는 기대빈도를 다르게 설정했거나 관측값에 오타가 있을 때 발생하는 차이일 수 있습니다. 하지만 원론적으로 ①번의 기대빈도 계산이 확실히 틀렸습니다.)④ 카이제곱(3) = 7.8이라면, 귀무가설을 기각한다. (옳음)자유도(df)는 범주 수 - 1이므로 $4 - 1 = 3$입니다. 계산된 카이제곱값($8.267$)이 임계치인 $7.8$보다 크다면, 통계적으로 유의미한 차이가 있다고 판단하여 귀무가설을 기각하게 됩니다.🔍 정답 확인기대빈도 계산($180/4 = 45$)이 가장 기본적이고 명확하게 틀린 설명입니다.정답: ①"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcfed07",
   "metadata": {},
   "source": [
    "# 70. 아래 사례의 유의성 검정으로 옳은 것은?\n",
    "어느 중학교에서 1학년 학생들의 키 차이가 2학년이 되면 더 커질 것이라고 예상된다. 1학년에서 6명을 뽑고, 2학년에서 8명을 뽑아서 각각의 키 분산을 조사해 봤더니, 1학년의 분산은 10.0이었고, 2학년의 분산은 50.0이었다. 두 모집단의 분산은 같다고 볼 수 있는지 검정하라(유의수준 $\\alpha=0.05$).(표: DEGREE OF NUMERATOR (V1) 임계값 표 포함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac7fc7",
   "metadata": {},
   "source": [
    "① F 통계량, p-value < 유의수준, 귀무가설 채택② F 통계량, p-value < 유의수준, 귀무가설 기각③ 카이제곱, p-value < 유의수준, 귀무가설 채택④ 카이제곱, p-value < 유의수준, 귀무가설 기각"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3a17a",
   "metadata": {},
   "source": [
    "두 집단의 **분산 차이(분산의 동질성)**를 검정하는 통계 방법은 **F-검정(F-test)**입니다. 주어진 데이터를 바탕으로 F-검정을 수행하는 과정은 다음과 같습니다.1. 가설 설정귀무가설($H_0$): 두 집단의 분산은 같다. ($\\sigma_1^2 = \\sigma_2^2$)대립가설($H_1$): 두 집단의 분산은 다르다. ($\\sigma_1^2 \\neq \\sigma_2^2$)2. 검정통계량(F-값) 계산F-통계량은 일반적으로 큰 분산을 작은 분산으로 나눈 값입니다.1학년(집단 1): $n_1 = 6$, $s_1^2 = 10.0$2학년(집단 2): $n_2 = 8$, $s_2^2 = 50.0$F-값 = $50.0 / 10.0 = \\mathbf{5.0}$3. 자유도 결정분자 자유도($v_1$): $n_2 - 1 = 8 - 1 = \\mathbf{7}$ (큰 분산 집단)분모 자유도($v_2$): $n_1 - 1 = 6 - 1 = \\mathbf{5}$ (작은 분산 집단)4. 임계값 비교 및 결론문제에 제시된 F-분포표에서 분자 자유도 7, 분모 자유도 5인 지점의 임계값($F_{0.05}(7, 5)$)을 찾습니다.통상적으로 $F_{0.05}(7, 5)$의 값은 약 4.88 정도입니다.판단: 계산된 F-값(5.0) > 임계값(4.88)따라서 귀무가설을 기각합니다. 즉, 1학년과 2학년의 키 분산은 통계적으로 유의미한 차이가 있다고 결론지을 수 있습니다.🔍 요약사용하는 검정: F-검정검정통계량: 5.0결과: 분산이 같다는 귀무가설 기각 (분산이 서로 다름)정답: F-검정을 통해 귀무가설을 기각하고, 두 집단의 분산이 다르다고 판단하는 것이 옳습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d6b16",
   "metadata": {},
   "source": [
    "# 71. 정준연결(Canonical link)의 로그 함수로 알맞은 것은?\n",
    "① 정규분포 ② 베르누이분포 ③ 포아송분포 ④ 감마분포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9cfdad",
   "metadata": {},
   "source": [
    "일반화 선형 모델(GLM)에서 **정준 연결 함수(Canonical Link Function)**는 지수족 분포의 자연스러운 파라미터와 선형 예측자를 연결하는 역할을 합니다.각 분포에 대응하는 정준 연결 함수는 다음과 같습니다.💡 주요 분포별 정준 연결 함수분포 (Distribution)정준 연결 함수 (Link Function)함수 형태정규분포항등 함수 (Identity)$\\mu$베르누이/이항분포로짓 함수 (Logit)$\\ln(\\frac{p}{1-p})$포아송분포로그 함수 (Log)$\\ln(\\mu)$감마분포역함수 (Inverse)$1/\\mu$🔍 해설문제에서 **로그 함수(Log Function)**를 정준 연결 함수로 사용하는 분포를 물었으므로 정답은 포아송분포입니다.포아송분포: 주로 단위 시간이나 공간에서 발생하는 사건의 횟수를 모델링할 때 사용하며, 선형 예측자의 결과가 항상 양수가 되어야 하므로 로그 함수를 연결 함수로 사용합니다.정답: ③ 포아송분포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0a559",
   "metadata": {},
   "source": [
    "# 72. 포아송분포에 대한 적합도 검정을 한다. 보기 중 옳지 않은 것은?\n",
    "① 하루에 일어난 사건에 대한 평균을 구해야 한다.② 카이제곱값이 클수록 귀무가설을 기각한다.③ 람다는 어떤 일정 시간과 공간의 구간 안에서 발생한 평균 사건 수를 의미하지 않는다.④ $P$값이 유의수준보다 작으면 귀무가설을 기각한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca9468",
   "metadata": {},
   "source": [
    "포아송분포(Poisson Distribution)의 정의와 가설 검정의 원리를 고려할 때, 옳지 않은 설명은 ③번입니다.💡 상세 해설① 하루에 일어난 사건에 대한 평균을 구해야 한다. (옳음)포아송분포의 적합도 검정을 위해서는 관측 데이터로부터 분포의 핵심 파라미터인 평균($\\lambda$)을 먼저 추정해야 합니다.② 카이제곱값이 클수록 귀무가설을 기각한다. (옳음)적합도 검정에서 카이제곱값($\\chi^2$)은 '관측값과 기대값의 차이'를 나타냅니다. 이 값이 클수록 실제 데이터가 가정된 분포(포아송분포)와 다르다는 증거가 되므로 귀무가설을 기각하게 됩니다.③ 람다는 어떤 일정 시간과 공간의 구간 안에서 발생한 평균 사건 수를 의미하지 않는다. (틀림)포아송분포에서 람다($\\lambda$)의 정의 그 자체를 부정하고 있습니다. 람다는 정해진 시간, 거리, 면적 등 특정 구간 내에서 발생하는 평균 사건 수를 의미하는 핵심 파라미터입니다.④ $P$값이 유의수준보다 작으면 귀무가설을 기각한다. (옳음)모든 가설 검정의 공통된 원칙입니다. $P$-value가 설정한 유의수준(예: 0.05)보다 작다는 것은 관측된 결과가 우연히 일어날 확률이 매우 낮다는 뜻이므로 귀무가설을 기각합니다.🔍 포아송 적합도 검정 요약귀무가설($H_0$): 데이터가 포아송분포를 따른다.대립가설($H_1$): 데이터가 포아송분포를 따르지 않는다.검정 통계량: 카이제곱 검정 통계량을 사용하며, 자유도는 (범주 수 - 1 - 추정된 파라미터 수)로 결정됩니다.정답: ③"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ecac5",
   "metadata": {},
   "source": [
    "# 73. 과대적합일 때 대응방법으로 옳지 않은 것은?\n",
    "① 정규화(Regularization) ② 배치정규화(Batch Regulation) ③ 드롭아웃(Drop-Out) ④ 맥스풀링(Max Pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8598f3",
   "metadata": {},
   "source": [
    "과적합(Overfitting)은 모델이 학습 데이터에만 너무 과하게 맞추어져서 실제 새로운 데이터에 대한 성능이 떨어지는 현상을 의미합니다. 이를 해결하기 위한 방법 중 옳지 않은 것은 **④ 맥스풀링(Max Pooling)**입니다.\n",
    "\n",
    "💡 상세 해설\n",
    "① 정규화(Regularization): L1(Lasso)이나 L2(Ridge) 정규화를 통해 모델의 가중치가 너무 커지지 않도록 규제를 가하여 모델의 복잡도를 줄입니다. (옳음)\n",
    "\n",
    "② 배치 정규화(Batch Normalization): 각 층의 출력을 표준화하여 학습 과정을 안정화시키고 가중치 초기화의 영향을 줄임으로써, 간접적으로 과적합을 방지하는 효과가 있습니다. (옳음)\n",
    "\n",
    "③ 드롭아웃(Drop-Out): 학습 과정 중에 무작위로 일부 뉴런을 생략하여 특정 뉴런에만 의존하는 현상을 막아 과적합을 방지하는 아주 대표적인 기법입니다. (옳음)\n",
    "\n",
    "④ 맥스풀링(Max Pooling): 이는 CNN(합성곱 신경망)에서 이미지의 크기(해상도)를 줄이거나 주요 특징을 강조하기 위해 사용하는 차원 축소 기법입니다. 파라미터 수를 줄여 연산량을 감소시키는 효과는 있지만, 과적합을 직접적으로 해결하기 위한 '대응 방법'으로 분류하기에는 무리가 있습니다. (틀림)\n",
    "\n",
    "🔍 과적합 해결을 위한 기타 방법들\n",
    "데이터 증강(Data Augmentation): 더 많은 데이터를 학습시켜 일반화 성능을 높입니다.\n",
    "\n",
    "조기 종료(Early Stopping): 검증 오차가 증가하기 시작하는 시점에 학습을 중단합니다.\n",
    "\n",
    "가중치 감소(Weight Decay): 가중치의 크기에 비례하는 비용을 손실 함수에 추가합니다.\n",
    "\n",
    "정답: ④"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29dffe7",
   "metadata": {},
   "source": [
    "# 74. 다음 중 시공간시각화 기법으로 가장 알맞은 것은?\n",
    "① 히스토그램 ② 체르노프페이스 ③ 지도매핑 ④ 평행좌표계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0ef95",
   "metadata": {},
   "source": [
    "시공간(Spatiotemporal) 시각화는 위치(공간) 정보와 시간(흐름) 정보를 동시에 표현하는 기법을 의미합니다. 따라서 정답은 ③ 지도매핑입니다.\n",
    "\n",
    "💡 선택지별 시각화 기법 상세 설명\n",
    "① 히스토그램 (Histogram): 수치형 데이터의 분포를 막대 형태로 나타내는 기법입니다. (단변량 데이터 시각화)\n",
    "\n",
    "② 체르노프 페이스 (Chernoff Faces): 사람의 얼굴 특징(눈의 크기, 입의 모양 등)에 데이터의 변수를 매핑하여 다변량 데이터를 표현하는 기법입니다.\n",
    "\n",
    "③ 지도매핑 (Map Mapping): 지리적 공간 정보 위에 데이터의 수치나 변화를 시각화합니다. 여기에 시간의 흐름을 더하면(애니메이션 효과 등) 완벽한 시공간 시각화가 됩니다. (예: 지역별 시간에 따른 감염병 확산 지도)\n",
    "\n",
    "④ 평행좌표계 (Parallel Coordinates): 여러 개의 수직 축을 평행하게 배치하여 다변량 데이터 간의 관계와 패턴을 파악하는 기법입니다.\n",
    "\n",
    "🔍 시공간 시각화의 대표적인 예시\n",
    "지도매핑 외에도 다음과 같은 기법들이 시공간 시각화에 자주 사용됩니다.\n",
    "\n",
    "카토그램 (Cartogram): 데이터의 수치에 따라 지도의 면적을 왜곡하여 표현.\n",
    "\n",
    "버블 맵 (Bubble Map): 지도 위에 시간별 데이터 크기를 원의 크기로 표현.\n",
    "\n",
    "플로우 맵 (Flow Map): 공간상의 이동 경로와 시간적 흐름을 선으로 표현.\n",
    "\n",
    "정답: ③"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52824f",
   "metadata": {},
   "source": [
    "# 75. 보고서 작성시 적절한 방법으로 가장 거리가 먼 것은?\n",
    "① 전문용어를 많이 사용한다. ② 쉽게 이해할 수 있도록 작성한다. ③ 비즈니스에 사용할 수 있도록 한다. ④ 보고서를 통해 성과기준과 기여도를 표현할 수 있도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe0e88",
   "metadata": {},
   "source": [
    "데이터 분석 보고서나 비즈니스 보고서를 작성할 때 가장 중요한 것은 전달력과 실행 가능성입니다. 따라서 보고서 작성 방법으로 가장 거리가 먼 것은 **① 전문용어를 많이 사용한다.**입니다.💡 왜 1번이 틀렸나요?보고서의 일차적인 목적은 의사결정권자나 관련 부서 사람들에게 분석 결과를 전달하여 행동을 이끌어내는 것입니다.전문용어를 과도하게 사용하면 해당 분야의 전문가가 아닌 독자는 내용을 이해하기 어려워집니다.훌륭한 보고서는 복잡한 분석 과정을 일반인의 언어로 풀어서 설명하고, 핵심 통찰(Insight)을 명확하게 전달해야 합니다.🔍 바람직한 보고서 작성 원칙항목설명이해 용이성도표, 그래프, 쉬운 문장을 활용해 누구나 직관적으로 이해할 수 있어야 합니다.비즈니스 연계단순한 현상 나열이 아니라, 이 결과가 비즈니스에 어떤 영향을 미치는지 제시해야 합니다.성과 및 기여도분석이나 프로젝트를 통해 기대되는 기대효과, ROI, 성과 지표를 명확히 포함해야 합니다.결론 중심바쁜 의사결정권자를 위해 핵심 결론(Key Message)을 앞부분에 배치하는 것이 좋습니다.정답: ①"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd7d8ca",
   "metadata": {},
   "source": [
    "# 76. 비교그래프가 아닌 것은?\n",
    "① 막대그래프 ② 레이더차트 ③ 히트맵 ④ 산점도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b313874",
   "metadata": {},
   "source": [
    "데이터 시각화에서 비교 그래프는 여러 항목 간의 수치 차이나 크기를 한눈에 파악하기 위한 목적으로 사용됩니다. 반면, 산점도는 두 변수 사이의 상관관계나 분포를 파악하는 것이 주된 목적입니다.따라서 정답은 ④ 산점도입니다.💡 각 시각화 기법의 목적① 막대그래프 (Bar Chart): 각 범주(항목) 간의 수치 크기를 막대 길이로 나타내어 가장 직관적으로 양적인 비교를 할 수 있는 도구입니다.② 레이더차트 (Radar Chart): 여러 측정 항목(변수)의 수치를 하나의 그래프에 표시하여 항목 간의 균형이나 강약을 비교할 때 유용합니다. (예: 선수 능력치 비교)③ 히트맵 (Heat Map): 색상의 진도를 통해 데이터의 밀도나 수치의 크기를 표현하며, 여러 영역 간의 상태나 비중 차이를 시각적으로 비교하기 좋습니다.④ 산점도 (Scatter Plot): 두 개의 연속형 변수 사이의 **관계(상관관계, 군집, 경향성)**를 점으로 찍어 표현하는 그래프입니다. 항목 간의 수치 비교보다는 \"X가 증가할 때 Y는 어떻게 변하는가?\"를 확인하는 데 초점이 맞춰져 있습니다.🔍 시각화 목적에 따른 분류 요약분류주요 그래프 기법비교 (Comparison)막대그래프, 레이더차트, 히트맵, 버블차트관계 (Relationship)산점도, 산점도 행렬분포 (Distribution)히스토그램, 박스플롯(Box Plot)시간 흐름 (Trend)선그래프(Line Chart), 영역차트정답: ④"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079f2dc",
   "metadata": {},
   "source": [
    "# 77. 누적히스토그램에 대한 설명으로 가장 거리가 먼 것은?\n",
    "① 범주형과 수치형 모두의 분포를 알 수 있다. ② 히스토그램의 y축을 평균으로도 나타낼 수 있다. ③ 누적히스토그램은 누적확률밀도함수와 반비례적인 형태를 보인다. ④ 계급수를 잘 정해야 정확한 분포 파악이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ad207",
   "metadata": {},
   "source": [
    "누적 히스토그램(Cumulative Histogram)은 각 계급에 속하는 빈도를 누적하여 차례대로 더해가는 방식의 그래프입니다. 각 보기를 분석했을 때 가장 거리가 먼 설명은 ③번입니다.\n",
    "\n",
    "💡 상세 해설\n",
    "① 범주형과 수치형 모두의 분포를 알 수 있다. (옳음) 일반적으로 수치형 데이터의 연속적인 분포를 보는 데 사용되지만, 범주형 데이터를 순서대로 나열하여 누적 빈도를 시각화하는 경우에도 활용이 가능합니다.\n",
    "\n",
    "② 히스토그램의 y축을 평균으로도 나타낼 수 있다. (옳음) 표현 방식에 따라 빈도(Frequency)뿐만 아니라, 특정 구간의 평균치나 상대 빈도(확률)를 누적하여 나타내는 변형된 형태도 존재합니다.\n",
    "\n",
    "③ 누적히스토그램은 누적확률밀도함수와 반비례적인 형태를 보인다. (틀림) 누적 히스토그램은 값이 누적될수록 우상향하는 계단식 형태를 띱니다. 이는 누적확률밀도함수(CDF)의 모양과 **정비례(유사)**한 형태를 보이는 것이지, 반비례(한쪽이 커질 때 다른 쪽이 작아지는 관계)하는 것이 아닙니다.\n",
    "\n",
    "④ 계급수를 잘 정해야 정확한 분포 파악이 가능하다. (옳음) 일반 히스토그램과 마찬가지로 계급(Bin)의 크기를 너무 크게 잡으면 세부 분포가 뭉개지고, 너무 작게 잡으면 노이즈가 심해져 전체적인 흐름을 파악하기 어렵습니다.\n",
    "\n",
    "🔍 누적 히스토그램의 특징\n",
    "우상향 구조: 누적 빈도를 나타내므로 오른쪽으로 갈수록 막대의 높이가 결코 낮아지지 않습니다.\n",
    "\n",
    "최종 높이: 마지막 막대의 높이는 항상 전체 데이터의 총합(또는 상대 빈도일 경우 100%)을 나타냅니다.\n",
    "\n",
    "용도: 데이터의 특정 백분위수(Percentile)나 중위수 등을 시각적으로 빠르게 가늠할 때 유용합니다.\n",
    "\n",
    "정답: ③"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13530cf9",
   "metadata": {},
   "source": [
    "# 78. 다음 그래프의 이름으로 적절한 것은?\n",
    "(그래프: 히트맵 형식의 데이터 시각화 자료) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb2a169",
   "metadata": {},
   "source": [
    "① 히트맵 ② 산점도 ③ 영역차트 ④ 체르노프페이스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd93df5",
   "metadata": {},
   "source": [
    "문제에서 설명하신 그래프는 데이터의 수치나 밀도를 색상의 농도로 표현하는 방식이므로 정답은 ① 히트맵입니다.\n",
    "\n",
    "💡 각 시각화 기법의 특징\n",
    "① 히트맵 (Heat Map): 데이터를 열 분포 형태의 비주얼로 변환하여, 수치에 따라 서로 다른 색상으로 표현합니다. 2차원 평면 위에 다차원 데이터를 효과적으로 나타내며 상관관계 분석에 자주 쓰입니다.\n",
    "\n",
    "② 산점도 (Scatter Plot): 두 연속형 변수 사이의 관계를 점으로 나타냅니다.\n",
    "\n",
    "③ 영역차트 (Area Chart): 선 그래프의 아래 영역에 색을 채운 것으로, 시간의 흐름에 따른 합계나 부분의 비중 변화를 볼 때 유용합니다.\n",
    "\n",
    "④ 체르노프 페이스 (Chernoff Faces): 사람 얼굴의 눈, 코, 입 등의 모양에 데이터 수치를 대입하여 다변량 데이터를 분석하는 기법입니다.\n",
    "\n",
    "정답: ① 히트맵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d056b2",
   "metadata": {},
   "source": [
    "# 79. 효과적인 인포그래픽의 조건 중 가장 적절하지 않은 것은?\n",
    "① 인포메이션(Information)과 시각적 그래프의 합성어이다. ② 최대한 많은 정보를 담는다. ③ 쉽게 이해할 수 있도록 그래픽과 텍스트를 조합해 사용한다. ④ 실용적 메시지 전달을 위해 차트, 다이어그램, 일러스트레이션 등을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ce315",
   "metadata": {},
   "source": [
    "인포그래픽(Infographic)은 복잡한 정보를 직관적으로 전달하는 것이 핵심입니다. 따라서 효과적인 인포그래픽 조건 중 가장 적절하지 않은 것은 **② 최대한 많은 정보를 담는다.**입니다.\n",
    "\n",
    "💡 왜 2번이 틀렸나요?\n",
    "인포그래픽의 본질은 **'정보의 요약과 단순화'**입니다.\n",
    "\n",
    "너무 많은 정보를 한꺼번에 담으려 하면 시각적으로 복잡해져서 오히려 핵심 메시지를 전달하기 어려워집니다.\n",
    "\n",
    "효과적인 인포그래픽은 선택과 집중을 통해 독자가 가장 알아야 할 중요한 통찰(Insight)만을 간결하게 보여주어야 합니다.\n",
    "\n",
    "🔍 효과적인 인포그래픽의 특성\n",
    "① 정보와 그래픽의 결합: 정보(Information)와 그래픽(Graphic)의 합성어로, 수치나 개념을 시각화하여 전달력을 높입니다.\n",
    "\n",
    "③ 직관적 이해: 텍스트만으로는 설명하기 힘든 관계나 흐름을 그래픽을 통해 한눈에 파악할 수 있게 합니다.\n",
    "\n",
    "④ 다양한 시각적 도구 활용: 데이터의 성격에 맞춰 차트(통계), 다이어그램(구조), 일러스트레이션(맥락) 등을 적절히 섞어 사용합니다.\n",
    "\n",
    "정답: ②"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc021438",
   "metadata": {},
   "source": [
    "# 80. 분석 모델 리모델링 및 활용 과정별 명칭과 설명에 대하여 잘못 짝지어진 것은?\n",
    "① 최적화(Optimization) : 조건 변화나 가중치 변화 시 계수값 조정 또는 제약조건 추가로 재조정하여 손실 함수를 줄인다. ② 일반화(Generalization) : 기존 데이터가 아닌 새로운 데이터를 넣으면 처음부터 학습시켜야 한다. ③ 표준화(Standardization) : 데이터 요소들을 평균이 0이고 분산이 1인 분포로 변형한다. ④ 정규화(Normalization) : 특성값의 범위를 [0, 1]로 옮긴다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3c18b",
   "metadata": {},
   "source": [
    "분석 모델의 유지보수 및 학습 과정에 대한 설명 중 잘못된 것은 **② 일반화(Generalization)**입니다.\n",
    "\n",
    "💡 상세 해설\n",
    "① 최적화(Optimization): (옳음) 모델이 예측한 값과 실제값의 차이인 손실 함수(Loss Function)를 최소화하기 위해 가중치(Weight)나 계수를 조정하는 과정입니다.\n",
    "\n",
    "② 일반화(Generalization): (틀림) 일반화는 학습 데이터가 아닌 새로운 데이터(Unseen Data)가 들어왔을 때도 모델이 정확하게 예측해내는 능력을 의미합니다. 새로운 데이터를 넣는다고 해서 처음부터 다시 학습시켜야 하는 것이 아니라, 학습된 모델이 얼마나 범용적으로 잘 작동하는지가 일반화의 핵심입니다.\n",
    "\n",
    "③ 표준화(Standardization): (옳음) 데이터의 평균을 0, 표준편차를 1로 변환하는 기법으로, Z-score Normalization이라고도 불립니다.\n",
    "\n",
    "④ 정규화(Normalization): (옳음) 데이터의 범위를 0과 1 사이로 압축하는 기법(Min-Max Scaling)을 의미하며, 서로 다른 특성(Feature)들의 단위를 맞출 때 사용합니다.\n",
    "\n",
    "🔍 일반화 능력이 중요한 이유\n",
    "모델이 학습 데이터에만 너무 과하게 최적화되면 **과적합(Overfitting)**이 발생하여 일반화 능력이 떨어집니다. 반대로 너무 단순하게 학습하면 **과소적합(Underfitting)**이 발생합니다. 데이터 분석의 궁극적인 목표는 적절한 학습을 통해 높은 일반화 성능을 갖춘 모델을 만드는 것입니다.\n",
    "\n",
    "정답: ②"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee400a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
