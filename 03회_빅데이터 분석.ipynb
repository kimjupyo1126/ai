{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44c53dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:99% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
       "div.text_cell_render.rendered_html{font-size:20pt;}\n",
       "div.text_cell_render li, div.text_cell_render p, code{font-size:22pt; line-height:30px;}\n",
       "div.output {font-size:24pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:24pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
       "table.dataframe{font-size:24px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:99% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
    "div.text_cell_render.rendered_html{font-size:20pt;}\n",
    "div.text_cell_render li, div.text_cell_render p, code{font-size:22pt; line-height:30px;}\n",
    "div.output {font-size:24pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:24pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
    "table.dataframe{font-size:24px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2713e",
   "metadata": {},
   "source": [
    "# 01. 다음 중 빅데이터의 특징인 3V에 해당하지 않는 것을 고르시오.\n",
    "① 규모(Volume) ② 다양성(Variety) ③ 속도(Velocity) ④정확성(Validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a2440",
   "metadata": {},
   "source": [
    "정답은 ④정확성(Validity)입니다. (제시된 선택지에 번호 중복이 있으나, 마지막 항목을 의미합니다.)\n",
    "\n",
    "규모 (Volume): 데이터의 양이 물리적으로 매우 거대함을 의미합니다. 테라바이트(TB)나 페타바이트(PB) 단위를 넘어설 정도로 방대한 양의 데이터를 다룹니다.\n",
    "\n",
    "다양성 (Variety): 데이터의 형태가 정형화된 수치 데이터뿐만 아니라 텍스트, 이미지, 영상, SNS 메시지 등 비정형 데이터까지 포함하여 매우 다양해짐을 의미합니다.\n",
    "\n",
    "속도 (Velocity): 데이터가 생성되고 유통되는 속도가 매우 빠르며, 이를 실시간으로 수집하고 분석하여 가치를 창출해야 함을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff6c31",
   "metadata": {},
   "source": [
    "# 02.빅데이터가 만들어낸 변화에 관한 설명 중 틀린 것을 고르시오.\n",
    "① 정해진 특정한 정보만 처리하는 것이 아닌, 가능한 많은 데이터를 모으고 그 데이터를 다양한 방식으로 조합해 숨은 정보를 찾아냄.\n",
    "② Iot/클라우드 기술의 발전으로 데이터 처리비용이 감소하게 되면서 데이터 활용방법이 표본조사에서 전수조사로 변화됬다.\n",
    "③ 수집 데이터의 양이 증가할수록 분석의 정확도가 높아져 양질의 분석결과 산출에 긍정적인 영향을 주었다.\n",
    "④ 특정한 상관관계가 중요시되던 과거와 달리, 인과관계를 통한 인사이트 도출이 점점 확산되고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa8e69",
   "metadata": {},
   "source": [
    "정답은 ④ 특정한 상관관계가 중요시되던 과거와 달리, 인과관계를 통한 인사이트 도출이 점점 확산되고 있다.\n",
    "\n",
    "과거 (인과관계 중시): 데이터의 양이 적고 처리 비용이 비쌌던 시절에는 \"왜 이런 결과가 나왔는가?\"라는 논리적 이유(원인)를 밝히는 것이 중요했습니다.\n",
    "\n",
    "현재 (상관관계 중심): 빅데이터 시대에는 \"이유는 정확히 알 수 없어도, A가 발생할 때 B도 함께 일어나는 경향이 있다\"는 패턴(상관관계)을 발견하는 것만으로도 충분히 가치 있는 예측을 할 수 있게 되었습니다.\n",
    "\n",
    "예: 맥주와 기저귀가 함께 팔리는 정확한 인과관계보다는, 두 상품이 같이 팔린다는 상관관계를 파악해 매대에 배치하는 것이 빅데이터의 핵심 활용 사례입니다.\n",
    "\n",
    "데이터 조합: 빅데이터는 단일 목적의 데이터 활용을 넘어, 서로 다른 성격의 데이터를 융합(Mash-up)하여 새로운 가치를 찾아내는 데 최적화되어 있습니다.\n",
    "\n",
    "표본조사 → 전수조사: 과거에는 기술적 한계로 일부만 추출(표본)해 검사했지만, 이제는 IT 기술의 발전으로 전체 데이터(전수)를 분석하는 것이 가능해졌습니다.\n",
    "\n",
    "데이터의 양과 질: 분석 대상이 되는 데이터의 규모가 커질수록(Big Data), 노이즈가 섞이더라도 전체적인 경향성을 파악하기 쉬워져 분석의 정확도와 품질이 향상됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ece88c",
   "metadata": {},
   "source": [
    "# 03. 다음 중 데이터 과학자의 소양으로 옳지 않은 것은?\n",
    "① 데이터 자동화 프로그램을 개발한다. ② 분석대상이 되는 비즈니스 영역에 대해 설득력 있게 전달한다. ③ 분석 인사이트를 기반으로 최적 분석 설계 및 노하우를 제공한다. ④ 데이터에 대한 이해를 통해 적절한 방법론을 제시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80e22c",
   "metadata": {},
   "source": [
    "정답: ① 데이터 자동화 프로그램을 개발한다.\n",
    "\n",
    "①번이 정답인 이유 (오답인 이유): 데이터 자동화 프로그램을 개발하거나 시스템 아키텍처를 구축, 관리하는 것은 주로 **데이터 엔지니어(Data Engineer)**의 영역입니다. 데이터 과학자는 만들어진 환경 위에서 데이터를 분석하고 가치를 창출하는 데 집중합니다.\n",
    "\n",
    "②번 (소프트 스킬): 데이터 과학자는 분석 결과를 의사결정자에게 전달해야 하므로, 비즈니스 영역에 대한 통찰력과 이를 설득력 있게 전달하는 커뮤니케이션 및 스토리텔링 능력이 필수적입니다.\n",
    "\n",
    "③번 & ④번 (하드 스킬): 통계학, 머신러닝 등 학문적 지식을 바탕으로 데이터의 특성을 파악하고, 문제 해결을 위한 최적의 분석 모델과 방법론을 설계하는 것은 데이터 과학자의 핵심 역량입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67139d",
   "metadata": {},
   "source": [
    "# 04. 하둡 기반의 대용량 데이터의 분산분석을 지원하는 플랫폼 ETL과 Low-Latency 지원, Long Term Query 및 AD Hoc Query 지원하는 프로젝트는? \n",
    "① Tajo ② Pig ③ Oozie ④ Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d85aae",
   "metadata": {},
   "source": [
    "정답: ① Tajo (타조)\n",
    "\n",
    "왜 Tajo인가요?\n",
    "\n",
    "Tajo는 우리나라에서 주도하여 개발한 아파치 오픈소스 프로젝트입니다.\n",
    "\n",
    "하둡의 HDFS 데이터를 SQL을 사용하여 분석할 수 있게 해주며, 특히 Low-Latency(저지연) 응답을 지원하도록 설계되었습니다.\n",
    "\n",
    "장시간 실행되는 Long-Term Query와 필요할 때마다 즉석에서 실행하는 Ad-Hoc Query(즉석 질의) 모두를 지원하는 것이 큰 특징입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cec5e7",
   "metadata": {},
   "source": [
    "# 05. 전통적 머신러닝과 비교해서 빅데이터를 활용한 머신러닝의 특징으로 옳지 않은 것은?\n",
    "① 빅데이터 플랫폼 기반의 풍부한 데이터를 활용할 수 있다. ② 데이터의 특징을 파악하여 모델에 최적화하는 단계가 정밀해졌다. \n",
    "③ GPU 및 클라우드 환경이 적용되어 연산속도가 빨라졌다. ④ 사람의 개입이 더 늘어났다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b331b",
   "metadata": {},
   "source": [
    "정답: ④ 사람의 개입이 더 늘어났다.\n",
    "    \n",
    "상세 해설\n",
    "이 문제는 전통적인 머신러닝 방식과 현대의 빅데이터/딥러닝 기반 머신러닝의 차이점을 묻는 문제입니다.\n",
    "\n",
    "④번이 정답인 이유 (오답인 이유): 전통적인 머신러닝에서는 분석가(사람)가 데이터의 특징(Feature)을 직접 고르고 가공하는 '특징 공학(Feature Engineering)' 단계에 많은 개입을 해야 했습니다. 하지만 빅데이터와 딥러닝 기술이 발전하면서, 알고리즘이 스스로 데이터의 특징을 찾아내고 학습하는 방향으로 진화했습니다. 따라서 사람의 개입은 오히려 줄어들고 자동화되는 것이 특징입니다.\n",
    "\n",
    "①번 (데이터의 양): 과거에는 샘플링된 소량의 데이터로 학습했지만, 이제는 하둡이나 스파크 같은 빅데이터 플랫폼을 통해 방대한 양의 데이터를 학습에 활용할 수 있게 되었습니다.\n",
    "\n",
    "②번 (최적화와 정밀도): 데이터의 양이 많아지고 분석 기술이 발달함에 따라, 과거보다 모델의 복잡도를 높이고 세밀하게 튜닝하여 예측의 정밀도를 높이는 것이 가능해졌습니다.\n",
    "\n",
    "③번 (인프라의 발전): 방대한 데이터를 빠르게 처리하기 위해 GPU(그래픽 처리 장치)의 병렬 연산과 언제 어디서든 자원을 끌어 쓸 수 있는 클라우드 컴퓨팅 환경이 결합되어 분석 속도가 비약적으로 향상되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9782eda",
   "metadata": {},
   "source": [
    "# 06. 개인정보보호법의 개인정보보호 원칙은 OECD의 프라이버시 8원칙을 참고하여 반영하고 있다. 다음 중 OECD 8원칙에 대한 설명이 잘못된 것은?\n",
    "① 수집 제한의 원칙 : 사생활 침해를 최소화하는 방법으로 처리하며 익명처리의 원칙에 해당한다. ② 목적 명확화의 원칙 : 목적 범위 내에서 적법하게 처리, 목적 외 활용을 금지한다. ③ 처리방침 공개 원칙 : 개인정보처리방침 등을 공개한다. ④ 책임의 원칙 : 개인정보처리자의 책임 준수, 신뢰 확보를 위해 노력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006f360",
   "metadata": {},
   "source": [
    "제시해주신 6번 문제의 정답과 상세 해설입니다. 이 문제는 정보보호 관련 시험에서 매우 자주 출제되는 OECD 프라이버시 8원칙의 정확한 정의를 묻고 있습니다.\n",
    "\n",
    "정답: ① 수집 제한의 원칙 : 사생활 침해를 최소화하는 방법으로 처리하며 익명처리의 원칙에 해당한다.\n",
    "상세 해설\n",
    "①번이 정답인 이유 (잘못된 설명):\n",
    "\n",
    "수집 제한의 원칙(Collection Limitation Principle): 데이터 수집 시 적법하고 공정한 수단에 의해야 하며, 정보 주체(본인)에게 알리거나 동의를 얻어야 한다는 원칙입니다.\n",
    "\n",
    "선택지에 나온 '사생활 침해 최소화'나 '익명 처리'는 우리나라 개인정보 보호법 제3조(개인정보 보호 원칙)에 명시된 개인정보 최소수집의 원칙이나 익명처리의 원칙에 더 가까운 설명이며, OECD 8원칙 중 '수집 제한'의 핵심 정의와는 거리가 있습니다.\n",
    "\n",
    "②번 (목적 명확화의 원칙): 데이터를 수집할 때 이용 목적을 명확히 하고, 그 이후의 사용은 수집 목적을 달성하는 데 한정되어야 한다는 원칙으로 올바른 설명입니다.\n",
    "\n",
    "③번 (공개의 원칙): 개인정보의 존재, 이용 목적, 관리자의 신원 등을 쉽게 알 수 있도록 정책을 공개해야 한다는 원칙입니다.\n",
    "\n",
    "④번 (책임의 원칙): 개인정보 관리자는 위 원칙들이 지켜지도록 필요한 조치를 취하고 책임을 져야 한다는 원칙입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a5262",
   "metadata": {},
   "source": [
    "# 07. 빅데이터의 활용과 관련된 법률에 대한 다음 설명 중 연결이 잘못된 것은? \n",
    "① 신용정보법 – 신용정보업의 건전한 육성 및 신용질서 확립 ② 전자서명법 – 전자서명 기본 사항 규정으로 전자문서 활성화 ③ GDPR – 유럽연합 회원국별로 차등 적용되는 개인정보보호의 특별법 ④ 지능정보화기본법 – 지능정보화 정책 수립 및 지능정보사회 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476fdd48",
   "metadata": {},
   "source": [
    "정답: ③ GDPR – 유럽연합 회원국별로 차등 적용되는 개인정보보호의 특별법\n",
    "해설\n",
    "GDPR (General Data Protection Regulation): 유럽연합(EU)의 일반 개인정보보호법으로, 회원국별로 차등 적용되는 것이 아니라 모든 회원국에 직접적이고 동일하게 적용되는 법적 구속력을 가진 규칙입니다. 특별법이 아닌 일반법 성격을 띠며, EU 내 데이터 보호 표준을 하나로 통합하는 것을 목적으로 합니다.\n",
    "\n",
    "신용정보법: 신용정보업을 건전하게 육성하고 신용정보의 효율적 이용과 체계적 관리를 통해 건전한 신용질서를 확립하는 것을 목적으로 합니다. (옳은 설명)\n",
    "\n",
    "전자서명법: 전자서명에 관한 기본 사항을 정하여 전자서명의 안전성과 신뢰성을 확보하고 전자문서의 이용을 활성화하는 데 기여합니다. (옳은 설명)\n",
    "\n",
    "지능정보화기본법: 지능정보화 정책의 수립 및 시행을 통해 지능정보사회의 기반을 조성하고 국가 경쟁력을 강화하는 것을 목적으로 합니다. (옳은 설명)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af1e54",
   "metadata": {},
   "source": [
    "# 08. 다음에서 설명하고 있는 개인정보보호법의 정의는 무엇인가?\n",
    "\n",
    "추가 정보의 사용/결합 없이는 특정 개인을 알아볼 수 없는 정보이며, ( )의 처리를 위해 개인정보의 일부를 삭제하거나 일부 또는 전부를 대체하는 등의 방법으로 추가 정보 없이는 특정 개인을 알아볼 수 없도록 처리한다.\n",
    "\n",
    "① 고유식별정보 ② 가명정보 ③ 익명정보 ④ 민감정보"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6745ccfe",
   "metadata": {},
   "source": [
    "정답: ② 가명정보\n",
    "해설\n",
    "가명정보(Pseudonymized Data): 개인정보의 일부를 삭제하거나 대체하여 추가 정보 없이는 특정 개인을 알아볼 수 없도록 처리한 정보를 의미합니다. 이는 통계 작성, 과학적 연구, 공익적 기록 보존 등을 위해 정보 주체의 동의 없이도 활용할 수 있도록 데이터 3법 개정을 통해 도입되었습니다.\n",
    "\n",
    "고유식별정보: 주민등록번호, 여권번호 등 개인을 고유하게 구분하기 위해 부여된 정보를 말합니다.\n",
    "\n",
    "익명정보: 다른 정보를 더하더라도 더 이상 특정 개인을 알아볼 수 없게 처리된 정보입니다. 가명정보와 달리 추가 정보가 있어도 재식별이 불가능합니다.\n",
    "\n",
    "민감정보: 사상, 신념, 정치적 견해, 건강, 성생활 등 개인의 사생활을 현저히 침해할 우려가 있는 정보를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307cc6d",
   "metadata": {},
   "source": [
    "# 09. 다음 설명으로 옳은 것은?\n",
    "\n",
    "정보주체인 개인이 '정보이동권(Right to Data Portability)'에 근거하여 본인 데이터에 대한 개방을 요청하면, 기업이 보유한 데이터를 개인(요청자) 또는 개인이 지정한 제3자에게 개방하도록 하는 것\n",
    "\n",
    "① 마이데이터 ② 개인정보보호법 ③ 가명처리 ④ 데이터 3법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f430d7b",
   "metadata": {},
   "source": [
    "정답: ① 마이데이터\n",
    "해설\n",
    "마이데이터(MyData): 정보주체인 개인이 자신의 데이터를 본인 혹은 본인이 지정한 제3자(다른 기업이나 기관 등)에게 전송하도록 요구할 수 있는 **개인정보 전송요구권(정보이동권)**을 핵심으로 하는 서비스입니다. 이를 통해 개인은 여러 곳에 흩어진 자신의 정보를 한곳에 모아 관리하거나, 자신에게 최적화된 맞춤형 서비스를 제공받을 수 있습니다.\n",
    "\n",
    "개인정보보호법: 개인정보의 처리 및 보호에 관한 사항을 정하여 개인의 자유와 권리를 보호함을 목적으로 하는 법률입니다.\n",
    "\n",
    "가명처리: 개인정보의 일부를 삭제하거나 대체하여 추가 정보 없이는 특정 개인을 알아볼 수 없도록 처리하는 것을 말합니다.\n",
    "\n",
    "데이터 3법: 개인정보보호법, 정보통신망법, 신용정보법의 개정안을 통칭하며, 가명정보 도입과 마이데이터의 법적 근거를 마련한 법안들입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903b0de",
   "metadata": {},
   "source": [
    "# 10. 데이터 분석 마스터 플랜의 수행순서로 적합한 것은? \n",
    "① 분석과제 도출 → 우선순위 평가 → 우선순위 정렬 → 중장기 분석 로드맵 수립 ② 분석과제 도출 → 우선순위 정렬 → 우선순위 평가 → 중장기 분석 로드맵 수립 ③ 분석대상 수행계획 수립 → 우선순위 평가 → 단기적 세부 이행 계획 → 중장기 분석 로드맵 수립 ④ 분석대상 수행계획 수립 → 단기적 세부 이행 계획 → 우선순위 평가 → 중장기 분석 로드맵 수립"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f90dc5c",
   "metadata": {},
   "source": [
    "정답: ① 분석과제 도출 → 우선순위 평가 → 우선순위 정렬 → 중장기 분석 로드맵 수립\n",
    "해설\n",
    "데이터 분석 마스터 플랜(Master Plan)을 수립하는 일반적인 절차는 다음과 같은 단계로 진행됩니다:\n",
    "\n",
    "분석과제 도출: 비즈니스 모델 및 전략 목표를 달성하기 위해 필요한 분석 과제들을 발굴합니다.\n",
    "\n",
    "우선순위 평가: 도출된 과제들을 전략적 중요성, 비즈니스 성과(ROI), 실행 가능성 등의 기준에 따라 평가합니다.\n",
    "\n",
    "우선순위 정렬: 평가 결과를 바탕으로 어떤 과제를 먼저 수행할지 최종 순위를 확정합니다.\n",
    "\n",
    "중장기 분석 로드맵 수립: 결정된 우선순위에 따라 단기, 중기, 장기적으로 추진할 분석 과제의 이행 계획을 로드맵 형태로 구성합니다.\n",
    "\n",
    "오답 분석\n",
    "②번: 우선순위를 정렬하기 위해서는 먼저 각 과제에 대한 평가가 선행되어야 하므로 순서가 바뀌었습니다.\n",
    "\n",
    "③, ④번: 마스터 플랜의 시작은 분석 대상의 수행 계획을 수립하는 것이 아니라, 먼저 어떤 과제를 할 것인지 도출하는 것부터 시작됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b9573",
   "metadata": {},
   "source": [
    "# 11. 빅데이터 분석 기획에서 다음이 설명하는 단계를 선택하시오.\n",
    "\n",
    "업무별 분석요건을 상세히 정의하고 분석을 통해 개선사항 도출. 분석요건별 문제점에 따른 이슈와 개선 목표 사이의 갭(Gap) 분석을 진행\n",
    "\n",
    "① 도메인 이슈 도출 ② 분석 목표 수립 ③ 프로젝트 계획 ④ 보유 데이터 자산 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d87194",
   "metadata": {},
   "source": [
    "정답: ② 분석 목표 수립\n",
    "해설\n",
    "분석 기획 단계 중 분석 목표 수립 단계는 도출된 분석 기획안을 바탕으로 구체적인 목표를 설정하는 과정입니다.\n",
    "\n",
    "요건 정의 및 개선사항 도출: 업무별로 구체적인 분석 요건을 상세히 정의하고, 분석을 통해 달성하고자 하는 비즈니스 개선 사항을 명확히 합니다.\n",
    "\n",
    "갭(Gap) 분석: 현재의 상태(As-Is)에서 발생하는 문제점(이슈)과 우리가 도달하고자 하는 목표 상태(To-Be) 사이의 차이를 분석하여, 이를 어떻게 해소할지 전략을 세우는 핵심 활동이 이 단계에서 이루어집니다.\n",
    "\n",
    "다른 보기 설명\n",
    "① 도메인 이슈 도출: 비즈니스 영역의 전반적인 현황을 파악하고 해결해야 할 근본적인 과제를 찾아내는 초기 단계입니다.\n",
    "\n",
    "③ 프로젝트 계획: 분석 목표가 수립된 후, 이를 실행하기 위한 일정, 인력, 예산 등 구체적인 수행 방안을 수립하는 단계입니다.\n",
    "\n",
    "④ 보유 데이터 자산 확인: 분석에 필요한 데이터가 사내외에 존재하는지, 데이터의 품질과 양은 충분한지 등을 점검하는 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d224041",
   "metadata": {},
   "source": [
    "# 12. 빅데이터 분석절차는 빅데이터 분석방법론을 토대로 5단계 절차로 수행된다. 다음 중 빅데이터 분석절차로 가장 알맞은 것은? \n",
    "① 분석 기획 → 데이터 준비 → 시스템 구현 → 데이터 분석 → 평가 및 전개 ② 분석 기획 → 데이터 분석 → 시스템 구현 → 데이터 준비 → 평가 및 전개 ③ 데이터 준비 → 분석 기획 → 데이터 분석 → 시스템 구현 → 평가 및 전개 ④ 분석 기획 → 데이터 준비 → 데이터 분석 → 시스템 구현 → 평가 및 전개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffbd54a",
   "metadata": {},
   "source": [
    "정답: ④ 분석 기획 → 데이터 준비 → 데이터 분석 → 시스템 구현 → 평가 및 전개\n",
    "해설\n",
    "빅데이터 분석 방법론에 따른 표준적인 5단계 절차는 다음과 같습니다:\n",
    "\n",
    "분석 기획(Planning): 비즈니스 이해 및 도메인 문제 파악을 통해 분석 목표를 설정하고 프로젝트 계획을 수립합니다.\n",
    "\n",
    "데이터 준비(Preparing): 분석에 필요한 데이터를 식별하고, 수집 및 정제하여 분석 가능한 형태로 준비합니다.\n",
    "\n",
    "데이터 분석(Analyzing): 준비된 데이터를 바탕으로 탐색적 분석(EDA)을 실시하고 모델링을 통해 인사이트를 도출합니다.\n",
    "\n",
    "시스템 구현(Developing): 분석 결과를 실제 업무에 적용하기 위해 시스템화하거나 모델을 배포합니다.\n",
    "\n",
    "평가 및 전개(Deploying): 프로젝트 성과를 평가하고 최종 결과물을 현업에 적용하며 유지보수 계획을 수립합니다.\n",
    "\n",
    "오답 분석\n",
    "①번: 시스템 구현이 데이터 분석보다 먼저 올 수 없습니다. 분석이 완료되어야 그 결과를 시스템에 반영할 수 있기 때문입니다.\n",
    "\n",
    "②번: 분석을 하기 위해서는 반드시 데이터를 먼저 준비해야 하므로 순서가 잘못되었습니다.\n",
    "\n",
    "③번: 모든 프로젝트의 시작은 무엇을 왜 할 것인지 정하는 기획 단계부터입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10957bda",
   "metadata": {},
   "source": [
    "# 13. 다음 중 빅데이터 분석 단계에서 수행하는 내용으로 적절한 것을 모두 고른 것은?\n",
    "\n",
    "가. 필요 데이터의 정의 나. 분석용 데이터 준비 다. 텍스트 분석 라. 탐색적 분석 마. 모델링\n",
    "\n",
    "① 가, 나 ② 가, 다 ③ 나, 다, 라 ④ 나, 다, 라, 마"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96259416",
   "metadata": {},
   "source": [
    "정답: ④ 나, 다, 라, 마\n",
    "해설\n",
    "빅데이터 분석 방법론의 분석(Analyzing) 단계는 준비된 데이터를 활용하여 실제 인사이트를 추출하는 과정입니다. 각 항목이 어느 단계에 속하는지 분류하면 다음과 같습니다:\n",
    "\n",
    "가. 필요 데이터의 정의: 분석 기획 단계에서 수행합니다. 어떤 데이터가 필요한지 결정하는 것은 기획의 영역입니다.\n",
    "\n",
    "나. 분석용 데이터 준비: 분석 단계의 초기 활동으로, 분석 목적에 맞게 데이터를 최종적으로 정렬하고 확인하는 과정입니다.\n",
    "\n",
    "다. 텍스트 분석: 분석 단계에서 수행하는 구체적인 분석 기법 중 하나입니다. 비정형 데이터에서 정보를 추출합니다.\n",
    "\n",
    "라. 탐색적 분석(EDA): 분석 단계의 핵심 활동으로, 데이터의 특성을 파악하고 통계적 통찰을 얻는 과정입니다.\n",
    "\n",
    "마. 모델링: 분석 단계에서 수행하며, 알고리즘을 적용하여 예측이나 분류 모델을 구축하는 단계입니다.\n",
    "\n",
    "요약\n",
    "분석 단계에서는 기획과 준비가 끝난 데이터를 가지고 **탐색(라)**하고, 필요에 따라 텍스트 분석(다) 등을 수행하며, 최종적으로 **모델링(마)**을 통해 결과를 도출하기 위해 **데이터를 최종 점검(나)**합니다. 따라서 '가'를 제외한 나머지가 정답에 해당합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b11d9",
   "metadata": {},
   "source": [
    "# 14. 데이터 분석과정의 고려사항으로 옳지 않은 것은? \n",
    "① 비즈니스 이해를 시작으로 도출하고자 하는 분석 목표를 정의한다. ② WBS를 기준으로 범위와 일정에 변경이 없는지 주기적으로 확인한다. ③ 수집 데이터 양과 분석주기 파악은 불필요하다. ④ 초기 파악된 리스크는 수시로 재평가해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4bd01",
   "metadata": {},
   "source": [
    "정답: ③ 수집 데이터 양과 분석주기 파악은 불필요하다.\n",
    "해설\n",
    "데이터 분석 프로젝트를 성공적으로 수행하기 위한 고려사항 중 데이터의 양과 분석 주기를 파악하는 것은 매우 중요한 요소입니다.\n",
    "\n",
    "수집 데이터 양 파악: 분석에 필요한 데이터의 규모를 알아야 저장 공간, 처리 성능, 분석 기법 등을 적절히 설계할 수 있습니다.\n",
    "\n",
    "분석 주기 파악: 실시간 분석이 필요한지, 배치(Batch) 단위의 주기적 분석이 필요한지에 따라 시스템의 아키텍처와 운영 방식이 완전히 달라지기 때문에 반드시 파악해야 합니다.\n",
    "\n",
    "다른 보기 설명\n",
    "① 비즈니스 이해와 분석 목표 정의: 프로젝트의 가장 첫 단계로, 무엇을 해결하고자 하는지 명확히 정의하는 것은 필수적입니다.\n",
    "\n",
    "② WBS(Work Breakdown Structure) 관리: 작업 분할 구조도(WBS)를 통해 프로젝트의 범위와 일정을 지속적으로 모니터링하고 관리해야 지연을 방지할 수 있습니다.\n",
    "\n",
    "④ 리스크 재평가: 분석 프로젝트는 데이터의 품질이나 모델의 성능 등 불확실성이 크기 때문에, 초기 리스크뿐만 아니라 진행 과정에서 발생하는 리스크를 수시로 점검해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cf0f6d",
   "metadata": {},
   "source": [
    "# 15. 재현 자료(Synthetic data)에 대한 설명으로 옳은 것은? \n",
    "① 원본과 통계적으로 유사하나 가상으로 다시 만들어진 데이터이다. ② 완전 재현 데이터는 민감한 정보에 대해서만 재현 데이터로 대체하는 방식이다. ③ 부분 재현 데이터는 정보 모두를 재현 데이터로 생성하는 방식이다. ④ 암호화 상태에서 데이터를 결합하고 연산/분석 등이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b94879",
   "metadata": {},
   "source": [
    "정답: ① 원본과 통계적으로 유사하나 가상으로 다시 만들어진 데이터이다.\n",
    "해설\n",
    "재현 자료(Synthetic data): 실제 데이터가 가진 통계적 특성(분포, 상관관계 등)을 유지하면서 수학적 알고리즘을 통해 가상으로 생성해낸 데이터를 의미합니다. 실제 개인정보 노출 위험을 최소화하면서 분석이나 연구에 활용할 수 있다는 장점이 있습니다.\n",
    "\n",
    "오답 분석\n",
    "②번: 민감한 정보뿐만 아니라 모든 데이터를 가상의 데이터로 대체하는 방식이 '완전 재현 데이터'에 가깝습니다.\n",
    "\n",
    "③번: 모든 정보를 생성하는 것이 아니라, 식별 위험이 있는 특정 변수(칼럼)만 선택적으로 가상 데이터로 생성하여 원본과 결합하는 방식이 '부분 재현 데이터'입니다.\n",
    "\n",
    "④번: 이 설명은 재현 자료가 아니라 동형암호(Homomorphic Encryption) 기술에 대한 설명입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fcfac",
   "metadata": {},
   "source": [
    "# 16. 다음 중 데이터 비식별화 방법에 대한 설명으로 가장 적절하지 않은 것은?\n",
    "① 데이터 마스킹 : 개인식별이 가능한 데이터에 직접적으로 식별할 수 없는 다른 값으로 대체 ② 데이터 삭제 : 개인정보 식별이 가능한 특정 데이터값을 삭제 ③ 데이터 범주화 : 단일 식별 정보를 해당 그룹의 대표값으로 변환하거나 구간값으로 변환 ④ 총계처리 : 개인정보에 대해 통계값을 적용해 특정 개인을 판단할 수 없도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14612c9b",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.\n",
    "\n",
    "정답: ① 데이터 마스킹 : 개인식별이 가능한 데이터에 직접적으로 식별할 수 없는 다른 값으로 대체\n",
    "해설\n",
    "데이터 비식별화 기술 중 데이터 마스킹과 **가명처리(치환)**는 서로 다른 개념입니다.\n",
    "\n",
    "데이터 마스킹(Data Masking): 데이터의 전부 또는 일부분을 *특수문자(예: , #) 등으로 가려서 보이지 않게 처리하는 방법입니다. (예: 홍길동 → 홍*동)\n",
    "\n",
    "가명처리/치환(Substitution): 질문의 ①번 설명처럼 식별 가능한 데이터를 전혀 다른 임의의 값으로 대체하는 것은 보통 '치환'이나 '가명처리'의 범주에 해당합니다.\n",
    "\n",
    "다른 보기 설명 (적절한 설명)\n",
    "② 데이터 삭제(Data Suppression): 개인 식별에 핵심적인 데이터 값 자체를 완전히 삭제하여 식별 위험을 제거하는 방법입니다. (예: 상세 주소 삭제)\n",
    "\n",
    "③ 데이터 범주화(Data Categorization/Aggregation): 정확한 수치나 고유 정보를 범위(구간)나 그룹의 대표값으로 변환하는 방법입니다. (예: 28세 → 20대)\n",
    "\n",
    "④ 총계처리(Data Aggregation): 개인 단위의 데이터를 보여주지 않고 집합물에 대한 **통계값(합계, 평균 등)**만 보여주어 특정 개인의 정보를 추론할 수 없게 하는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e0ec8",
   "metadata": {},
   "source": [
    "# 17. 다음 중 민감정보가 아닌 것을 고르시오.\n",
    "① 종교 ② 신용등급 ③ 정치성향 ④ 인종정보"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8273c7dd",
   "metadata": {},
   "source": [
    "정답: ② 신용등급\n",
    "해설\n",
    "개인정보보호법상 민감정보는 사생활을 현저히 침해할 우려가 있는 정보를 의미하며, 신용등급은 여기에 해당하지 않습니다.\n",
    "\n",
    "민감정보의 범위: 사상·신념, 노동조합·정당의 가입·탈퇴, 정치적 견해(정치성향), 건강, 성생활 등에 관한 정보, 그리고 유전정보, 인종·민족 정보, 범죄경력자료 등이 포함됩니다. 따라서 ① 종교, ③ 정치성향, ④ 인종정보는 모두 민감정보에 해당합니다.\n",
    "\n",
    "신용등급의 성격: 신용등급은 개인의 경제적 신용 상태를 나타내는 정보로, 보통 개인신용정보로 분류되어 관리됩니다. 이는 민감정보보다는 일반 개인정보 중에서도 경제적 가치가 있는 정보의 범주에 속합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af48e7",
   "metadata": {},
   "source": [
    "# 18. 전통적 분석 환경에서의 ETL에 대한 설명으로 알맞지 않은 것은? \n",
    "① ETL은 Extract, Transform, Load 세 단어의 축약어로 데이터 소스시스템 및 환경으로부터 비즈니스 데이터로 변환 후 데이터 마트, 데이터 웨어하우스, ODS로 적재한다. ② ETL 구현을 위해 일괄 ETL(Batch ETL) 실시간 ETL(Real Time ETL)로 구분할 수 있다. ③ 대용량 데이터 처리를 위해 MPP(작업 단계에서 다수의 프로세서가 동시 처리할 수 있게 하는 병렬처리 시스템)를 지원한다. ④ ETL은 중간 단계에 저장하는 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e7b74",
   "metadata": {},
   "source": [
    "정답: ④ ETL은 중간 단계에 저장하는 역할을 한다.\n",
    "해설\n",
    "ETL의 본질: ETL은 데이터를 추출(Extract), 변환(Transform), **적재(Load)**하는 **프로세스(과정)**를 의미합니다. 데이터를 어딘가에 저장하는 물리적인 공간이나 역할은 주로 **ODS(Operational Data Store)**나 Staging Area가 담당하며, ETL 자체는 데이터를 옮기고 가공하는 일련의 흐름입니다.\n",
    "\n",
    "다른 보기 설명 (옳은 설명)\n",
    "① ETL의 정의 및 적재 대상: ETL은 원천 시스템에서 데이터를 가져와 비즈니스 목적에 맞게 변환한 후, 데이터 마트(DM), 데이터 웨어하우스(DW), ODS 등에 적재하는 핵심 기술입니다.\n",
    "\n",
    "② 구현 방식의 구분: 데이터 처리 시점에 따라 정해진 시간에 한꺼번에 처리하는 일괄(Batch) ETL과 데이터가 발생하는 즉시 처리하는 실시간(Real Time) ETL로 구분하여 운영할 수 있습니다.\n",
    "\n",
    "③ MPP 지원: 전통적인 환경이라 하더라도 대용량 데이터 처리가 필요한 경우, 여러 프로세서가 동시에 작업을 분담하여 처리하는 MPP(Massively Parallel Processing) 기술을 통해 효율성을 높일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2030f47",
   "metadata": {},
   "source": [
    "# 19. 데이터 웨어하우스(DW)의 특징으로 옳지 않은 것은? \n",
    "① 주제 중심 ② 통합 구조 ③ 시계열성 ④ 휘발성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a6bbb",
   "metadata": {},
   "source": [
    "정답: ④ 휘발성\n",
    "해설\n",
    "데이터 웨어하우스(DW)의 4대 특징은 윌리엄 인몬(William H. Inmon)에 의해 정의되었으며, 휘발성이 아니라 비휘발성이 올바른 특징입니다.\n",
    "\n",
    "비휘발성 (Non-volatile): 데이터 웨어하우스의 데이터는 일단 적재되면 일괄 갱신(Batch Update) 외에는 거의 변경되거나 삭제되지 않고 읽기 전용으로 보존됩니다. 반면, 일반적인 운영 데이터베이스(OLTP)는 수시로 데이터가 변하는 휘발성 성격을 띱니다.\n",
    "\n",
    "다른 보기 설명 (DW의 주요 특징)\n",
    "① 주제 중심 (Subject-oriented): 데이터 웨어하우스는 조직의 업무 중심이 아닌, **특정 주제(고객, 제품, 매출 등)**를 중심으로 데이터를 구성하여 분석에 용이하게 합니다.\n",
    "\n",
    "② 통합 구조 (Integrated): 여러 원천 시스템(DB)에서 가져온 데이터를 동일한 형식, 단위, 명칭 등으로 일관성 있게 통합하여 저장합니다.\n",
    "\n",
    "③ 시계열성 (Time-variant): 단순 현재 데이터만 기록하는 운영 시스템과 달리, 과거부터 현재까지의 데이터를 시간의 흐름에 따라 축적하여 추세 분석이 가능하도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70254a80",
   "metadata": {},
   "source": [
    "# 20. NoSQL의 유형별 종류로 옳게 짝지어진 것을 고르시오. \n",
    "① Document-oriented : Oracle Berkeley DB, Voldmort ② Key-Value : Redis, HyperTable ③ Column-Oriented : Cassandra, Google BigTable ④ Graph : Neo4j, HBase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a126641",
   "metadata": {},
   "source": [
    "정답: ③ Column-Oriented : Cassandra, Google BigTable\n",
    "해설\n",
    "NoSQL은 데이터 저장 방식에 따라 크게 네 가지 유형으로 분류됩니다:\n",
    "\n",
    "Column-Oriented (열 지향): 데이터를 열(Column) 단위로 저장하여 대규모 데이터 처리와 확장이 용이합니다. 대표적으로 Cassandra, Google BigTable, HBase 등이 있습니다.\n",
    "\n",
    "오답 분석\n",
    "① Document-oriented: 데이터를 JSON이나 XML 같은 문서 형식으로 저장합니다. 대표적인 제품은 MongoDB, CouchDB 등입니다. (보기의 Oracle Berkeley DB와 Voldemort는 Key-Value 방식에 해당합니다.)\n",
    "\n",
    "② Key-Value: 가장 단순한 형태로 키(Key)와 값(Value)의 쌍으로 저장합니다. Redis, Voldemort, Oracle Berkeley DB 등이 대표적입니다. (보기의 HyperTable은 Column-Oriented 방식입니다.)\n",
    "\n",
    "④ Graph: 데이터 간의 관계를 노드와 에지로 표현합니다. Neo4j, AllegroGraph 등이 대표적입니다. (보기의 HBase는 Column-Oriented 방식입니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05777f4c",
   "metadata": {},
   "source": [
    "# 21. 데이터 정제에 대한 설명으로 옳지 않은 것을 고르시오. \n",
    "① 정제는 데이터 전처리의 한 과정이다. ② 결측값을 채우고 이상값을 삭제하거나 대체한다. ③ 데이터의 누락값, 불일치, 오류의 수정 및 숫자나 날짜 등의 형식에 대해 일관성 유지를 수행하게 된다. ④ 데이터 정제는 1회성으로 완료된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea517a",
   "metadata": {},
   "source": [
    "정답: ④ 데이터 정제는 1회성으로 완료된다.\n",
    "해설\n",
    "데이터 정제의 반복성: 데이터 정제는 단 한 번의 과정으로 끝나는 것이 아니라, 데이터 수집부터 분석 모델링 단계까지 지속적이고 반복적으로 수행되는 과정입니다. 분석을 진행하면서 새로운 데이터 결함이 발견되거나, 분석 목적에 맞춰 정제 기준이 변경되는 경우가 많기 때문입니다.\n",
    "\n",
    "다른 보기 설명 (옳은 설명)\n",
    "① 데이터 전처리의 과정: 데이터 정제는 수집된 원시 데이터를 분석에 적합한 형태로 가공하는 **데이터 전처리(Preprocessing)**의 핵심적인 단계입니다.\n",
    "\n",
    "② 결측값 및 이상값 처리: 값이 비어있는 **결측값(Missing Value)**을 적절한 값으로 채워 넣거나, 상식적인 범위를 벗어난 **이상값(Outlier)**을 찾아내어 삭제하거나 통계적으로 타당한 값으로 대체하는 활동을 포함합니다.\n",
    "\n",
    "③ 데이터 일관성 유지: 데이터의 누락이나 논리적 불일치, 오타 등의 오류를 수정하고, 서로 다른 원천 시스템에서 온 날짜 형식(예: YYYY-MM-DD vs DD/MM/YY)이나 숫자 단위 등을 통일하여 일관성을 확보합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3886e2e",
   "metadata": {},
   "source": [
    "# 22. PCA(주성분분석) 기법에 대한 설명으로 옳은 것은? \n",
    "① 데이터 처리 기법이다. ② 다수 변수들을 변수들 간의 상관관계를 분석하여 공통 차원들을 통해 축약한다. ③ 비유사성을 측정하여 2차원 또는 3차원 공간상에 점으로 표현한다. ④ 상관행렬과 공분산행렬을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f5d997",
   "metadata": {},
   "source": [
    "정답: ④ 상관행렬과 공분산행렬을 이용한다.\n",
    "해설\n",
    "공분산 및 상관행렬 활용: 주성분분석(PCA)은 여러 변수 사이의 분산-공분산 행렬이나 상관계수 행렬을 사용하여 고유값(Eigenvalue)과 고유벡터(Eigenvector)를 구함으로써 주성분을 추출하는 기법입니다. 변수들의 단위가 다를 때는 상관행렬을, 단위가 같을 때는 공분산행렬을 주로 사용합니다.\n",
    "\n",
    "오답 분석\n",
    "①번: PCA는 단순한 데이터 처리(정제 등) 기법이 아니라, 고차원 데이터를 저차원으로 변환하는 차원 축소(Dimension Reduction) 또는 분석 기법입니다.\n",
    "\n",
    "②번: 이 설명은 **요인분석(Factor Analysis)**에 더 가깝습니다. PCA는 변수들 간의 상관관계를 이용해 데이터의 분산을 최대한 보존하는 새로운 '주성분'을 만드는 것이 목적이며, 요인분석은 변수 뒤에 숨어있는 공통 차원(잠재 요인)을 찾아내는 것이 목적입니다.\n",
    "\n",
    "③번: 비유사성(거리)을 측정하여 저차원 공간에 점으로 표현하는 기법은 **다차원 척도법(MDS, Multidimensional Scaling)**에 대한 설명입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347eafa3",
   "metadata": {},
   "source": [
    "# 23. 다음 중 요인분석(Factor Analysis)에 해당되는 것을 모두 고르시오.\n",
    "\n",
    "가. 데이터에 관찰할 수 있는 잠재적 변수가 존재한다고 가정한다. 나. 다수 변수들을 변수 간의 상관관계를 분석하고 공통 차원으로 축약하는 통계 기법이다. 다. 상관계수 ±3을 벗어나는 자료는 부적합하다. 라. 사회과학, 설문조사 등에 많이 활용되는 기법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcdce63",
   "metadata": {},
   "source": [
    "① 가 ② 가, 나 ③ 가, 다, 라 ④ 가, 나, 다, 라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb8c02",
   "metadata": {},
   "source": [
    "정답: ② 가, 나\n",
    "해설\n",
    "가. 잠재적 변수 가정 (옳음): 요인분석은 직접 측정할 수 없는 배후의 변수인 '잠재 요인(Latent Factor)'이 존재한다고 가정하고, 관측된 변수들이 이 요인에 의해 어떻게 영향을 받는지 분석합니다.\n",
    "\n",
    "나. 공통 차원으로 축약 (옳음): 다수의 변수 간 상관관계를 분석하여 유사한 변수끼리 묶어 '공통 요인'이라는 적은 수의 차원으로 정보를 요약하고 축약하는 통계 기법입니다.\n",
    "\n",
    "오답 분석\n",
    "다. 상관계수 범위 (틀림): 상관계수는 수학적으로 -1에서 +1 사이의 값만을 가집니다. ±3이라는 수치는 존재할 수 없으므로 자료의 부적합성을 판단하는 기준이 될 수 없습니다.\n",
    "\n",
    "라. 활용 분야 (주의): 사회과학이나 설문조사 등에 매우 널리 활용되는 것은 사실이나, 문제지 상의 선택지 구성과 일반적인 데이터 분석 자격증(ADsP 등)의 기출 맥락에서는 보통 '가'와 '나'를 핵심 정의로 묶어 정답으로 채택하는 경우가 많습니다. (만약 보기 구성이 ① 가, 나, 라 였다면 정답이 될 수 있으나, 현재 구성상 가장 핵심적인 정의인 '가, 나'를 선택하는 것이 적절합니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d9ef6",
   "metadata": {},
   "source": [
    "# 24. Box-Cox 변환에 대한 설명으로 옳지 않은 것을 고르시오.\n",
    "① 변수 변환 기법이다. ② 로그변환을 이용한다. ③ 차원축소 기법이다. ④ 거듭제곱변환을 이용한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683bd367",
   "metadata": {},
   "source": [
    "정답: ③ 차원축소 기법이다.해설Box-Cox 변환의 본질: Box-Cox 변환은 정규분포를 따르지 않는 데이터를 정규분포에 가깝게 만들거나 분산을 안정화시키기 위해 사용하는 변수 변환(Transformation) 기법입니다.차원축소와의 차이: 여러 개의 변수를 적은 수의 변수로 줄이는 **차원축소(PCA, 요인분석 등)**와 달리, Box-Cox는 개별 변수의 분포 형태를 수정하는 기법입니다.다른 보기 설명 (옳은 설명)① 변수 변환 기법이다: 데이터의 스케일을 조정하거나 왜도(Skewness)를 제거하여 통계적 가정(정규성, 등분산성)을 만족시키기 위한 변환 도구입니다.② 로그변환을 이용한다: Box-Cox 변환 공식에서 파라미터 $\\lambda$가 0일 때, 이는 **로그 변환($ln(y)$)**과 동일한 연산이 됩니다.④ 거듭제곱변환을 이용한다: 데이터 $y$에 대해 $y^\\lambda$ 형태의 거듭제곱을 적용하여 최적의 정규성을 찾는 수치적 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9132e1e1",
   "metadata": {},
   "source": [
    "# 25. 변수 변환 기법 중 스케일링 기법이 아닌 것은? \n",
    "① 범주화 ② 최소-최대 정규화 ③ 표준화 ④ 최대-절대값 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da41258",
   "metadata": {},
   "source": [
    "정답: ① 범주화\n",
    "해설\n",
    "스케일링(Scaling) 기법: 변수의 단위를 무시하고 상대적인 크기를 조정하여 분석 성능을 높이는 기법을 말합니다. ②, ③, ④는 모두 수치형 데이터를 특정 범위 내로 변환하는 스케일링 기법입니다.\n",
    "\n",
    "범주화(Categorization): 연속형 수치 데이터를 특정 기준에 따라 구간을 나누어 이산적인 범주(예: 10대, 20대 등)로 변환하는 기법입니다. 이는 데이터의 스케일을 조정하는 것이 아니라 데이터의 **성격(Type)**을 변환하는 기법에 해당합니다.\n",
    "\n",
    "다른 보기 설명 (스케일링 기법 종류)\n",
    "② 최소-최대 정규화 (Min-Max Normalization): 모든 데이터를 0과 1 사이의 범위로 변환하는 기법입니다.\n",
    "\n",
    "③ 표준화 (Standardization): 데이터가 평균 0, 표준편차 1인 표준정규분포를 따르도록 변환하는 기법으로 Z-score 정규화라고도 합니다.\n",
    "\n",
    "④ 최대-절대값 정규화 (Max-Abs Scaler): 데이터의 절대값이 0과 1 사이에 오도록 최대값을 기준으로 변환하는 기법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf7e5db",
   "metadata": {},
   "source": [
    "# 26. 불균형 데이터에 대한 설명으로 옳지 않은 것은?\n",
    "① 불균형 데이터가 존재할 경우 많은 비율을 가진 집단의 정확도(Accuracy)가 높아지므로 모형의 성능 판별이 어려워지게 된다. ② 적은 비율을 가진 집단의 재현율(Recall)은 작아지는 현상이 발생할 수 있다. ③ 변수가 가진 데이터에서 각 집단에 속하는 데이터의 수가 동일하지 않은 상태이다. ④ 기존 변수에 특정 조건 혹은 함수 등을 활용하여 만들거나 기존 변수들을 조합하여 새롭게 만들어진 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3adc712",
   "metadata": {},
   "source": [
    "정답: ④ 기존 변수에 특정 조건 혹은 함수 등을 활용하여 만들거나 기존 변수들을 조합하여 새롭게 만들어진 과정이다.\n",
    "해설\n",
    "불균형 데이터(Imbalanced Data): 예측하려는 각 클래스(집단)에 속하는 데이터의 양이 현저하게 차이가 나는 상태를 의미합니다.\n",
    "\n",
    "오답 분석 (④번): ④번의 설명은 불균형 데이터에 대한 정의가 아니라, 기존의 변수들을 결합하거나 가공하여 새로운 변수를 생성하는 **변수 생성(Derived Variable, 파생 변수 생성)**에 대한 설명입니다.\n",
    "\n",
    "다른 보기 설명 (불균형 데이터의 특징)\n",
    "① 정확도(Accuracy)의 함정: 다수 클래스의 비중이 너무 크면, 모형이 무조건 다수 클래스로만 예측해도 전체 정확도가 매우 높게 나타납니다. 이로 인해 실제 모델의 변별력을 제대로 판단하기 어려워집니다.\n",
    "\n",
    "② 재현율(Recall) 저하: 데이터 수가 적은 소수 클래스에 대한 학습이 충분히 이루어지지 않아, 실제 소수 클래스 데이터를 제대로 찾아내지 못하는 재현율 저하 현상이 발생하기 쉽습니다.\n",
    "\n",
    "③ 불균형의 정의: 각 클래스(Label)별 데이터 빈도가 균일하지 않고 한쪽으로 치우친 상태 그 자체를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b021f6",
   "metadata": {},
   "source": [
    "# 27. 상관관계 분석에 대한 설명으로 가장 부적절한 것은? \n",
    "① 두 개의 변수 간의 직선관계의 선형성과 산점도로 확인할 수 있다. ② 양(+)의 상관관계는 두 변수가 동반 증가하는 것이며, 음(-)의 상관관계는 두 변수의 값이 반대로 증감하는 것이다. ③ -1 ~ +1 사이의 값으로 -1과 +1은 완전한 비선형관계를 의미한다. ④ 두 변수의 선형관계 측정을 위한 수치로 공분산과 상관계수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102de1d2",
   "metadata": {},
   "source": [
    "정답: ③ -1 ~ +1 사이의 값으로 -1과 +1은 완전한 비선형관계를 의미한다.해설상관계수의 해석: 상관계수는 $-1$에서 $+1$ 사이의 값을 가지며, 양 끝단인 $-1$과 $+1$은 두 변수 사이에 오차 없이 완벽한 선형(직선)관계가 있음을 의미합니다.오답 분석: $+1$은 완벽한 양의 선형관계, $-1$은 완벽한 음의 선형관계를 뜻하므로 **'비선형관계'**라고 설명한 ③번은 틀린 내용입니다. 상관계수가 $0$에 가까울수록 선형성이 없음을 의미합니다.다른 보기 설명 (상관관계의 특징)① 선형성과 산점도: 상관관계 분석은 두 변수가 얼마나 직선에 가깝게 분포하는지(선형성)를 파악하며, 이를 시각적으로 가장 잘 보여주는 도구가 산점도입니다.② 양과 음의 상관관계: 한 변수가 커질 때 다른 변수도 커지면 양(+)의 관계, 한 변수가 커질 때 다른 변수가 작아지면 음(-)의 관계라고 정의합니다.④ 공분산과 상관계수: 두 변수가 함께 변하는 정도를 나타내는 공분산을 각 변수의 표준편차로 나누어 정규화한 수치가 상관계수입니다. 두 수치 모두 선형관계의 강도를 측정하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac268b5b",
   "metadata": {},
   "source": [
    "# 28. 오른쪽으로 긴 꼬리를 갖는 분포의 왜도로 가장 적합한 것은? (그래프 이미지: 왼쪽이 높고 오른쪽으로 꼬리가 긴 형태) \n",
    "① 왜도 < 0 ② 왜도 = 0 ③ 왜도 > 0 ④ 왜도 < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f056a0",
   "metadata": {},
   "source": [
    "정답: ③ 왜도 > 0해설왜도(Skewness): 데이터 분포의 비대칭 정도를 나타내는 척도입니다.오른쪽으로 긴 꼬리(Positive Skew): 그래프의 봉우리가 왼쪽에 치우쳐 있고 오른쪽 방향으로 꼬리가 길게 늘어진 형태를 말합니다.수치적 특징: 이 경우 왜도 값은 **0보다 큰 값($\\text{왜도} > 0$)**을 가집니다. 또한, 대표값들 사이에 최빈값 < 중앙값 < 평균 순서의 크기 관계가 성립합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f46189",
   "metadata": {},
   "source": [
    "# 29. 중심경향치(대표값)에 대한 설명으로 옳은 것은? \n",
    "① 대표값은 평균, 표준편차, 분산으로 구분할 수 있다. ② 평균은 중앙값보다 분포 함수를 쉽게 구할 수 있다. ③ 중앙값보다 평균이 이상값에 더 민감하다. ④ 극단치가 존재하는 경우 평균이 대표값으로 합리적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef238219",
   "metadata": {},
   "source": [
    "정답: ③ 중앙값보다 평균이 이상값에 더 민감하다.\n",
    "해설\n",
    "평균과 이상값: 평균은 모든 관측값을 더한 후 개수로 나누기 때문에, 데이터 중 아주 크거나 작은 **이상값(Outlier)**이 포함되면 그 값에 의해 크게 영향을 받습니다. 반면 중앙값은 순서대로 나열했을 때 가운데 위치한 값이므로 이상값의 크기에 민감하게 반응하지 않습니다.\n",
    "\n",
    "오답 분석\n",
    "①번: 평균은 대표값이 맞지만, 표준편차와 분산은 데이터가 퍼진 정도를 나타내는 산포도(Dispersion) 지표입니다. 대표값에는 평균, 중앙값, 최빈값 등이 포함됩니다.\n",
    "\n",
    "②번: 평균과 중앙값은 데이터를 요약하는 수치일 뿐이며, 이를 통해 데이터의 전체적인 확률 분포 함수를 직접적으로 더 쉽게 구할 수 있다는 설명은 적절하지 않습니다.\n",
    "\n",
    "④번: 극단치(이상값)가 존재하는 경우에는 평균이 데이터의 중심을 왜곡할 가능성이 높습니다. 따라서 이럴 때는 평균보다 중앙값을 대표값으로 사용하는 것이 더 합리적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80490f6c",
   "metadata": {},
   "source": [
    " # 30. 다음 중 특성이 다른 하나를 고르시오. \n",
    " ① 평균 ② 사분위수 ③ 중앙값 ④ 최빈값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8d3b7",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.정답: ② 사분위수해설통계량은 데이터를 요약하는 목적에 따라 크게 **중심경향치(대표값)**와 산포도(퍼짐 정도) 등으로 나뉩니다.중심경향치 (①, ③, ④): 데이터의 중심 위치를 나타내는 값들입니다.평균(Mean): 모든 관측값의 합을 개수로 나눈 값입니다.중앙값(Median): 크기순으로 나열했을 때 정가운데 위치한 값입니다.최빈값(Mode): 가장 빈번하게 나타나는 값입니다.위치 및 산포 관련 지표 (②):사분위수(Quartile): 데이터를 4등분 하는 지점의 값을 의미합니다. 물론 제2사분위수($Q2$)가 중앙값과 같긴 하지만, 사분위수 자체는 데이터의 분포 범위나 위치 정보를 제공하는 지표로서 단일 '중심'을 나타내는 대표값들과는 그 성격이 다릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6571b",
   "metadata": {},
   "source": [
    "# 31. 다음 중 평균에 대한 설명으로 가장 옳지 않은 것은? \n",
    "① 평균은 관측치의 절대 크기의 중앙이므로 모든 관측치를 더한 값에 관측치의 개수(n)로 나눈 값을 의미한다. ② 평균값과 관측치 값이 같을 수 있다. ③ 소수의 극단치에 영향을 받지 않는다. ④ 극단치가 존재하지 않으면 평균과 중앙값은 거의 일치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad8008",
   "metadata": {},
   "source": [
    "정답: ③ 소수의 극단치에 영향을 받지 않는다.해설평균의 민감성: 산술평균은 모든 데이터를 더하여 개수로 나누는 방식이기 때문에, 데이터 세트 내에 매우 크거나 작은 **소수의 극단치(이상값, Outlier)**가 포함될 경우 그 값에 의해 결과값이 크게 왜곡됩니다.비교 지표: 반면 **중앙값(Median)**은 데이터의 크기 순서상 가운데 위치를 찾으므로 극단치에 영향을 거의 받지 않는 강건한(Robust) 성질을 가집니다.다른 보기 설명 (옳은 설명)① 평균의 정의: 산술평균에 대한 정확한 정의로, 모든 관측값의 합을 전체 개수 $n$으로 나눈 산술적 결과물입니다.② 관측값과의 일치: 데이터 세트에 들어있는 실제 관측값 중 하나가 계산된 평균값과 정확히 일치하는 경우는 흔히 발생할 수 있습니다. (예: 2, 4, 6의 평균은 4이며, 이는 관측값 중 하나임)④ 분포의 대칭성: 데이터에 극단치가 없고 좌우 대칭인 정규분포 형태를 띨수록 평균, 중앙값, 최빈값은 서로 거의 일치하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e50888",
   "metadata": {},
   "source": [
    "# 32 두 개 이상의 범주를 갖는 하나의 실험요인(독립변수)에 대한 평균 차이를 검정하는 분석기법은 무엇인가? \n",
    "① 상관관계분석 ② 일원분산분석 ③ 이원분산분석 ④ 공분산분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cfe832",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.\n",
    "\n",
    "정답: ② 일원분산분석\n",
    "해설\n",
    "일원분산분석(One-way ANOVA): **하나의 독립변수(실험요인)**가 **두 개 이상의 범주(집단)**를 가질 때, 그 집단 간의 종속변수 평균 차이가 통계적으로 유의미한지 검정하는 방법입니다.\n",
    "\n",
    "예: 학력(고졸, 대졸, 대학원졸)에 따른 월급의 평균 차이 분석\n",
    "\n",
    "다른 보기 설명\n",
    "① 상관관계분석: 두 수치형 변수 간에 밀접한 관련성이 있는지(선형적 관계)를 분석하는 기법입니다. 평균 차이를 검정하는 도구가 아닙니다.\n",
    "\n",
    "③ 이원분산분석(Two-way ANOVA): 두 개의 독립변수가 종속변수에 미치는 영향을 분석하며, 각 변수의 주효과와 두 변수 간의 상호작용 효과를 함께 검정합니다.\n",
    "\n",
    "④ 공분산분석(ANCOVA): 분산분석에 회귀분석의 원리를 결합한 것으로, 종속변수에 영향을 줄 수 있는 **연속형 외생변수(공변량)**를 통제한 상태에서 집단 간 평균 차이를 분석합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee013c",
   "metadata": {},
   "source": [
    "# 33. 다음 중 전수조사 예시로 옳은 것은? \n",
    "① 암환자 조사 ② 잠자리 수 조사 ③ 차량 안전장치 결합 조사 ④ 대통령 선거 지지율 조사"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d263d4f9",
   "metadata": {},
   "source": [
    "정답: ③ 차량 안전장치 결함 조사\n",
    "해설\n",
    "전수조사(Census): 조사 대상이 되는 집단(모집단)의 모든 개체를 일일이 조사하는 방식입니다.\n",
    "\n",
    "정답인 이유: 차량의 안전장치(에어백, 브레이크 등) 결함은 사람의 생명과 직결되는 중대한 사안입니다. 따라서 일부만 뽑아서 검사하는 표본조사가 아니라, 생산된 모든 차량을 전수조사하여 안전성을 확보해야 합니다.\n",
    "\n",
    "오답 분석 (표본조사 예시)\n",
    "① 암환자 조사: 전 세계 또는 국가 내의 모든 암환자를 실시간으로 전수조사하기에는 비용과 시간이 막대하게 소요되므로, 보통 특정 등록 사업이나 표본 집단을 통해 실태를 파악합니다.\n",
    "\n",
    "② 잠자리 수 조사: 야생 동물이나 곤충의 개체 수는 물리적으로 모든 개체를 다 세는 것이 불가능하므로, 일정 구역을 정해 조사한 뒤 전체 수를 추정하는 표본조사를 실시합니다.\n",
    "\n",
    "④ 대통령 선거 지지율 조사: 선거 전 지지율 조사는 전체 유권자(수천만 명)를 모두 조사할 수 없기 때문에, 대표성을 가진 수천 명의 표본을 추출하여 여론을 조사합니다. (참고: 실제 선거 '개표' 결과는 전수조사 결과라고 할 수 있습니다.)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee718f",
   "metadata": {},
   "source": [
    "# 34. 다음에서 설명하는 표본추출 방법은 무엇인가?\n",
    "\n",
    "서로 인접한 기본 단위들로 구성된 집단(군집)을 만들고, 추출된 집단 내의 일부 또는 전체를 조사하여 표본을 추출하는 방법 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336332e1",
   "metadata": {},
   "source": [
    "① 단순랜덤표본추출 ② 층화확률표본추출 ③ 계통표본추출 ④ 집락표본추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c9a54",
   "metadata": {},
   "source": [
    "정답: ④ 집락표본추출해설집락표본추출(Cluster Sampling): 모집단을 서로 인접한 기본 단위들로 구성된 여러 개의 **집단(군집/Cluster)**으로 나누고, 이들 집단 중 일부를 랜덤하게 선택한 뒤 선택된 집단 내의 구성원을 전수 또는 일부 조사하는 방법입니다.특징: 집단 내부는 이질적이지만 집단 간에는 동질적인 성격을 띠도록 구성하는 것이 이상적입니다. 조사 비용과 시간을 절약할 수 있다는 장점이 있습니다.다른 보기 설명① 단순랜덤표본추출(Simple Random Sampling): 모집단의 모든 구성원에게 번호를 부여하고 난수표 등을 이용해 무작위로 추출하는 가장 기본적인 방법입니다.② 층화확률표본추출(Stratified Random Sampling): 모집단을 성별, 연령 등 특정 기준에 따라 서로 겹치지 않는 **층(Strata)**으로 나눈 뒤, 각 층에서 독립적으로 표본을 추출하는 방법입니다. 집단 내부가 동질적일 때 효과적입니다.③ 계통표본추출(Systematic Sampling): 모집단 목록에서 첫 번째 표본을 랜덤하게 뽑은 후, 일정한 **간격($k$번째)**마다 표본을 추출하는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e3f07",
   "metadata": {},
   "source": [
    "# 35. 만약 X가 팝콘 기계가 고장 나는 횟수를 나타내고, Y가 음료수 기계가 고장 나는 횟수를 나타내면, $X \\sim Po(3.4)$ 이고 $Y \\sim Po(2.3)$ 일 때, $X+Y$의 분포를 구하시오.\n",
    "① $X+Y \\sim Po(5.7)$ ② $X-Y \\sim Po(1.1)$ ③ $X+Y \\sim Po(1.1)$ ④ $X-Y \\sim Po(5.7)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9587e366",
   "metadata": {},
   "source": [
    "정답: ① $X+Y \\sim Po(5.7)$해설이 문제는 포아송 분포의 적률생성함수(MGF)의 성질 또는 **재생성(Reproducibility)**에 관한 기본 개념을 묻는 문제입니다.포아송 분포의 재생성: 서로 독립인 두 확률변수 $X$와 $Y$가 각각 파라미터 $\\lambda_1, \\lambda_2$를 갖는 포아송 분포를 따를 때, 두 변수의 합인 $X+Y$는 파라미터의 합인 $\\lambda_1 + \\lambda_2$를 갖는 포아송 분포를 따릅니다.문제 적용:$X \\sim Po(3.4)$ 이고 $Y \\sim Po(2.3)$ 이므로,$X+Y$의 새로운 파라미터 $\\lambda = 3.4 + 2.3 = 5.7$ 이 됩니다.따라서 $X+Y \\sim Po(5.7)$ 이라는 결론을 얻을 수 있습니다.오답 분석②, ③번: 두 변수의 차($X-Y$)는 일반적으로 포아송 분포를 따르지 않으며(스켈람 분포를 따름), 합의 기댓값을 단순히 뺀 결과는 포아송 분포의 정의에 맞지 않습니다.④번: $X-Y$가 포아송 분포를 따른다는 가정 자체가 잘못되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ae928",
   "metadata": {},
   "source": [
    "# 36. 정규분포에 대한 설명 중 가장 알맞지 않은 것은 무엇인가?\n",
    "① 직선 $x=\\mu$(평균)에 대하여 대칭인 종 모양의 곡선이다.\n",
    "② 곡선과 x축으로 둘러싸인 영역의 넓이는 1이다(확률의 총합은 100%이다).\n",
    "③ 곡선의 모양은 평균이 일정할 때, 표준편차가 작아지면 가운데로 밀집하고 표준편차가 커지면 양쪽으로 퍼진다.\n",
    "④ 곡선의 모양은 표준편차가 일정할 때, 평균이 변하면 대칭축의 위치와 곡선의 모양이 바뀐다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea416365",
   "metadata": {},
   "source": [
    "정답: ④ 곡선의 모양은 표준편차가 일정할 때, 평균이 변하면 대칭축의 위치와 곡선의 모양이 바뀐다.해설정규분포의 형태를 결정하는 두 가지 핵심 파라미터는 **평균($\\mu$)**과 **표준편차($\\sigma$)**입니다.평균($\\mu$)의 역할: 곡선의 위치를 결정합니다. 평균이 바뀌면 그래프가 좌우로 이동할 뿐, 모양 자체는 변하지 않습니다.표준편차($\\sigma$)의 역할: 곡선의 **모양(퍼짐 정도)**을 결정합니다. 표준편차가 일정하다면 곡선의 높이나 폭은 동일하게 유지됩니다.오답 분석: ④번에서 표준편차가 일정할 때 평균이 변하면 대칭축의 위치는 바뀌는 것이 맞으나, 곡선의 모양까지 바뀐다는 설명은 틀린 내용입니다.다른 보기 설명 (옳은 설명)① 대칭성과 모양: 정규분포 $N(\\mu, \\sigma^2)$은 평균 $\\mu$를 중심으로 좌우가 대칭인 종 모양(Bell-shape)을 띱니다.② 전체 면적: 확률밀도함수의 성질에 따라 곡선과 x축 사이의 전체 넓이는 확률의 총합인 1이 됩니다.③ 표준편차와 밀집도:표준편차($\\sigma$)가 작으면: 평균 근처에 데이터가 많이 모여 곡선이 가늘고 높게 솟습니다.표준편차($\\sigma$)가 크면: 데이터가 넓게 퍼져 곡선이 낮고 완만하게 퍼집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d304b",
   "metadata": {},
   "source": [
    "# 37. 중심극한정리의 문제로 옳은 것은?\n",
    "① 모집단이 이산분포이면 표본분포는 정규분포를 이룰 수 없다. ② 한 모집단이 아닌 여러 모집단에서 뽑은 표본평균도 동일한 정규분포를 따른다. ③ 표본이 많아질수록 모집단이 한 쪽으로 쏠려 있는 표본집단은 정규분포가 된다. ④ 모집단이 정규분포를 따르지 않아도 표본집단은 정규분포에 근사한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e8ce2",
   "metadata": {},
   "source": [
    "정답: ④ 모집단이 정규분포를 따르지 않아도 표본집단은 정규분포에 근사한다.해설중심극한정리(Central Limit Theorem, CLT)의 핵심: 모집단의 분포 모양(정규분포 여부)과 상관없이, 표본의 크기($n$)가 충분히 크다면(보통 $n \\ge 30$), 표본평균들의 분포는 정규분포에 근사하게 된다는 정리입니다.의의: 이 정리를 통해 우리는 모집단의 분포를 몰라도 표본평균을 이용해 모집단의 특성을 추론(가설검정, 신뢰구간 추정 등)할 수 있게 됩니다.오답 분석①번: 모집단이 이산분포(예: 이항분포, 포아송 분포)라 하더라도 표본의 크기가 충분히 크다면 표본평균의 분포는 정규분포를 이룰 수 있습니다.②번: 여러 모집단이 아니라 동일한 모집단에서 추출한 독립적인 표본들의 평균이 정규분포를 따르는 것입니다.③번: 표본이 많아진다고 해서 **표본집단(샘플 그 자체)**이 정규분포가 되는 것이 아니라, 표본평균(Sample Mean)들의 분포가 정규분포에 가까워지는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba2593",
   "metadata": {},
   "source": [
    "# 38. 다음에서 설명하는 법칙으로 가장 적절한 것은?( )는 정확한 분포가 알려지지 않았거나 다루기 힘든 경우 근사분포를 제공하며, 추출한 표본의 $n$이 충분히 크면(일반적으로 $n \\ge 30$ 이면) 모집단 분포의 모양에 상관없이 추출된 표본들의 평균의 분포는 표준정규분포 $N(0, 1)$를 따른다는 법칙이다.\n",
    "① 경험법칙 ② 중심극한정리 ③ 정규분포의 법칙 ④ 표본분포의 법칙"
   ]
  },
  {
   "cell_type": "raw",
   "id": "601d455e",
   "metadata": {},
   "source": [
    "정답: ② 중심극한정리해설중심극한정리(Central Limit Theorem, CLT): 모집단의 분포가 무엇이든 상관없이, 표본의 크기($n$)가 충분히 크면 표본평균들의 분포가 정규분포에 가까워진다는 정리입니다.문제의 핵심:정확한 분포를 몰라도 근사분포를 제공합니다.일반적으로 **$n \\ge 30$**이면 성립합니다.모집단의 모양에 상관없이 표본평균의 분포가 정규분포(표준화하면 표준정규분포)를 따른다는 점이 가장 큰 특징입니다.오답 분석① 경험법칙(Empirical Rule): 정규분포에서 데이터가 평균으로부터 1, 2, 3표준편차 이내에 존재할 확률(68%, 95%, 99.7%)을 나타내는 규칙입니다.③ 정규분포의 법칙: 정규분포 그 자체의 성질을 의미할 뿐, 표본 추출과 관련된 근사 정리를 일컫는 용어는 아닙니다.④ 표본분포의 법칙: 표본분포는 표본통계량의 확률분포를 통칭하는 말이며, 문제에서 설명하는 구체적인 '근사 원리'는 중심극한정리가 정확한 명칭입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912984e",
   "metadata": {},
   "source": [
    "# 39. 모평균 또는 모분산에 대해 이용되는 통계량 표본인 추정량의 4가지 준거에 해당하지 않는 것은? \n",
    "① 불편성 ② 유효성 ③ 신뢰성 ④ 충분성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e63dd4",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.정답: ③ 신뢰성해설통계학에서 좋은 추정량(Best Estimator)이 갖추어야 할 **4가지 판단 기준(준거)**에는 불편성, 유효성, 일치성, 충분성이 포함됩니다. 신뢰성은 일반적인 측정의 품질을 말할 때 사용되지만, 추정량의 수리적 조건인 4대 준거에는 포함되지 않습니다.추정량의 4가지 준거 (중요 개념)① 불편성(Unbiasedness): 추정량의 기댓값이 모수의 실제 값과 일치하는 성질입니다. 즉, 편향(Bias)이 없어야 합니다.② 유효성(Efficiency): 두 개의 불편추정량이 있을 때, 분산이 더 작은 추정량이 더 유효하다고 합니다. 분산이 작을수록 추정값이 모수 주위에 밀집하기 때문입니다.④ 충분성(Sufficiency): 표본이 가진 모수에 대한 정보를 최대한 모두 활용하여 추정하는 성질입니다.일치성(Consistency) (보기에는 없지만 포함됨): 표본의 크기($n$)가 커질수록 추정량이 모수에 근사하게 되는 성질입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d89b2",
   "metadata": {},
   "source": [
    "# 40. 유의확률에 대한 설명으로 가장 알맞지 않은 것은?\n",
    "① P값은 측정된 검정통계량의 값으로 계산된 확률로서 귀무가설을 기각하게 하는 최저의 유의수준이다.\n",
    "② P값은 $\\alpha$를 사전에 설정하여 산출된 검정통계량이 기각역에 의해 귀무가설을 판단하는 경우, 제2종 오류를 범할 위험의 최대값을 확인한다.\n",
    "③ 일반적으로 P값이 0.01보다 작으면 대립가설에 대한 신뢰성이 높아지고 귀무가설에 대한 신뢰성이 올라가며 대립가설을 기각한다.\n",
    "④ P값과 주어진 유의수준 $\\alpha$값을 비교하여 귀무가설의 기각여부를 결정하며, P값이 주어진 $\\alpha$의 값보다 작으면 귀무가설을 기각, $\\alpha$값보다 크면 귀무가설을 기각하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d506a",
   "metadata": {},
   "source": [
    "정답: ③ 일반적으로 P값이 0.01보다 작으면 대립가설에 대한 신뢰성이 높아지고 귀무가설에 대한 신뢰성이 올라가며 대립가설을 기각한다.해설P값(유의확률)의 의미: 귀무가설이 맞다는 전제하에, 우리가 관측한 데이터(또는 그보다 더 극단적인 데이터)가 나타날 확률입니다.오답 분석 (③번): P값이 매우 작다(예: 0.01 미만)는 것은 귀무가설 하에서 이런 결과가 나올 확률이 희박하다는 뜻입니다. 따라서 귀무가설을 기각하고 대립가설을 채택하게 됩니다. ③번 설명처럼 \"귀무가설에 대한 신뢰성이 올라가며 대립가설을 기각한다\"는 표현은 정반대로 설명된 것입니다.다른 보기 설명① P값의 정의: 귀무가설을 기각할 수 있는 최소한의 유의수준을 의미합니다. P값이 유의수준보다 작아야 기각이 가능하기 때문입니다.② 제1종 오류와의 관계: (보기에 일부 혼동이 있을 수 있으나) 일반적으로 유의수준 $\\alpha$는 제1종 오류(귀무가설이 참인데 기각할 확률)의 최대 허용 한계이며, P값은 실제 데이터에서 나타난 제1종 오류의 수준을 보여줍니다. (보기의 '제2종 오류' 부분은 문맥상 오타일 수 있으나, P값의 메커니즘을 설명하는 과정에서 흔히 비교되는 개념입니다.)④ 기각 판정 기준: 가설검정의 가장 표준적인 규칙입니다.$P\\text{-value} < \\alpha$: 귀무가설 기각 (통계적으로 유의함)$P\\text{-value} \\ge \\alpha$: 귀무가설 기각 실패 (채택)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01ebb5",
   "metadata": {},
   "source": [
    "# 41. 다음 중 인공지능 알고리즘을 알맞게 짝지은 것은? \n",
    "가. AI스피커를 사용하여 음성 분석 나. 카메라로 사진을 찍어 이미지 분석 다. 컴퓨터 비전과 자연어 처리를 결합해 이미지와 언어 간 번역 모델 사용 라. 바둑에서 인간의 데이터를 사용하지 않고, 규칙적으로 보상을 통한 학습 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a424eda",
   "metadata": {},
   "source": [
    "① RNN, CNN, RNN&CNN, 강화학습 ② CNN, RNN, RNN&CNN, 강화학습 ③ 강화학습, CNN, RNN, RNN&CNN ④ RNN, 강화학습, RNN&CNN, CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a808b28",
   "metadata": {},
   "source": [
    "정답: ① RNN, CNN, RNN&CNN, 강화학습\n",
    "해설\n",
    "각 사례에 활용되는 인공지능 알고리즘의 핵심 원리는 다음과 같습니다.\n",
    "\n",
    "가. AI 스피커 (음성 분석) → RNN (Recurrent Neural Network)\n",
    "\n",
    "음성이나 텍스트와 같이 시간의 흐름에 따라 순차적으로 나타나는 **시계열 데이터(Sequential Data)**를 처리하는 데 최적화된 알고리즘입니다. 이전 단계의 정보를 다음 단계로 전달하는 특징이 있어 자연어 처리와 음성 인식에 주로 사용됩니다.\n",
    "\n",
    "나. 카메라 사진 (이미지 분석) → CNN (Convolutional Neural Network)\n",
    "\n",
    "이미지의 공간적 정보를 보존하면서 특징을 추출하는 알고리즘입니다. 사진 속의 사물을 인식하거나 분류하는 컴퓨터 비전 분야의 핵심 기술입니다.\n",
    "\n",
    "다. 이미지와 언어 간 번역 → RNN & CNN (결합 모델)\n",
    "\n",
    "사진(이미지)을 보고 이를 설명하는 문장을 만들거나(Image Captioning), 특정 언어를 다른 언어로 번역하는 작업에는 시각 정보를 처리하는 CNN과 문맥을 파악하는 RNN 기술이 복합적으로 사용됩니다.\n",
    "\n",
    "라. 바둑 (보상을 통한 학습) → 강화학습 (Reinforcement Learning)\n",
    "\n",
    "알파고 제로(AlphaGo Zero)처럼 인간의 기보 없이, 수행한 행동에 대해 **보상(Reward)**을 극대화하는 방향으로 스스로 학습하는 방식입니다. 시행착오를 통해 최적의 전략을 찾아내는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567162ca",
   "metadata": {},
   "source": [
    "# 42. 다음 중 파라미터 선정에 대한 내용이 아닌 것은? \n",
    "① 사람의 수작업으로 측정되고 결정됨 ② 모델 내부에서 확인이 가능한 변수로 데이터를 통해 산출 가능한 값 ③ 예측을 수행할 때 모델에 의해 요구되어지는 값 ④ 측정되거나 데이터로부터 학습됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ca816",
   "metadata": {},
   "source": [
    "정답: ① 사람의 수작업으로 측정되고 결정됨\n",
    "해설\n",
    "이 문제는 인공지능 모델에서 **파라미터(Parameter, 매개변수)**와 **하이퍼파라미터(Hyperparameter, 초매개변수)**의 차이점을 명확히 알고 있는지 묻는 문제입니다.\n",
    "\n",
    "파라미터(Parameter): 모델 내부에서 데이터로부터 스스로 학습되는 값입니다. 사람이 직접 설정하는 것이 아니라, 학습 과정(Optimization)을 통해 데이터에 의해 산출됩니다. (예: 회귀 계수, 신경망의 가중치(Weight)와 편향(Bias))\n",
    "\n",
    "하이퍼파라미터(Hyperparameter): 모델 학습을 시작하기 전에 사람이 수작업으로 결정하는 설정값입니다. (예: 학습률(Learning Rate), 은닉층의 개수, 배치 크기 등)\n",
    "\n",
    "다른 보기 설명 (파라미터의 특징)\n",
    "② 모델 내부에서 확인 가능한 변수: 파라미터는 모델의 구성 요소로서 저장되며, 학습된 결과로 얻어지는 내부 변수입니다.\n",
    "\n",
    "③ 예측 시 모델에 의해 요구되는 값: 학습이 완료된 후 새로운 데이터를 예측할 때, 저장된 파라미터 값을 사용하여 결과값을 계산합니다.\n",
    "\n",
    "④ 데이터로부터 학습됨: 파라미터는 손실 함수(Loss Function)를 최소화하는 방향으로 데이터를 통해 최적화되고 측정됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f0f18",
   "metadata": {},
   "source": [
    "# 43. 아래 설명의 문제에 대한 해결 기법으로 가장 알맞지 않은 것은?\n",
    "\n",
    "( )는 독립변수들 간에 강한 상관관계가 나타나는 문제이다. 이러한 문제가 존재하면 정확한 회귀계수의 추정이 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef976e",
   "metadata": {},
   "source": [
    "① 상관계수가 높은 변수 중 하나 혹은 일부를 제거한다. ② 변수를 그대로 사용하지 않고 기존 관측치를 그대로 이용한다. ③ 자료를 주성분분석(PCA)의 상황을 보아 상관관계의 이유를 파악하여 해결한다. ④ 주성분분석을 이용한 대각행렬의 형태로 공선성을 없애 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd3e7e",
   "metadata": {},
   "source": [
    "정답: ② 변수를 그대로 사용하지 않고 기존 관측치를 그대로 이용한다.\n",
    "해설\n",
    "이 문제는 다중공선성(Multicollinearity) 문제와 그 해결 방법에 대해 묻고 있습니다.\n",
    "\n",
    "다중공선성: 회귀 분석에서 독립변수들 사이에 강한 상관관계가 나타나, 회귀 계수 추정의 신뢰도가 떨어지고 분석 결과가 왜곡되는 현상을 말합니다.\n",
    "\n",
    "오답 분석 (②번): 다중공선성이 발생했을 때는 변수를 '그대로' 사용하거나 기존 관측치를 '그대로' 이용하는 것이 아니라, 변수를 변환하거나 선택적으로 제거해야 문제를 해결할 수 있습니다. 아무런 조치 없이 기존 데이터를 그대로 사용하면 다중공선성 문제는 해결되지 않습니다.\n",
    "\n",
    "다른 보기 설명 (다중공선성 해결 방법)\n",
    "① 변수 제거: 상관관계가 매우 높은(VIF 지수가 높은) 변수들을 서로 중복된 정보를 제공하므로, 그중 하나를 삭제하여 모델을 단순화합니다.\n",
    "\n",
    "③ 상관관계 파악: 변수들 간의 상관 행렬을 확인하거나 주성분 분석을 통해 어떤 변수들이 서로 얽혀 있는지 파악하는 것은 해결의 첫걸음입니다.\n",
    "\n",
    "④ 주성분 분석(PCA) 활용: 원래의 변수들을 서로 독립(상관관계가 0)인 새로운 성분들로 변환하여 사용하는 방법입니다. PCA를 거치면 변수들 간의 공선성이 사라진 대각행렬 형태의 데이터를 얻을 수 있어 문제를 근본적으로 해결할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076585c",
   "metadata": {},
   "source": [
    "# 44. 다중회귀 모형(모델)의 통계적 유의성 확인 방법은 무엇인가?\n",
    "① F통계량을 확인한다. ② t검정을 수행한다. ③ ANOVA를 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d854b3d",
   "metadata": {},
   "source": [
    "정답: ① F통계량을 확인한다.해설회귀 분석에서 '유의성 확인'은 크게 두 가지 층위로 나뉩니다. 문제에서 묻는 회귀 모형(모델) 전체의 통계적 유의성을 판단할 때는 **F-검정(F-test)**을 사용합니다.F-통계량 (모형의 유의성): \"설정된 회귀 모델이 개별 변수들을 떠나 전체적으로 종속변수를 설명하기에 적합한가?\"를 판단합니다. 귀무가설($H_0$)은 \"모든 회귀 계수는 0이다(모델이 쓸모없다)\"이며, F-통계량의 p-값이 유의수준보다 작으면 이 모델은 통계적으로 유의하다고 결론 내립니다.t-검정 (개별 변수의 유의성): 모델이 유의하다는 전제하에, \"각각의 독립변수가 개별적으로 종속변수에 영향을 주는가?\"를 확인하기 위해 각 회귀 계수에 대해 실시합니다.ANOVA (분산분석): 회귀 분석 결과에서 F-통계량을 산출하기 위해 사용되는 분석 틀이 ANOVA 테이블입니다. 하지만 모형의 유의성을 직접적으로 나타내는 지표(통계량)를 묻는다면 F-통계량이 가장 적절한 답변입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfbf5d9",
   "metadata": {},
   "source": [
    "# 45. 의사결정나무의 회귀트리(Regression Tree) 모델(모형)을 만들 때의 분류 기준은 무엇인가? \n",
    "① F통계량 ② 분산감소량, F통계량 ③ 엔트로피지수 ④ 지니지수, 카이제곱통계량"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ac5b8d",
   "metadata": {},
   "source": [
    "정답: ② 분산감소량, F통계량\n",
    "해설\n",
    "의사결정나무는 목표 변수(타겟)의 형태에 따라 **분류 나무(Classification Tree)**와 **회귀 나무(Regression Tree)**로 나뉩니다. 이 문제에서는 '회귀 트리'의 분리 기준을 묻고 있습니다.\n",
    "\n",
    "회귀 나무 (Regression Tree): 목표 변수가 연속형 수치일 때 사용합니다.\n",
    "\n",
    "분산감소량: 각 노드에서 분할 전후의 분산(Variance) 차이를 측정하여, 분산이 가장 많이 감소하는(즉, 집단 내 동질성이 커지는) 방향으로 분할합니다.\n",
    "\n",
    "F-통계량: 분산분석(ANOVA)에서 사용하는 통계량으로, 집단 간의 평균 차이가 통계적으로 유의미한지 확인하여 분리 기준으로 활용합니다.\n",
    "\n",
    "오답 분석 (분류 나무의 기준)\n",
    "③ 엔트로피 지수 (Entropy Index): 목표 변수가 범주형일 때 사용하며, 정보 이득(Information Gain)을 계산하여 불순도를 측정합니다.\n",
    "\n",
    "④ 지니 지수 (Gini Index), 카이제곱 통계량: 역시 범주형 데이터를 분류할 때 사용하는 지표입니다.\n",
    "\n",
    "지니 지수: CART 알고리즘에서 사용하며 불순도를 나타냅니다.\n",
    "\n",
    "카이제곱 통계량: CHAID 알고리즘에서 집단 간의 독립성을 검정하여 분리 기준으로 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5d937",
   "metadata": {},
   "source": [
    "# 46. 다음 중 활성화 함수의 설명으로 적절한 것?\n",
    "① 로지스틱회귀 모델에서의 회귀계수와 유사하게 해석된다. ② L2 penalty를 사용한다. ③ 선형이면 의미가 없다. ④ 시그모이드 함수가 대표적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a12f8",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.정답: ③ 선형이면 의미가 없다.해설인공신경망(Deep Learning)에서 **활성화 함수(Activation Function)**의 본질적인 목적을 묻는 문제입니다.비선형성 도입: 활성화 함수를 사용하는 가장 큰 이유는 네트워크에 **비선형성(Non-linearity)**을 추가하기 위해서입니다.왜 선형이면 의미가 없는가?: 만약 활성화 함수가 선형 함수($y = ax$)라면, 층을 아무리 깊게 쌓아도 결국 하나의 선형 함수로 요약되어 버립니다 ($a(a(ax)) = a^3x$). 이는 복잡한 데이터를 학습하는 딥러닝의 장점을 무색하게 만듭니다. 따라서 신경망의 깊이를 의미 있게 하려면 반드시 비선형 활성화 함수를 사용해야 합니다.오답 분석①번: 활성화 함수는 입력 신호의 총합을 출력 신호로 변환하는 함수일 뿐, 로지스틱 회귀의 회귀계수(독립변수가 종속변수에 미치는 영향력)와는 해석의 궤가 다릅니다.②번: L2 penalty는 모델의 과적합을 방지하기 위한 규제(Regularization) 기법 중 하나이며, 활성화 함수의 종류나 기능과는 무관합니다.④번: 시그모이드(Sigmoid) 함수가 활성화 함수의 한 종류인 것은 맞지만, '가장 적절한 설명'을 고르는 문제에서는 활성화 함수의 존재 이유를 설명하는 ③번이 더 근본적이고 정확한 답이 됩니다. (최근에는 시그모이드보다 ReLU 계열이 더 많이 쓰이기도 합니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d83e86e",
   "metadata": {},
   "source": [
    "# 47. 활성화 함수 중 시그모이드와 하이퍼볼릭탄젠트(Tanh) 함수의 결과값을 순서대로 도출하시오.\n",
    "① $0 \\le y \\le 1, -1 \\le y \\le 1$ ② $-1 \\le y \\le 1, -1 \\le y \\le 1$\n",
    "③ $-1 \\le y \\le 0, -1 \\le y \\le 0$ ④ $-1 \\le y \\le 1, 0 \\le y \\le 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05814c40",
   "metadata": {},
   "source": [
    "답: ① $0 \\le y \\le 1, -1 \\le y \\le 1$해설각 활성화 함수의 정의와 출력값의 범위(치역)를 이해하는 것이 핵심입니다.시그모이드(Sigmoid) 함수:함수식: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$특징: 입력값이 아무리 크거나 작아도 출력값을 0과 1 사이로 압축합니다.범위: $0 \\le y \\le 1$ (또는 개구간으로 $(0, 1)$)주로 이진 분류의 출력층에서 확률값으로 변환할 때 사용됩니다.하이퍼볼릭탄젠트(Tanh) 함수:함수식: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$특징: 시그모이드 함수를 확장하여 중심을 0으로 맞춘 형태입니다. 시그모이드보다 기울기 소실(Gradient Vanishing) 문제가 다소 완화되는 경향이 있습니다.범위: $-1 \\le y \\le 1$ (또는 개구간으로 $(-1, 1)$)출력이 음수와 양수 모두에 걸쳐 있어 데이터의 평균이 0에 가깝게 유지되도록 돕습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcbbbf",
   "metadata": {},
   "source": [
    "# 48. 다음 신경망 활성화 함수의 출력물을 계산하시오. (입력 X가 2, 3이고 가중치 W가 각각 5, 3일 때의 출력 X) \n",
    "① 17 ② 18 ③ 19 ④ 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe29d0d",
   "metadata": {},
   "source": [
    "정답: ③ 19해설인공신경망의 가장 기본적인 연산인 선형 결합(Linear Combination) 과정을 계산하는 문제입니다. 별도의 활성화 함수식이 주어지지 않았으므로, 입력값과 가중치의 곱의 합을 구하면 됩니다.계산 공식: $y = \\sum (x_i \\cdot w_i) = (x_1 \\cdot w_1) + (x_2 \\cdot w_2)$값 대입:첫 번째 입력($x_1=2$)과 가중치($w_1=5$)의 곱: $2 \\times 5 = 10$두 번째 입력($x_2=3$)과 가중치($w_2=3$)의 곱: $3 \\times 3 = 9$최종 합계: $10 + 9 = 19$참고: 신경망의 기본 연산 구조실제 신경망에서는 이 합계($19$)에 **편향(Bias)**을 더한 후, 문제 46~47번에서 다루었던 활성화 함수를 통과시켜 최종 출력을 결정합니다.입력($X$) & 가중치($W$): 데이터와 그 중요도(가중치)를 곱함.합산(Summation): 모든 가중합을 더함. (이 문제의 단계)활성화(Activation): 비선형 함수를 통해 신호를 전달할지 결정함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c4a22e",
   "metadata": {},
   "source": [
    "# 49. 서포트벡터머신(SVM)의 설명으로 틀린 것은? \n",
    "① 분류와 회귀분석에 사용되는 지도학습 알고리즘이다. ② 데이터가 사상된 공간에서 경계선과 가장 근접한 데이터를 서포트벡터라고 부른다. ③ 테스트가 매우 쉽다. ④ 고차원에서의 특징 추출이 어려운 경우 차원의 저주를 회피한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f7ea9",
   "metadata": {},
   "source": [
    "정답: ④ 고차원에서의 특징 추출이 어려운 경우 차원의 저주를 회피한다.\n",
    "해설\n",
    "SVM과 차원의 저주: 서포트 벡터 머신(SVM)은 고차원 공간에서도 매우 효과적으로 작동하는 알고리즘이지만, **차원의 저주(Curse of Dimensionality)**로부터 완전히 자유로운 것은 아닙니다. 변수의 개수가 관측치의 개수보다 훨씬 많아지는 극단적인 고차원 상황에서는 모델의 성능이 저하되거나 과적합(Overfitting)이 발생할 위험이 있습니다. 따라서 \"차원의 저주를 회피한다\"는 단정적인 설명은 틀린 것입니다.\n",
    "\n",
    "다른 보기 설명 (옳은 설명)\n",
    "① 알고리즘의 용도: SVM은 주로 분류(SVC) 문제에 사용되지만, 회귀(SVR) 분석에도 사용될 수 있는 강력한 지도학습 알고리즘입니다.\n",
    "\n",
    "② 서포트 벡터의 정의: 결정 경계(Hyperplane)와 가장 가까이에 위치한 데이터 포인트들을 **서포트 벡터(Support Vector)**라고 하며, 이들이 경계선을 결정하는 핵심 역할을 합니다.\n",
    "\n",
    "③ 테스트의 효율성: SVM은 학습 과정은 복잡하고 시간이 걸릴 수 있지만, 일단 결정 경계가 만들어지면 새로운 데이터를 분류(테스트)하는 과정은 매우 빠르고 간단합니다.\n",
    "\n",
    "SVM의 핵심 개념: 마진(Margin)\n",
    "SVM의 목표는 두 클래스 사이의 거리를 나타내는 **마진(Margin)**을 최대화하는 것입니다. 마진이 클수록 새로운 데이터에 대한 일반화 성능이 좋아집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1b5d7",
   "metadata": {},
   "source": [
    "# 50. 다음 중 서포트벡터머신(SVM)의 커널 종류가 아닌 것은? \n",
    "① 다항식 커널 ② 회귀형 커널 ③ 가우시안 커널 ④ 시그모이드 커널"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea734185",
   "metadata": {},
   "source": [
    "정답: ② 회귀형 커널\n",
    "해설\n",
    "커널 트릭(Kernel Trick): SVM에서 저차원 공간의 데이터를 고차원 공간으로 매핑하여, 선형으로 분리되지 않는 데이터를 선형 분리가 가능한 형태로 변환하는 기법입니다.\n",
    "\n",
    "오답 분석 (②번): '회귀형'은 분석의 목적(분류 vs 회귀)을 나타내는 용어이지, 수학적으로 데이터를 변환하는 커널 함수(Kernel Function)의 종류를 일컫는 명칭이 아닙니다. SVM을 회귀 분석에 사용할 때는 이를 SVR(Support Vector Regression)이라고 부르지만, 이때도 다항식이나 가우시안 같은 커널을 선택해서 사용합니다.\n",
    "\n",
    "다른 보기 설명 (SVM의 주요 커널 종류)\n",
    "① 다항식 커널 (Polynomial Kernel): 데이터를 고차원 다항식 형태로 매핑합니다. 이미지 처리 등에 유용합니다.\n",
    "\n",
    "③ 가우시안 커널 (Gaussian RBF Kernel): 가장 널리 사용되는 커널로, 데이터 포인트를 무한한 차원의 공간으로 매핑한 것과 같은 효과를 줍니다. 방사 기저 함수라고도 불립니다.\n",
    "\n",
    "④ 시그모이드 커널 (Sigmoid Kernel): 인공신경망의 활성화 함수와 유사한 수식을 사용하여 데이터를 변환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006b6e8",
   "metadata": {},
   "source": [
    "# 51. 연관규칙의 \"A -> B\"의 향상도는? (지지도: 3/10, P(A): 6/10, P(B): 6/10) \n",
    "① 63% ② 36% ③ 38% ④ 83%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aeedb7",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.정답: ④ 83% (약 0.833)해설연관규칙 분석에서 **향상도(Lift)**는 품목 A가 주어지지 않았을 때 품목 B의 확률 대비, 품목 A가 주어졌을 때 B가 구매될 확률의 비율을 나타냅니다.1. 향상도 계산 공식$$\\text{Lift}(A \\rightarrow B) = \\frac{P(B|A)}{P(B)} = \\frac{P(A \\cap B)}{P(A) \\cdot P(B)} = \\frac{\\text{Support}(A \\cap B)}{\\text{Support}(A) \\cdot \\text{Support}(B)}$$2. 문제의 데이터 대입$P(A \\cap B)$ (지지도): $3/10 = 0.3$$P(A)$: $6/10 = 0.6$$P(B)$: $6/10 = 0.6$3. 계산 과정$$\\text{Lift} = \\frac{0.3}{0.6 \\times 0.6} = \\frac{0.3}{0.36}$$$$\\frac{30}{36} = \\frac{5}{6} \\approx 0.8333...$$이를 백분율로 환산하면 **약 83%**가 됩니다.향상도(Lift) 결과 해석Lift > 1: 두 품목 간에 양의 상관관계가 있음 (A를 살 때 B를 살 가능성이 높아짐).Lift = 1: 두 품목은 서로 독립임.Lift < 1: 두 품목 간에 음의 상관관계가 있음 (A를 사면 오히려 B를 안 살 가능성이 높음).참고: 이 문제의 경우 향상도가 1보다 작은 0.83이므로, 두 품목은 서로 연관성이 낮거나 같이 구매될 확률이 독립적인 경우보다 낮음을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e8087",
   "metadata": {},
   "source": [
    "# 52. 주성분분석(PCA)의 공분산행렬에 대한 설명으로 옳지 않은 것은?\n",
    "① 공분산행렬의 고유벡터는 데이터가 어떤 방향으로 분산되었는지를 나타낸다. ② 변수들 사이의 공분산을 행렬로 나타낸 값이다. ③ 정방행렬은 불가능하다. ④ 전치를 시켰을 때 동일한 행렬이 나타나는 대칭행렬이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179b647",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.정답: ③ 정방행렬은 불가능하다.해설주성분분석(PCA)에서 사용하는 **공분산 행렬(Covariance Matrix)**의 특성을 묻는 문제입니다.공분산 행렬의 구조: 변수의 개수가 $n$개일 때, 공분산 행렬은 $n \\times n$ 형태의 행렬이 됩니다. 행의 개수와 열의 개수가 같으므로 반드시 **정방행렬(Square Matrix)**이어야 합니다. 따라서 \"불가능하다\"고 설명한 ③번은 틀린 내용입니다.다른 보기 설명 (옳은 성질)① 고유벡터(Eigenvector)와 분산: 공분산 행렬을 고유값 분해했을 때 얻어지는 고유벡터는 데이터의 분산이 가장 큰 방향(주성분 축)을 가리킵니다.② 정의: 여러 변수들 사이의 공분산(두 변수가 함께 변하는 정도)을 일목요연하게 표 형태로 정리한 것이 공분산 행렬입니다.④ 대칭행렬(Symmetric Matrix): 변수 $A$와 $B$의 공분산은 변수 $B$와 $A$의 공분산과 같습니다($Cov(A, B) = Cov(B, A)$). 이로 인해 대각선을 기준으로 대칭을 이루므로, 전치(Transpose)를 해도 원래 행렬과 동일한 대칭행렬의 성질을 가집니다.PCA의 핵심 단계데이터의 공분산 행렬을 구한다.공분산 행렬을 고유값 분해하여 고유값과 고유벡터를 찾는다.고유값이 큰 순서대로 고유벡터를 선택하여 데이터를 저차원으로 투영한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150dc97",
   "metadata": {},
   "source": [
    "# 54. 다음 시계열의 요인 중 틀린 것은? \n",
    "① 추세요인(Trend Factor): 인구증가, 자본축적, 기술진보 ② 계절요인(Seasonal Factor): 아이스크림 판매량, 농업생산량 ③ 불규칙 요인: 전쟁, 홍수 ④ 순환요인(Cyclical Factor): 물가상승률, 인구밀도, 전염병"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c436f",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.\n",
    "\n",
    "정답: ④ 순환요인(Cyclical Factor): 물가상승률, 인구밀도, 전염병\n",
    "해설\n",
    "시계열 데이터는 크게 4가지 성분으로 분해될 수 있는데, ④번 보기의 예시들은 각 요인의 정의와 맞지 않습니다.\n",
    "\n",
    "순환요인(Cyclical Factor): 경기 변동과 같이 보통 2~10년 정도의 주기를 가지고 상승과 하락을 반복하는 변동을 의미합니다.\n",
    "\n",
    "오답 분석:\n",
    "\n",
    "물가상승률: 경제 전반의 흐름에 따라 변동하지만, 특정 주기를 가진 순환보다는 추세나 불규칙 요인이 복합적으로 작용합니다.\n",
    "\n",
    "인구밀도: 장기적으로 변하는 추세요인에 가깝습니다.\n",
    "\n",
    "전염병: 예상치 못한 시점에 발생하는 불규칙 요인에 해당합니다.\n",
    "\n",
    "다른 보기 설명 (시계열의 4대 요인)\n",
    "① 추세요인(Trend Factor): 데이터가 장기간에 걸쳐 지속적으로 증가하거나 감소하는 경향입니다. 인구 증가, 기술 진보에 따른 생산성 향상 등이 대표적입니다.\n",
    "\n",
    "② 계절요인(Seasonal Factor): 일정한 주기를 가지고 1년 안에 반복되는 변동입니다. 여름의 아이스크림 판매량, 설날의 떡국 매출 등이 해당합니다.\n",
    "\n",
    "③ 불규칙 요인(Irregular/Random Factor): 천재지변(홍수), 전쟁, 파업 등 명확한 규칙성 없이 우연히 발생하는 변동입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd2549",
   "metadata": {},
   "source": [
    "# 55. 다음 시계열 분석 방법에 대한 설명으로 가장 거리가 먼 것은? \n",
    "① 자기회귀 모델(AR): 현재의 시계열과 과거의 자신과의 관계를 정의 ② 이동평균 모델(MA): 현재의 시계열과 과거 자신의 오차와의 관계를 정의 ③ 자기회귀 이동평균 모델(ARMA): 현재와 과거의 자신을 고려하여 정의 ④ 자기회귀 누적이동평균 모델(ARIMA): 현재와 추세 간의 관계를 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a82d6fb",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.정답: ④ 자기회귀 누적이동평균 모델(ARIMA): 현재와 추세 간의 관계를 정의해설시계열 분석 모델인 AR, MA, ARMA, ARIMA의 정의를 정확히 알고 있는지 묻는 문제입니다.ARIMA(Auto-Regressive Integrated Moving Average) 모델:이 모델은 정상성(Stationarity)이 없는 시계열 데이터를 분석하기 위해 만들어졌습니다.핵심은 '현재와 추세 간의 관계'를 정의하는 것이 아니라, 비정상 시계열을 **차분(Differencing)**하여 정상 시계열로 변환한 후, AR과 MA 모델을 결합하여 분석하는 방법입니다.다른 보기 설명 (옳은 정의)① 자기회귀 모델 (AR, Auto-Regressive): 현시점의 데이터를 과거의 관측값들의 선형 결합으로 설명하는 모델입니다. ($X_t$는 $X_{t-1}, X_{t-2}...$의 영향을 받음)② 이동평균 모델 (MA, Moving Average): 현시점의 데이터를 **과거의 백색잡음(오차항)**들의 선형 결합으로 설명하는 모델입니다. ($X_t$는 $\\epsilon_{t-1}, \\epsilon_{t-2}...$의 영향을 받음)③ 자기회귀 이동평균 모델 (ARMA): AR 모델과 MA 모델을 결합한 형태로, 과거의 관측값과 과거의 오차항을 모두 고려하여 현재를 예측합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc5a8c",
   "metadata": {},
   "source": [
    "# 56. 다음 중 시계열 분석에서 사용하는 개념으로 옳지 않은 것은? \n",
    "① 백색잡음: 일반적인 정규분포에서 도출된 무작위 잡음(Random Noise) 값을 의미 ② 자기상관: 시계열적 관점으로 보았을 때 이동된 시간 사이에서의 자기 자신과의 불규칙 관계를 의미 ③ 평활법: 지수평활법과 이동평균법으로 구분 ④ 분해법: 시계열의 변동폭이 시간의 흐름에 따라 일정하거나 점차 커지는 경우 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2934f",
   "metadata": {},
   "source": [
    "정답: ② 자기상관: 시계열적 관점으로 보았을 때 이동된 시간 사이에서의 자기 자신과의 불규칙 관계를 의미해설자기상관(Autocorrelation): '불규칙 관계'가 아니라, 시계열 데이터 내에서 현재 시점의 값과 일정 시간(시차, Lag)이 떨어진 과거의 값 사이의 선형적 관련성을 의미합니다. 즉, 과거의 값이 현재의 값에 얼마나 규칙적인 영향을 미치는지를 측정하는 척도입니다. '불규칙 관계'라는 표현은 정의에 어긋납니다.다른 보기 설명 (옳은 성질)① 백색잡음(White Noise): 평균이 0이고 분산이 일정하며, 자기상관이 없는 무작위 변수들을 의미합니다. 시계열 모델이 설명하지 못하는 순수한 무작위 오차항입니다.③ 평활법(Smoothing): 불규칙한 변동을 제거하여 데이터의 흐름을 매끄럽게 파악하는 기법입니다.이동평균법: 최근 $n$개 데이터의 평균을 사용.지수평활법: 최근 데이터에 더 높은 가중치를 부여.④ 분해법(Decomposition): 시계열 데이터를 추세, 계절, 순환, 불규칙 요인으로 나누어 분석하는 방법입니다. 변동폭의 양상에 따라 **가법 모델(Additive)**이나 **승법 모델(Multiplicative)**을 선택하여 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69dd0c8",
   "metadata": {},
   "source": [
    "# 57. 다음 중 딥러닝 기법의 기반이 되는 모델은?\n",
    "① 연관성 분석 모델 ② 신경망 모델 ③ 회귀 모델 ④ 군집 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b55d1a",
   "metadata": {},
   "source": [
    "정답: ② 신경망 모델\n",
    "해설\n",
    "딥러닝(Deep Learning)의 정의: 딥러닝은 인간의 뇌 구조를 모방한 **인공신경망(Artificial Neural Network, ANN)**을 기반으로 하는 머신러닝의 한 분야입니다.\n",
    "\n",
    "심층 신경망: 신경망의 층(Layer)을 깊게(Deep) 쌓아서 복잡한 데이터의 패턴을 학습하기 때문에 '딥러닝'이라는 명칭이 붙었습니다. 따라서 딥러닝의 근간이 되는 모델은 신경망 모델입니다.\n",
    "\n",
    "다른 보기 설명\n",
    "① 연관성 분석 모델: 데이터 내의 항목 간 'A를 구매하면 B도 구매한다'와 같은 규칙을 찾는 기법입니다(예: 장바구니 분석).\n",
    "\n",
    "③ 회귀 모델: 독립변수와 종속변수 간의 선형적 관계를 수식으로 모델링하여 수치를 예측하는 전통적인 통계 기법입니다.\n",
    "\n",
    "④ 군집 모델: 별도의 정답(레이블) 없이 데이터 간의 유사성을 측정하여 비슷한 특성을 가진 그룹으로 묶는 비지도 학습 기법입니다.\n",
    "\n",
    "정리: 머신러닝과 딥러닝의 관계\n",
    "인공지능(AI) > 머신러닝(ML) > 신경망(ANN) > 딥러닝(DL)\n",
    "\n",
    "딥러닝은 신경망 모델 중에서도 은닉층(Hidden Layer)이 2개 이상인 **심층 신경망(DNN)**을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d17ac",
   "metadata": {},
   "source": [
    "# 58. 네트워크 중심성 평가에서 사용하는 중심성이 아닌 것은? \n",
    "① 연결중심성(Degree Centrality) ② 매개중심성(Betweenness Centrality) ③ 근접중심성(Closeness Centrality) ④ 원격중심성(Remote Centrality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e07601",
   "metadata": {},
   "source": [
    "정답: ④ 원격중심성(Remote Centrality)\n",
    "해설\n",
    "사회관계망 분석(SNA)이나 네트워크 분석에서 노드의 중요도를 측정하는 4대 주요 중심성 지표가 있습니다. 원격중심성은 표준적인 네트워크 분석에서 사용되는 용어가 아닙니다.\n",
    "\n",
    "네트워크 중심성의 주요 유형\n",
    "① 연결중심성(Degree Centrality):\n",
    "\n",
    "한 노드가 얼마나 많은 연결(간선)을 가지고 있는지를 측정합니다. 단순히 친구가 가장 많은 사람이 누구인지를 찾는 방식입니다.\n",
    "\n",
    "② 매개중심성(Betweenness Centrality):\n",
    "\n",
    "노드가 네트워크 내의 다른 노드들 사이의 최단 경로에 얼마나 자주 위치하는지를 측정합니다. 서로 다른 집단 간을 연결하는 '가교' 역할을 하는 노드를 찾을 때 유리합니다.\n",
    "\n",
    "③ 근접중심성(Closeness Centrality):\n",
    "\n",
    "한 노드에서 다른 모든 노드까지 도달하는 데 걸리는 거리의 합의 역수로 계산합니다. 정보가 네트워크 전체로 가장 빨리 전파될 수 있는 중심 위치를 나타냅니다.\n",
    "\n",
    "아이겐벡터 중심성(Eigenvector Centrality):\n",
    "\n",
    "연결된 이웃 노드들의 중요도를 반영합니다. 즉, \"중요한 사람들과 많이 연결된 사람이 더 중요하다\"는 개념입니다. 구글의 페이지랭크(PageRank) 알고리즘의 기초가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74aab0",
   "metadata": {},
   "source": [
    "# 59. 비모수 통계에 대한 설명으로 가장 거리가 먼 것은?\n",
    "① 가정에 대한 불만족, 작은 샘플 사이즈, 순위로만 된 데이터를 사용한다. ② 평균을 사용한다. ③ 부호검정 방법이 있다. ④ 통계량은 중위수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89393bee",
   "metadata": {},
   "source": [
    "정답: ② 평균을 사용한다.해설비모수 통계(Non-parametric Statistics)와 모수 통계(Parametric Statistics)의 가장 큰 차이점을 묻는 문제입니다.평균 vs 중위수: 모수 통계는 모집단이 정규분포를 따른다는 가정하에 **평균($\\mu$)**과 표준편차를 사용하여 분석합니다. 반면, 비모수 통계는 데이터가 정규분포를 따르지 않거나 표본 크기가 작을 때 사용하며, 평균 대신 데이터의 순위나 **중위수(Median)**를 대푯값으로 사용합니다. 따라서 \"평균을 사용한다\"는 설명은 모수 통계에 해당하는 내용입니다.다른 보기 설명 (비모수 통계의 특징)① 사용 배경:모집단의 분포 가정을 충족하지 못할 때(정규성 결여) 사용합니다.표본의 크기가 너무 작아($n < 30$) 정규분포를 가정하기 힘들 때 유용합니다.수치적 크기보다는 **순위(Rank)**로 된 서열 데이터를 분석할 때 적합합니다.③ 부호검정(Sign Test): 비모수 통계의 대표적인 방법 중 하나로, 관측값들이 특정 기준값보다 큰지 작은지를 '부호'로 표시하여 차이를 검정합니다.④ 통계량: 평균이 이상치(Outlier)에 민감하기 때문에, 비모수 통계에서는 데이터를 크기순으로 나열했을 때 한가운데 위치하는 값인 중위수를 주요 통계량으로 활용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166108a",
   "metadata": {},
   "source": [
    "# 60. 다음 비모수 통계에 대한 검정 기법으로 알맞은 것은?\n",
    "\n",
    "약을 탔을 때, 반응을 전후로 살펴본다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a153d1",
   "metadata": {},
   "source": [
    "① 윌콕슨부호 ② 윌콕슨순위합 ③ 프리드만 ④ 크루스칼왈리스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9686f408",
   "metadata": {},
   "source": [
    "정답: ① 윌콕슨부호 (Wilcoxon Signed Rank Test)\n",
    "해설\n",
    "이 문제는 비모수 통계에서 대응표본(Paired Sample)의 차이를 검정하는 방법을 묻는 문제입니다.\n",
    "\n",
    "문제 상황 분석: \"약을 먹기 전과 후의 반응을 살펴본다\"는 것은 동일한 대상에 대해 두 번 측정하여 그 차이를 비교하는 대응표본 상황입니다.\n",
    "\n",
    "윌콕슨 부호 순위 검정: 모수 통계의 **대응표본 t-검정(Paired t-test)**에 대응하는 비모수 방법입니다. 전후 차이값의 부호와 순위를 모두 고려하여 변화가 있는지 검정합니다.\n",
    "\n",
    "오답 분석\n",
    "② 윌콕슨 순위합 검정 (Rank Sum Test): 서로 독립된 두 집단을 비교할 때 사용합니다. (모수 통계의 독립표본 t-검정에 대응)\n",
    "\n",
    "③ 프리드만 검정 (Friedman Test): 세 집단 이상의 대응표본을 비교할 때 사용합니다. (모수 통계의 반복측정 분산분석에 대응)\n",
    "\n",
    "④ 크루스칼-왈리스 검정 (Kruskal-Wallis Test): 서로 독립된 세 집단 이상의 평균 차이를 비교할 때 사용합니다. (모수 통계의 일원배치 분산분석에 대응)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda14a84",
   "metadata": {},
   "source": [
    "# 61. 회귀 모델의 평가지표 수식으로 옳지 않은 것은?\n",
    "① $MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$ ② $RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$\n",
    "③ $MAPE = \\frac{100\\%}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$ ④ $MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b9ac7d",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.정답: ③ $MAPE = \\frac{100\\%}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$해설MAPE(Mean Absolute Percentage Error, 평균 절대 백분율 오차): 오차를 실제 값으로 나눈 비율의 절대값을 평균하여 백분율로 나타낸 지표입니다.수식의 오류: 제시된 ③번 수식은 실제 값($y_i$)으로 나누는 과정이 빠져 있습니다. 올바른 수식은 다음과 같습니다.$$MAPE = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|$$여기서 $y_i$는 실제값, $\\hat{y}_i$는 예측값입니다.다른 보기 설명 (옳은 수식)① MAE (Mean Absolute Error, 평균 절대 오차): 실제값과 예측값의 차이(오차)의 절대값을 평균한 값입니다. 모델의 예측이 평균적으로 실제값과 얼마나 떨어져 있는지를 직관적으로 보여줍니다.② RMSE (Root Mean Squared Error, 평균 제곱근 오차): MSE에 루트를 씌운 값입니다. 실제값과 단위가 같아져 해석이 용이하며, 큰 오차에 대해 더 민감하게 반응(패널티 부여)하는 특징이 있습니다.④ MSE (Mean Squared Error, 평균 제곱 오차): 오차를 제곱하여 평균한 값입니다. 이상치에 대해 민감하게 반응하며 회귀 모델의 손실 함수로 가장 널리 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c4b07",
   "metadata": {},
   "source": [
    "# 62. 다음 중 Fail을 Fail로 예측하는 기준은? \n",
    "① 정밀도(Precision) ② 재현율(Recall) ③ 신뢰도(Reliability) ④ 특이도(Specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a4425",
   "metadata": {},
   "source": [
    "정답: ④ 특이도(Specificity)해설이 문제는 분류 모델의 성능 평가지표인 **혼동 행렬(Confusion Matrix)**의 개념을 묻는 문제입니다. 일반적으로 모델에서 'Fail'을 음성(Negative, 0), 'Success'를 양성(Positive, 1)으로 간주할 때의 기준입니다.특이도(Specificity): 실제값도 **Negative(Fail)**인 것들 중에서 모델이 **Negative(Fail)**라고 정확하게 예측한 비율을 의미합니다.공식: $\\text{Specificity} = \\frac{TN}{TN + FP}$즉, **\"실제 Fail인 데이터를 Fail로 얼마나 잘 맞췄는가\"**를 나타내는 지표입니다.다른 보기 설명① 정밀도(Precision): 모델이 **Success(Positive)**라고 예측한 것 중에서 실제값도 Success인 비율입니다. (예측의 정확성)② 재현율(Recall): 실제값이 Success인 것들 중에서 모델이 Success라고 맞춘 비율입니다. (민감도와 같은 개념입니다.)③ 신뢰도(Reliability): 통계적 추정이나 연관 규칙 등에서 사용되는 용어로, 분류 모델의 혼동 행렬 지표에는 직접적으로 포함되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f616200",
   "metadata": {},
   "source": [
    "# 63. 혼동행렬에서의 민감도, 정밀도 계산으로 옳지 않은 것은?\n",
    "① 재현율(Recall) = TP / (TP + FN) ② 정밀도(Precision) = TP / (TP + FP) ③ 정확도(Accuracy) = (TP + TN) / (TP + TN + FP + FN) ④ F1-Score = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29379848",
   "metadata": {},
   "source": [
    "질문하신 문제의 보기들은 모두 통계적으로 올바른 공식을 기술하고 있습니다. 만약 이 문제가 기출문제라면, 보통은 분모의 구성 요소를 살짝 바꾸어 오답을 유도하는 경우가 많습니다.예를 들어, 정밀도의 분모에 FP 대신 FN을 넣거나, F1-Score를 산술평균으로 표현하는 방식입니다. 현재 제시된 보기들 중에는 틀린 것이 없으므로, 각 지표의 정확한 의미를 다시 한번 정리해 드립니다.분류 지표 상세 해설① 재현율(Recall) = $TP / (TP + FN)$ (옳음)**민감도(Sensitivity)**라고도 불립니다. 실제 양성인 데이터 중에서 모델이 얼마나 빠뜨리지 않고 양성으로 검출했는지를 측정합니다. 암 진단과 같이 '놓쳐서는 안 되는' 상황에서 가장 중요합니다.② 정밀도(Precision) = $TP / (TP + FP)$ (옳음)모델이 양성이라고 예측한 데이터 중에서 실제 양성인 비율입니다. 스팸 메일 분류와 같이 '정상 메일을 스팸으로 오인하면 안 되는' 상황에서 중요합니다.③ 정확도(Accuracy) = $(TP + TN) / \\text{Total}$ (옳음)전체 데이터 중 예측이 실제와 일치(양성을 양성으로, 음성을 음성으로)한 비율입니다. 가장 직관적이지만 데이터가 불균형할 경우 신뢰도가 떨어집니다.④ F1-Score = $2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ (옳음)정밀도와 재현율의 조화평균입니다. 두 지표 중 어느 하나가 너무 낮으면 값도 낮아지게 설계되어 있어, 모델의 균형 잡힌 성능을 평가할 때 쓰입니다.자주 출제되는 '오답' 유형 (주의!)시험 문제에서 \"옳지 않은 것\"으로 정답이 되는 사례들은 주로 다음과 같습니다.정밀도 오답: $Precision = TP / (TP + \\mathbf{FN})$ (분모에 $FP$가 와야 함)재현율 오답: $Recall = TP / (TP + \\mathbf{FP})$ (분모에 $FN$이 와야 함)F1-Score 오답: $F1 = (\\text{Precision} + \\text{Recall}) / 2$ (조화평균이 아닌 산술평균은 틀림)특이도(Specificity) 오답: $Specificity = TN / (\\mathbf{TP} + TN)$ (분모에 $FP$가 와야 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3cb7d2",
   "metadata": {},
   "source": [
    "# 64. 다음 중 이진분류기 평가 측정지표로 옳지 않은 것은? \n",
    "① 정밀도 ② 오분류율 ③ 정확도 ④ 평균절대오차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9acc6",
   "metadata": {},
   "source": [
    "정답: ④ 평균절대오차 (MAE, Mean Absolute Error)해설분류(Classification) 모델과 회귀(Regression) 모델의 평가지표를 구분할 수 있는지 묻는 문제입니다.④ 평균절대오차 (MAE): 모델이 예측한 값과 실제 값 사이의 차이(오차)를 계산하는 지표로, 연속형 수치를 예측하는 회귀(Regression) 모델에서 사용합니다.이진 분류(Binary Classification) 지표: '맞다/틀리다' 또는 'A/B'를 구분하는 이진 분류에서는 다음과 같은 지표를 사용합니다.① 정밀도 (Precision): 양성이라고 예측한 것 중 실제 양성인 비율.② 오분류율 (Error Rate): 전체 데이터 중 잘못 예측한 비율 ($1 - \\text{정확도}$).③ 정확도 (Accuracy): 전체 데이터 중 올바르게 예측한 비율."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df9e36",
   "metadata": {},
   "source": [
    "# 65. 다음 중 선형 및 다중회귀분석에 대한 설명으로 옳지 않은 것은?\n",
    "① 회귀분석의 잔차는 정규분포이다. ② 단순회귀분석은 잔차에 대한 선형성을 진단한다. ③ 다중회귀분석은 두 개 이상의 독립변수가 하나의 종속변수에 미치는 영향을 추정한다. ④ 다중회귀분석은 선택된 독립변수 간 높은 상관관계를 통해 다중공선성 문제를 회피한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d068f0",
   "metadata": {},
   "source": [
    "정답: ④ 다중회귀분석은 선택된 독립변수 간 높은 상관관계를 통해 다중공선성 문제를 회피한다.해설다중회귀분석의 핵심 가정과 주의사항인 **다중공선성(Multicollinearity)**을 정확히 이해하고 있는지 묻는 문제입니다.다중공선성 문제: 독립변수들 간에 상관관계가 매우 높을 때 발생하는 현상입니다. 이 경우 회귀 계수의 분산이 커져 모델의 신뢰도가 떨어지고, 어떤 변수가 중요한지 판단하기 어려워집니다.오답 분석: ④번은 \"높은 상관관계를 통해 문제를 회피한다\"고 설명하고 있으나, 실제로는 **\"높은 상관관계가 문제를 일으키는 원인\"**입니다. 따라서 다중공선성 문제를 피하려면 상관관계가 높은 변수를 제거하거나 주성분 분석(PCA) 등을 통해 변수를 통합해야 합니다.다른 보기 설명 (회귀분석의 기본 가정)① 잔차의 정규성: 회귀분석의 중요한 가정 중 하나로, 잔차(실제값과 예측값의 차이)의 분포가 평균이 0인 정규분포를 이루어야 합니다.② 선형성: 독립변수와 종속변수 간의 관계가 선형적(직선 형태)이어야 한다는 가정입니다. 단순회귀에서는 잔차 그래프를 통해 이를 진단할 수 있습니다.③ 다중회귀의 정의: 독립변수($X$)가 2개 이상이고 종속변수($Y$)가 1개일 때, 이들 사이의 선형 관계를 분석하는 기법을 말합니다.다중공선성 판별 지표: VIF (Variance Inflation Factor)시험에 자주 출제되는 다중공선성 판별 기준은 VIF입니다.VIF $\\ge$ 10: 해당 독립변수들 사이에 심각한 다중공선성이 있다고 판단하며, 변수 처리가 필요합니다.VIF < 5: 통상적으로 다중공선성 문제가 미미하다고 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78e5e8",
   "metadata": {},
   "source": [
    "# 66. AUC(Area Under Curve)의 설명으로 옳지 않은 것은? \n",
    "① ROC 커브의 면적을 AUC라고 한다. ② AUC의 면적은 넓을수록 좋은 모델이다. ③ TPR(True Positive Rate)과 FPR(False Positive Rate)은 서로 비례적인 관계에 있다. ④ 분류 모델의 성능을 보여주는 그래프이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c89bed2",
   "metadata": {},
   "source": [
    "정답: ③ TPR(True Positive Rate)과 FPR(False Positive Rate)은 서로 비례적인 관계에 있다.\n",
    "해설\n",
    "ROC 커브와 AUC의 원리를 이해하는 것이 핵심입니다.\n",
    "\n",
    "TPR과 FPR의 관계: TPR(민감도)과 FPR(1-특이도)은 서로 트레이드오프(Trade-off) 관계에 있습니다. 임계값(Threshold)을 조절함에 따라 한쪽이 높아지면 다른 쪽도 높아지는 경향은 있지만, 이를 단순히 \"서로 비례한다\"고 표현하는 것은 통계적으로 옳지 않습니다. 모델의 목적은 FPR을 낮게 유지하면서 TPR을 최대한 높이는 것이며, 이 두 지표의 관계를 곡선으로 나타낸 것이 ROC 커브입니다.\n",
    "\n",
    "다른 보기 설명 (옳은 성질)\n",
    "① 정의: ROC(Receiver Operating Characteristic) 곡선 아래의 면적을 **AUC(Area Under the Curve)**라고 부릅니다.\n",
    "\n",
    "② 성능 지표: AUC 값은 0.5에서 1 사이의 값을 가집니다. 면적이 넓을수록(1에 가까울수록) 모델의 분류 성능이 우수함을 의미합니다. (0.5이면 무작위 추측과 같습니다.)\n",
    "\n",
    "④ 모델 평가: AUC는 이진 분류 모델의 성능을 임계값에 의존하지 않고 종합적으로 평가할 때 사용하는 대표적인 지표입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4178bd22",
   "metadata": {},
   "source": [
    "# 67. 교차검증 기법 중 학습용과 검증용 데이터로 나누어 검증하며, 재표본추출(Re-sampling)을 수행하지 않는 기법은?\n",
    "① 리브-p-아웃 교차검증(Leave-p-Out Cross Validation, LPoCV) ② 리브-원-아웃 교차검증(Leave-One-Out Cross Validation, LOOCV) ③ k-폴드 교차검증(k-fold CV) ④ 홀드아웃(Hold-out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3eb05b",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.정답: ④ 홀드아웃(Hold-out)해설모델의 일반화 성능을 평가하기 위한 데이터 분할 기법들의 차이점을 묻는 문제입니다.홀드아웃(Hold-out): 가장 단순한 데이터 분할 방법입니다. 전체 데이터를 **학습용(Training)**과 **시험용(Test/Validation)**으로 단 한 번만 나누어 사용합니다. 데이터를 섞어서 다시 뽑는 재표본추출(Re-sampling)이나 반복적인 검증 과정이 없기 때문에 계산이 빠르지만, 데이터를 어떻게 나누느냐에 따라 평가 결과가 달라지는 단점이 있습니다.오답 분석 (재표본추출/반복 검증 기법)① 리브-p-아웃(LPoCV): 전체 $n$개 데이터 중 $p$개를 검증용으로, $n-p$개를 학습용으로 사용하여 가능한 모든 조합을 반복 검증합니다.② 리브-원-아웃(LOOCV): $p=1$인 경우로, 단 한 개의 데이터만 검증용으로 쓰고 나머지는 학습용으로 씁니다. 데이터 개수만큼 반복 검증을 수행합니다.③ k-폴드 교차검증(k-fold CV): 데이터를 k개의 그룹(Fold)으로 나누어, 각 그룹을 한 번씩 검증용으로 사용하고 나머지를 학습용으로 사용하여 총 k번 반복 검증합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233bf405",
   "metadata": {},
   "source": [
    "# 68. 다음 중 교차검증의 목적으로 가장 거리가 먼 것은?\n",
    "① 확보된 데이터의 수가 적은 분석 신뢰도에 대한 평가 ② 학습 결과로 생성된 모델에 대한 성능을 측정하고 평가 ③ 학습 단계에서의 과적합(Over Fitting) 문제 해결 ④ 데이터의 오차에 대한 교정을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ff47c",
   "metadata": {},
   "source": [
    "정답: ④ 데이터의 오차에 대한 교정을 수행\n",
    "해설\n",
    "교차검증(Cross Validation)의 본질: 교차검증은 모델의 일반화 성능을 평가하고 과적합을 방지하기 위한 '평가 및 검증' 기법이지, 데이터 자체에 포함된 노이즈나 수치적 오차를 직접적으로 수정하거나 교정하는 알고리즘이 아닙니다. 데이터 오차 교정은 전처리(Preprocessing) 단계에서 수행해야 할 작업입니다.\n",
    "\n",
    "다른 보기 설명 (교차검증의 주요 목적)\n",
    "① 데이터 부족 시 신뢰도 평가: 데이터를 여러 번 나누어 학습과 검증을 반복하므로, 한정된 데이터 안에서 최대한 객관적인 성능 지표를 얻을 수 있습니다.\n",
    "\n",
    "② 모델 성능 측정 및 평가: 모델이 새로운 데이터에 대해 얼마나 잘 작동할지(일반화 성능)를 가늠하는 가장 표준적인 방법입니다.\n",
    "\n",
    "③ 과적합(Overfitting) 문제 해결: 특정 학습 데이터셋에만 과하게 최적화되는 것을 방지하여, 모델의 편향을 줄이고 안정성을 높입니다.\n",
    "\n",
    "정리: 교차검증의 핵심 이점\n",
    "모든 데이터를 학습과 검증에 사용할 수 있음 (데이터 효율성).\n",
    "\n",
    "특정 데이터 분할에 따른 성능 왜곡(평가 결과의 변동)을 줄임.\n",
    "\n",
    "하이퍼파라미터 튜닝 시 최적의 모델 설정을 찾는 기준이 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c0232",
   "metadata": {},
   "source": [
    "# 69. 다음 중 카이제곱 수식으로 알맞은 것은? \n",
    "($o_i$: $i$번째 범주의 관측도수, $e_i$: $i$번째 범주의 기대도수)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56d135",
   "metadata": {},
   "source": [
    "① $\\sum_{i=1}^k \\frac{(o_i - e_i)}{e_i}$ ② $\\sum_{i=1}^k \\frac{(o_i - e_i)^2}{e_i}$ ③ $\\sum_{i=1}^k \\frac{(e_i - o_i)}{o_i}$ ④ $\\sum_{i=1}^k \\frac{(e_i - o_i)^2}{o_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee55cd",
   "metadata": {},
   "source": [
    "정답: ② $\\sum_{i=1}^k \\frac{(o_i - e_i)^2}{e_i}$해설**카이제곱 통계량($\\chi^2$)**은 범주형 데이터에 대해 **관측된 값($o_i$)**이 **기대했던 값($e_i$)**과 통계적으로 유의미하게 차이가 나는지를 검정할 때 사용합니다.분자 $(o_i - e_i)^2$: 관측도수와 기대도수의 차이(편차)를 구한 뒤, 음수가 나오지 않도록 제곱합니다. 차이가 클수록 통계량의 값도 커집니다.분모 $e_i$: 차이의 제곱을 다시 기대도수로 나누어 표준화합니다. 기대치에 비해 실제 편차가 어느 정도인지를 비율로 보는 것입니다.시그마($\\sum$): 모든 범주($k$개)에서 계산된 값을 합산하여 최종 카이제곱 통계량을 산출합니다.카이제곱 검정의 주요 용도적합도 검정 (Goodness of Fit): 표본 집단의 분포가 특정한 이론적 분포(예: 정규분포 등)를 따르는지 검정합니다.독립성 검정 (Test of Independence): 두 범주형 변수(예: 성별과 선호 메뉴) 간에 연관성이 있는지, 아니면 서로 독립적인지 검정합니다.동질성 검정 (Test of Homogeneity): 여러 모집단에서 추출한 표본들의 범주별 비율이 서로 같은지 검정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b970f",
   "metadata": {},
   "source": [
    "# 70. 다음 가설 검정 설명 중 틀린 것은?\n",
    "① P값(p-value)이 유의수준보다 클 때, 귀무가설을 기각한다. ② 대립가설이 방향성을 갖는 경우를 단측검정이라 한다. ③ 모수는 모집단 분포 특성을 규정짓는 척도이며, 표본을 이용하여 이 모수에 대해 유의성을 검정한다. ④ 기각역이란 P값(p-value)이 유의수준보다 같거나 작아지게 하는 검정통계량의 영역이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a0314",
   "metadata": {},
   "source": [
    "정답: ① P값(p-value)이 유의수준보다 클 때, 귀무가설을 기각한다.해설가설 검정에서 p-value와 **유의수준($\\alpha$)**의 관계를 정확히 이해하는 것이 핵심입니다.p-value와 기각 결정: p-value는 \"귀무가설이 참이라는 가정하에, 현재와 같은 데이터가 관측될 확률\"입니다.$p \\text{-value} \\le \\text{유의수준}(\\alpha)$: 관측된 결과가 일어날 확률이 너무 희박하므로, 귀무가설을 기각하고 대립가설을 채택합니다.$p \\text{-value} > \\text{유의수준}(\\alpha)$: 귀무가설을 기각할 만큼 증거가 충분하지 않으므로, 귀무가설을 기각하지 못합니다(채택).따라서 ①번은 \"기각한다\"가 아니라 \"기각할 수 없다\"가 되어야 옳은 설명입니다.다른 보기 설명 (옳은 성질)② 단측검정(One-tailed test): \"A가 B보다 크다\" 또는 \"작다\"처럼 대립가설에 방향성이 있는 경우입니다. 반면 \"차이가 있다(같지 않다)\"는 방향성이 없는 양측검정을 사용합니다.③ 모수와 검정: 가설 검정의 궁극적인 목적은 표본 데이터를 통해 우리가 알지 못하는 **모집단의 특성(모수)**에 대한 가설이 통계적으로 타당한지 확인하는 것입니다.④ 기각역(Rejection Region): 검정통계량이 이 영역에 속하면 귀무가설을 기각하게 되는 구간입니다. 이 영역에 통계량이 들어왔다는 것은 p-value가 유의수준보다 작아졌다는 것과 같은 의미입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de8ad5",
   "metadata": {},
   "source": [
    "# 71. 다음 중 적합도 검정과 관련 없는 지표는?\n",
    "① y절편 ② P값(P-Value) ③ F통계량 ④ 잔차 히스토그램"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e22bb64",
   "metadata": {},
   "source": [
    "정답: ① y절편해설회귀 분석이나 통계 모델에서 **적합도 검정(Goodness of Fit)**은 \"우리가 만든 모델이 실제 데이터를 얼마나 잘 설명하고 반영하는가?\"를 판단하는 과정입니다.y절편(y-intercept): 독립변수($x$)가 0일 때 종속변수($y$)의 값을 나타내는 회귀 계수의 하나일 뿐, 모델이 데이터에 얼마나 잘 들어맞는지를 나타내는 '검정 지표'는 아닙니다.다른 보기 설명 (적합도 검정과 관련된 지표)② P값(P-Value): 회귀 모델 전체가 통계적으로 유의미한지(F-검정의 p-value) 혹은 개별 변수가 유의미한지 판단하여 모델의 타당성을 검토합니다.③ F통계량: \"회귀 모델이 무의미하다\"는 귀무가설을 검정하여, 모델 전체의 적합성을 평가하는 핵심 지표입니다.④ 잔차 히스토그램: 회귀 분석의 기본 가정인 잔차의 정규성을 시각적으로 확인하는 도구입니다. 잔차가 정규분포를 따라야 모델이 데이터에 적절히 적합되었다고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5112ca1a",
   "metadata": {},
   "source": [
    "# 72. 인공신경망 과적합을 막기 위한 길로 옳지 않은 것은?\n",
    "① 설명변수의 수를 늘린다. ② 학습률 줄이기 ③ 가중치 조절 ④ 은닉층 줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1af51d",
   "metadata": {},
   "source": [
    "정답: ① 설명변수의 수를 늘린다.\n",
    "해설\n",
    "**과적합(Overfitting)**은 모델이 훈련 데이터에 너무 과하게 맞춰져서 새로운 데이터(테스트 데이터)에 대한 예측력이 떨어지는 현상을 말합니다. 보통 모델이 불필요하게 복잡할 때 발생합니다.\n",
    "\n",
    "설명변수의 수와 과적합: 설명변수(독립변수)의 수를 늘리면 모델은 데이터의 아주 사소한 노이즈까지 학습하게 되어 모델이 훨씬 복잡해집니다. 이는 과적합을 유발하는 직접적인 원인이 됩니다. 따라서 과적합을 막기 위해서는 오히려 변수를 선택하거나 차원을 축소하여 변수의 수를 적절히 유지해야 합니다.\n",
    "\n",
    "다른 보기 설명 (과적합 방지 전략)\n",
    "② 학습률(Learning Rate) 줄이기: 학습률이 너무 크면 가중치가 최적값을 지나쳐 발산할 수 있습니다. 적절히 작은 학습률은 모델이 안정적으로 수렴하도록 도와 과적합 위험을 줄일 수 있습니다.\n",
    "\n",
    "③ 가중치 조절(Regularization): L1, L2 규제(Penalty) 등을 통해 가중치의 값이 너무 커지지 않도록 제한함으로써 모델의 복잡도를 낮추고 일반화 성능을 높입니다.\n",
    "\n",
    "④ 은닉층 줄이기: 신경망의 층(Layer)이 너무 깊거나 은닉 노드가 너무 많으면 모델의 자유도가 높아져 과적합이 발생하기 쉽습니다. 층의 개수를 줄이는 것은 모델을 단순화하는 효과적인 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241e578",
   "metadata": {},
   "source": [
    "# 73. 파라미터 최적화 기법 중 아래 설명 기법은?\n",
    "\n",
    "파라미터의 기울기를 구해 기울어진 방향으로 파라미터값을 갱신하는 일을 반복해 최적의 값을 추출하는 기법이다. 급격한 변곡점의 한계를 보인다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33fc793",
   "metadata": {},
   "source": [
    "① 확률적 경사하강법(Stochastic Gradient Descent, SGD) ② 모멘텀(Momentum) ③ 아다그라드(Adagrad) ④ 아담(Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd8202",
   "metadata": {},
   "source": [
    "정답: ① 확률적 경사하강법(Stochastic Gradient Descent, SGD)\n",
    "해설\n",
    "이 문제는 기본적인 경사하강법(Gradient Descent) 계열의 최적화 기법을 구분하는 문제입니다.\n",
    "\n",
    "핵심 원리: 파라미터의 기울기(Gradient)를 이용하여 손실 함수의 값이 낮아지는 방향으로 가중치를 업데이트합니다.\n",
    "\n",
    "급격한 변곡점의 한계: 기본적인 경사하강법이나 SGD는 단순히 현재 시점의 기울기 값에만 의존합니다. 이 때문에 함수의 곡면이 가파르거나 변곡점이 급격한 구간에서는 학습 속도가 매우 느려지거나, 지그재그(Oscillation) 현상이 심해져 최적값에 도달하기 어렵다는 한계를 보입니다.\n",
    "\n",
    "확률적(Stochastic)의 의미: 전체 데이터가 아닌 무작위로 선택된 일부 데이터를 사용하여 기울기를 계산함으로써, 계산 속도를 높이고 데이터가 큰 환경에서도 효율적으로 동작하게 만든 방식입니다.\n",
    "\n",
    "다른 보기 설명 (한계를 극복한 기법들)\n",
    "② 모멘텀(Momentum): \"관성\"을 이용합니다. 이전의 기울기 방향을 기억하여 급격한 변곡점이나 로컬 미니멈(Local Minimum) 구간에서 가속도를 붙여 빠르게 탈출할 수 있도록 돕습니다.\n",
    "\n",
    "③ 아다그라드(Adagrad): \"적응적\"으로 학습률을 조절합니다. 많이 변화한 파라미터는 학습률을 줄이고, 적게 변화한 파라미터는 학습률을 높여 효율적으로 학습합니다.\n",
    "\n",
    "④ 아담(Adam): 모멘텀과 RMSProp(Adagrad의 발전형)의 장점을 결합한 기법입니다. 현재 딥러닝에서 가장 표준적으로 사용되는 강력한 최적화 알고리즘입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e79455",
   "metadata": {},
   "source": [
    "# 74. 다음 중 아래 설명으로 맞는 방법론은?\n",
    "\n",
    "한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법이다. 오분류 데이터들에 높은 가중치를 부여하여 새로운 분류 규칙을 만들고 반복해 최종 예측 모델을 만드는 알고리즘이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165303a",
   "metadata": {},
   "source": [
    "① 배깅(Bagging) ② 부스팅(Boosting) ③ 랜덤포레스트(Random Forests) ④ 스태킹(Stacked Generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7a93e",
   "metadata": {},
   "source": [
    "정답: ② 부스팅(Boosting)\n",
    "해설\n",
    "앙상블(Ensemble) 기법 중 하나인 **부스팅(Boosting)**의 핵심 메커니즘을 설명하는 문제입니다.\n",
    "\n",
    "핵심 원리: 부스팅은 약한 학습기(Weak Learner)들을 **순차적(Sequential)**으로 학습시킵니다.\n",
    "\n",
    "가중치 부여: 이전 단계의 학습기가 **틀린 데이터(오분류 데이터)**에 대해 가중치를 높여서, 다음 학습기가 그 데이터를 더 잘 맞추도록 집중 학습하는 방식입니다. 이 과정을 반복하여 최종적으로 매우 강력한 모델을 만들어냅니다.\n",
    "\n",
    "주요 알고리즘: AdaBoost, Gradient Boosting Machine(GBM), XGBoost, LightGBM 등이 있습니다.\n",
    "\n",
    "다른 보기 설명\n",
    "① 배깅(Bagging): 'Bootstrap Aggregating'의 약자로, 전체 데이터를 여러 개의 병렬적인 샘플(Bootstrap)로 나누어 학습시킨 후 그 결과를 평균(회귀) 또는 투표(분류)를 통해 합치는 방식입니다. (예: 랜덤 포레스트의 기초 원리)\n",
    "\n",
    "③ 랜덤 포레스트(Random Forests): 배깅의 대표적인 모델로, 여러 개의 의사결정나무를 독립적으로 만들어서 결과를 합치는 방식입니다. 변수를 무작위로 선택하여 모델 간의 상관관계를 줄이는 특징이 있습니다.\n",
    "\n",
    "④ 스태킹(Stacking): 여러 가지 서로 다른 모델들의 예측 결과를 다시 새로운 모델의 입력 데이터로 사용하여 최종 예측을 수행하는 메타 학습 기법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f0866",
   "metadata": {},
   "source": [
    "# 75. 다음 중 시각화 기법이 아닌 것은? \n",
    "① 산점도 ② 박스플롯 ③ 히스토그램 ④ 원형인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd26361e",
   "metadata": {},
   "source": [
    "정답: ④ 원형인코딩 (One-Hot Encoding)\n",
    "해설\n",
    "데이터 분석 과정에서 데이터를 시각적으로 표현하는 시각화 기법과 데이터를 모델이 이해할 수 있는 수치로 바꾸는 전처리 기법을 구분하는 문제입니다.\n",
    "\n",
    "④ 원형인코딩(원-핫 인코딩): 범주형 데이터(예: 성별, 지역 등)를 컴퓨터가 계산할 수 있도록 0과 1로 이루어진 벡터로 변환하는 데이터 전처리(Encoding) 기법입니다. 시각화를 위한 도구가 아닙니다.\n",
    "\n",
    "다른 보기 설명 (시각화 기법)\n",
    "① 산점도 (Scatter Plot): 두 연속형 변수 간의 관계를 좌표평면 위의 점으로 나타내어 상관관계나 분포를 파악할 때 사용합니다.\n",
    "\n",
    "② 박스플롯 (Box Plot): 데이터의 최솟값, 제1사분위수, 중앙값, 제3사분위수, 최댓값을 상자 모양으로 나타내어 **분포의 비대칭성과 이상치(Outlier)**를 파악하기 좋습니다.\n",
    "\n",
    "③ 히스토그램 (Histogram): 연속형 데이터를 구간(Bin)별로 나누어 빈도를 막대 형태로 나타낸 것으로, 데이터의 분포 형태와 치우침을 파악할 때 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70242eb6",
   "metadata": {},
   "source": [
    "# 76. 시각화에 대한 성과지표 설명으로 가장 옳지 않은 것은? \n",
    "① 데이터 시각화는 데이터 분석 결과를 쉽게 이해할 수 있도록 시각적으로 표현하고 전달되는 과정이다. ② 비즈니스에 기여도 평가의 정량적 가치를 측정할 수 있다. ③ 데이터 시각화를 위해 새로운 지표를 생성하지 않아도 된다. ④ 비즈니스 의사결정에 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ae20e",
   "metadata": {},
   "source": [
    "정답: ③ 데이터 시각화를 위해 새로운 지표를 생성하지 않아도 된다.\n",
    "해설\n",
    "데이터 시각화의 목적과 성과 측정에 관한 이해를 묻는 문제입니다.\n",
    "\n",
    "지표 생성의 필요성: 효율적인 데이터 시각화와 그에 따른 성과를 측정하기 위해서는 기존의 단순한 데이터 수치 외에, 비즈니스 목적에 부합하는 **핵심 성과 지표(KPI)**나 시각화의 효과성을 판단할 수 있는 새로운 파생 지표를 생성하여 관리하는 것이 필수적입니다. 지표가 없다면 시각화가 비즈니스 목표 달성에 얼마나 기여했는지 객관적으로 평가할 수 없기 때문입니다.\n",
    "\n",
    "다른 보기 설명 (옳은 성질)\n",
    "① 시각화의 정의: 데이터 시각화의 가장 근본적인 목적으로, 복잡한 분석 결과를 그래프나 도표를 통해 직관적으로 전달하는 과정을 의미합니다.\n",
    "\n",
    "② 정량적 가치 측정: 시각화를 통해 도출된 인사이트가 실제 매출 증대, 비용 절감, 공정 시간 단축 등 비즈니스 성과에 얼마나 기여했는지 수치화하여 평가할 수 있습니다.\n",
    "\n",
    "④ 의사결정 지원: 경영진이나 실무자가 데이터를 한눈에 파악하여 신속하고 정확한 의사결정을 내릴 수 있도록 돕는 핵심적인 도구입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e3b664",
   "metadata": {},
   "source": [
    "# 77. 다음 중 아래 그래프의 설명으로 알맞은 것은? (이미지: 카르토그램 또는 육각형 격자 지도 형태) \n",
    "① 수치화된 데이터값의 크기를 서로 다른 크기의 원형으로 표현한다. ② 지리적 단위로 데이터의 의미를 색상으로 구분하여 표현한다. ③ 지역의 값을 표현하기 위해 형상 크기를 조절, 왜곡한 화면으로 표기한다. ④ 범주별 데이터의 비율을 나타내는 데 사용되며 각 조각의 크기는 각 범주의 비율이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa992d",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.\n",
    "\n",
    "정답: ③ 지역의 값을 표현하기 위해 형상 크기를 조절, 왜곡한 화면으로 표기한다.\n",
    "해설\n",
    "문제에서 설명하는 그래프는 **카르토그램(Cartogram)**에 대한 설명입니다.\n",
    "\n",
    "카르토그램의 핵심: 일반적인 지도는 실제 면적을 기준으로 그리지만, 카르토그램은 특정 데이터 값(예: 인구수, GDP, 선거득표수 등)의 크기에 따라 지리적 형상의 크기를 의도적으로 왜곡하거나 조절하여 표현합니다.\n",
    "\n",
    "시각적 효과: 데이터가 큰 지역은 실제 영토보다 크게 부풀리고, 데이터가 작은 지역은 작게 축소하여 데이터의 비중을 직관적으로 전달합니다.\n",
    "\n",
    "다른 보기 설명\n",
    "① 버블 차트(Bubble Chart) 또는 심볼 지도: 데이터의 크기를 원(버블)의 크기로 나타내는 방식입니다. 지리적 형상 자체를 왜곡하지는 않습니다.\n",
    "\n",
    "② 단계구분도(Choropleth Map): 지리적 단위를 실제 면적 그대로 유지하되, 수치의 높고 낮음을 색상의 명도나 채도로 구분하여 표현하는 가장 일반적인 통계 지도입니다.\n",
    "\n",
    "④ 파이 차트(Pie Chart): 전체에서 각 범주가 차지하는 비율을 원의 조각 형태로 나타내는 시각화 도구입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facef708",
   "metadata": {},
   "source": [
    "# 78. 아래 그래프와 같은 유형의 빅데이터 시각화 기법인 것은? (이미지: 버블 차트 형태의 산점도) \n",
    "① 카르토그램 ② 산점도 ③ 등치선도 ④ 스타차트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb34a99",
   "metadata": {},
   "source": [
    "제시된 문제의 정답과 해설은 다음과 같습니다.\n",
    "\n",
    "정답: ② 산점도 (Scatter Plot)\n",
    "해설\n",
    "문제에서 언급된 **'버블 차트 형태의 산점도'**는 시각화의 기본 기법 중 하나인 산점도에 데이터의 크기를 나타내는 '버블(원)'을 추가한 형태입니다.\n",
    "\n",
    "산점도(Scatter Plot): 두 변수 간의 관계를 좌표평면(X축, Y축) 상의 점으로 나타내는 기법입니다. 여기에 세 번째 변수의 크기에 따라 점의 크기를 다르게 표현하면 이를 버블 차트라고 부르며, 넓은 의미에서 산점도의 확장형으로 분류됩니다.\n",
    "\n",
    "특징: 변수들 사이의 상관관계, 데이터의 분포, 군집(Cluster) 여부를 파악하는 데 매우 유용합니다.\n",
    "\n",
    "다른 보기 설명\n",
    "① 카르토그램 (Cartogram): 앞선 문제에서 설명했듯이, 데이터 값에 따라 지도의 면적을 의도적으로 왜곡하여 표현하는 지도 시각화 기법입니다.\n",
    "\n",
    "③ 등치선도 (Isopleth Map): 고도나 기온, 강수량처럼 값이 같은 지점을 선으로 연결하여 나타낸 지도입니다(예: 등고선, 등압선).\n",
    "\n",
    "④ 스타 차트 (Star Chart): 여러 변수를 하나의 원형 그래프에 방사형으로 펼쳐서 표현하는 기법으로, '거미줄 차트(Radar Chart)'라고도 불립니다. 개별 대상의 다각형 모양을 통해 특성을 비교할 때 쓰입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421757e1",
   "metadata": {},
   "source": [
    "# 79. 다음은 어떤 시각화 기법에 대한 설명인가?\n",
    "\n",
    "다변량 데이터 사이에 존재하는 변수 사이의 연관성, 분포와 패턴을 찾음. 집단 간의 상관 관계를 확인. 수치의 변화 예측. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227e431",
   "metadata": {},
   "source": [
    "① 시간시각화 ② 관계시각화 ③ 비교시각화 ④ 공간시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41978d",
   "metadata": {},
   "source": [
    "정답: ② 관계시각화\n",
    "해설\n",
    "데이터 시각화의 목적에 따른 분류를 묻는 문제입니다.\n",
    "\n",
    "관계시각화(Relationship Visualization): 두 개 이상의 변수 사이에 존재하는 연관성, 상관관계, 그리고 상호작용을 파악하는 것이 주된 목적입니다.\n",
    "\n",
    "설명 속 특징:\n",
    "\n",
    "\"변수 사이의 연관성, 분포와 패턴\": 변수들이 서로 어떻게 얽혀 있는지 확인합니다.\n",
    "\n",
    "\"집단 간의 상관관계\": 한 변수가 변할 때 다른 변수가 어떻게 변하는지 측정합니다.\n",
    "\n",
    "\"수치의 변화 예측\": 상관성을 바탕으로 미래의 수치를 가늠해 볼 수 있습니다.\n",
    "\n",
    "대표적인 기법: 산점도(Scatter Plot), 버블 차트(Bubble Chart), 히트맵(Heat Map) 등이 이에 해당합니다.\n",
    "\n",
    "다른 보기 설명\n",
    "① 시간시각화: 시간에 따른 데이터의 추세나 변화를 파악하는 기법입니다. (예: 선 그래프, 막대 그래프, 영역 차트 등)\n",
    "\n",
    "③ 비교시각화: 여러 항목 또는 집단 간의 수치를 비교하여 크고 작음을 파악하는 기법입니다. (예: 막대 차트, 스타 차트, 파이 차트 등)\n",
    "\n",
    "④ 공간시각화: 지리적 위치 정보(위도, 경도)를 바탕으로 데이터의 분포를 지도 위에 표현하는 기법입니다. (예: 단계구분도, 카르토그램, 등치선도 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13780200",
   "metadata": {},
   "source": [
    "# 80. 다음 중 분석결과를 활용한 데이터 시각화를 통해 얻을 수 있는 대표적인 기능이 아닌 것은? \n",
    "① 커뮤니케이션 ② 인사이트 발굴 ③ 연동 모델 선정 ④ 디스커버리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87c774",
   "metadata": {},
   "source": [
    "정답: ③ 연동 모델 선정\n",
    "해설\n",
    "데이터 시각화의 본질적인 목적과 기능을 구분하는 문제입니다.\n",
    "\n",
    "연동 모델 선정: 분석을 위해 어떤 알고리즘(회귀, 신경망, SVM 등)을 사용할지 결정하거나 모델을 연동하는 과정은 분석 기획이나 모델링(Modeling) 단계에서 수행되는 전문적인 기술적 영역입니다. 시각화는 이미 분석된 결과나 원천 데이터의 패턴을 보여주는 역할을 할 뿐, 직접적으로 모델링 알고리즘 자체를 선정하는 기능을 수행하지는 않습니다.\n",
    "\n",
    "다른 보기 설명 (시각화의 3대 핵심 기능)\n",
    "① 커뮤니케이션(Communication): 복잡한 분석 결과를 이해관계자에게 쉽고 명확하게 전달하여 의사결정을 돕는 가장 강력한 도구입니다.\n",
    "\n",
    "② 인사이트 발굴(Insight): 수치로만 볼 때 파악하기 힘든 데이터 간의 숨겨진 의미나 특이사항을 시각적으로 즉각 포착할 수 있게 합니다.\n",
    "\n",
    "④ 디스커버리(Discovery): 탐색적 데이터 분석(EDA) 과정을 통해 가설을 검증하거나, 데이터 속에 숨겨진 새로운 패턴과 규칙을 발견(탐색)하도록 돕습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3bba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
