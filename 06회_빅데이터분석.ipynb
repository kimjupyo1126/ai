{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5c237b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:99% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
       "div.text_cell_render.rendered_html{font-size:20pt;}\n",
       "div.text_cell_render li, div.text_cell_render p, code{font-size:22pt; line-height:30px;}\n",
       "div.output {font-size:24pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:24pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
       "table.dataframe{font-size:24px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:99% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
    "div.text_cell_render.rendered_html{font-size:20pt;}\n",
    "div.text_cell_render li, div.text_cell_render p, code{font-size:22pt; line-height:30px;}\n",
    "div.output {font-size:24pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:24pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
    "table.dataframe{font-size:24px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f80dd",
   "metadata": {},
   "source": [
    "# 01. 정형, 반정형, 비정형으로 구분하는 빅데이터 특성은? \n",
    "① 속도 ② 규모 ③ 다양성 ④ 가치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce6b35e",
   "metadata": {},
   "source": [
    "빅데이터의 주요 특징을 설명하는 '3V' 모델에 따르면, 데이터의 형태에 따른 구분은 다음과 같은 의미를 갖습니다.데이터 유형별 분류 (다양성)빅데이터는 단순히 양이 많은 것뿐만 아니라, 아래와 같이 다양한 형태의 데이터를 포함하는 것이 특징입니다.구분설명예시정형 데이터 (Structured)고정된 필드에 저장되어 형태가 일정한 데이터관계형 데이터베이스(RDB), 스프레드시트(Excel)반정형 데이터 (Semi-structured)고정된 형태는 없으나, 데이터 내부에 구조를 설명하는 메타데이터가 포함된 데이터XML, JSON, HTML, 로그 파일비정형 데이터 (Unstructured)일정한 규격이나 형태가 전혀 없는 데이터텍스트 문서, 이미지, 오디오, 영상, SNS 메시지참고: 빅데이터의 3V규모 (Volume): 데이터의 물리적인 양이 방대함.속도 (Velocity): 데이터의 생성, 유통 및 분석 속도가 매우 빠름.다양성 (Variety): 정형, 반정형, 비정형 등 데이터의 종류가 다양함. (문제의 정답)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fd8b5",
   "metadata": {},
   "source": [
    "# 02. 빅데이터 위기요인이 아닌 것은?\n",
    "① 사생활 침해 ② 데이터 오용 ③ 책임원칙 훼손 ④ 인간과 인간의 연결 가능성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f155cf",
   "metadata": {},
   "source": [
    "정답은 ④ 인간과 인간의 연결 가능성입니다.빅데이터 전문가인 빅토르 마이어 쇤베르거(Viktor Mayer-Schönberger)는 저서 『빅데이터가 만드는 세상』을 통해 빅데이터 시대의 3대 위기요인을 다음과 같이 정의했습니다.빅데이터 3대 위기요인 및 통제 방안위기요인주요 내용해결(통제) 방안① 사생활 침해개인정보가 본래 목적 외에 가공·처리되어 사생활이 노출되거나 2차적 목적으로 남용될 위험동의제에서 책임제로 전환: 데이터 수집 시 '동의'를 받는 것보다 사용자의 '책임'을 강화함② 책임원칙 훼손분석 알고리즘이 특정인의 행동을 예측하여, 실제로 범죄를 저지르기 전에 불이익을 주는 등 잠재적 위협에 책임을 묻는 행위결과 기반 책임 원칙 고수: 예측된 행동이 아니라 실제 행동(결과)에 대해서만 책임을 묻도록 함③ 데이터 오용잘못된 지표나 데이터 분석 결과를 맹신하여 부정확한 결정을 내리거나 사회적 갈등을 유발하는 경우알고리즘 접근 허용 (알고리즈미스트): 분석 알고리즘의 부당함을 검증할 전문가(알고리즈미스트)를 통해 투명성을 확보함오답 해설④ 인간과 인간의 연결 가능성: 이는 빅데이터나 IT 기술 발전의 **긍정적 측면(가치)**이나 사회적 현상으로 볼 수 있으며, 그 자체를 '위기요인'으로 분류하지는 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319abb9",
   "metadata": {},
   "source": [
    "# 04. 데이터 분석 조직에 관한 설명으로 옳지 않은 것은?\n",
    "① 기능형은 전사의 핵심업무를 분석하지 못한다. ② 집중구조는 별도의 분석조직이 존재하므로 협업조직과의 업무 중복 가능성이 없다. ③ 분산구조는 전담조직 인력을 현업부서에 배치하므로 신속한 업무에 적합하다. ④ 기능형은 별도로 분석조직이 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c535003",
   "metadata": {},
   "source": [
    "정답은 ② 집중구조는 별도의 분석조직이 존재하므로 협업조직과의 업무 중복 가능성이 없다입니다.분석 조직의 유형은 크게 기능형, 집중구조, 분산구조의 세 가지로 나뉩니다. 각 구조의 핵심적인 특징과 차이점을 이해하면 쉽게 풀 수 있는 문제입니다.분석 조직 구조의 특징 비교조직 구조주요 특징장점단점기능 구조 (Functional)별도의 분석 조직 없이 각 현업 부서가 직접 분석 수행업무 밀착형 분석 가능전사적 관점의 핵심 분석 어려움, 부서 간 중복 가능성집중 구조 (Centralized)별도의 전담 조직이 전사의 모든 분석 업무를 담당전략적 중요도에 따른 우선순위 추진, 전사적 통제 용이현업 부서와의 업무 중복 및 이원화 가능성 존재분산 구조 (Decentralized)전담 조직 인력을 각 현업 부서에 **배치(파견)**하여 수행분석 결과의 신속한 현업 적용, 부서 간 협업 용이전사 차원의 관리 및 성과 평가가 어려울 수 있음선택지 상세 해설① 기능형은 전사의 핵심업무를 분석하지 못한다. (O): 각 부서 단위로 분석이 이루어지기 때문에, 여러 부서가 얽힌 전사적 관점의 핵심 과제를 도출하고 수행하는 데 한계가 있습니다.② 집중구조는 업무 중복 가능성이 없다. (X): 전담 조직이 모든 것을 관리하려다 보니, 정작 실제 업무를 수행하는 현업 부서에서 자체적으로 하는 분석 활동과 **중복되거나 업무가 이원화(이중화)**될 위험이 매우 큽니다.③ 분산구조는 전담조직 인력을 현업부서에 배치하므로 신속한 업무에 적합하다. (O): 분석가가 현업 부서에 소속되어 함께 움직이므로, 비즈니스 요구사항을 즉각 반영하여 분석 결과를 업무에 빠르게 적용할 수 있습니다.④ 기능형은 별도로 분석조직이 없다. (O): 기존의 조직 체계 내에서 필요할 때마다 분석을 수행하는 형태이므로 별도의 전문 조직이 존재하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b49601",
   "metadata": {},
   "source": [
    "# 05. 분석준비도(Readiness)의 진단영역으로 옳지 않은 것은? \n",
    "① 분석업무 ② 분석인력 ③ 분석기법 ④ 분석결과활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c181670",
   "metadata": {},
   "source": [
    "정답은 ④ 분석결과활용입니다.\n",
    "\n",
    "데이터 분석 수준 진단은 크게 **분석 준비도(Readiness)**와 분석 성숙도(Maturity) 두 가지 축으로 나뉩니다. 문제에서 묻는 '분석 준비도'는 기업이 데이터를 분석할 준비가 얼마나 되어 있는지를 6가지 영역으로 진단합니다.\n",
    "\n",
    "분석 준비도(Readiness)의 6대 진단 영역\n",
    "분석 준비도는 현재 조직의 인프라와 역량을 파악하는 데 중점을 둡니다.\n",
    "\n",
    "분석 업무: 발생한 사실 분석, 예측 분석, 시뮬레이션, 최적화 업무 등 분석 대상 업무의 존재 여부.\n",
    "\n",
    "분석 인력 및 조직: 분석 전문가, 분석 조직, 전사적인 분석 역량 교육 등.\n",
    "\n",
    "분석 기법: 분석 방법론, 분석 라이브러리, 기법의 활용 및 성과 관리.\n",
    "\n",
    "분석 데이터: 데이터 품질, 충분성, 신뢰성, 비정형 데이터 관리 체계 등.\n",
    "\n",
    "분석 문화: 경영진의 분석 의지, 데이터 기반 의사결정 문화, 분석 공유 문화.\n",
    "\n",
    "IT 인프라: 분석 전용 서버, 분석 소프트웨어, 데이터 보안 및 클라우드 활용 환경.\n",
    "\n",
    "왜 '분석결과활용'은 오답인가요?\n",
    "분석결과활용은 분석 준비도가 아니라 **분석 성숙도(Maturity)**의 진단 영역에 해당합니다.\n",
    "\n",
    "분석 성숙도: 비즈니스 부문, 조직·역량 부문, IT 부문에서 분석이 얼마나 체계적으로 내재화되어 있고, 그 결과를 실제 업무에 얼마나 활용하고 있는지를 평가합니다. (도입 → 활용 → 확산 → 최적화 단계로 구분)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89dc70",
   "metadata": {},
   "source": [
    "# 06. 다음 중 데이터 거버넌스의 구성요소에 해당하지 않는 것은? \n",
    "① 원칙 ② IT인프라 ③ 프로세스 ④ 조직"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f93dfe",
   "metadata": {},
   "source": [
    "정답은 ② IT인프라입니다.\n",
    "\n",
    "데이터 거버넌스(Data Governance)는 데이터의 품질 확보, 유통 및 보안 등 데이터의 체계적인 관리를 위해 수행하는 모든 활동과 관리 체계를 의미합니다. 일반적으로 데이터 거버넌스의 3대 구성요소는 다음과 같습니다.\n",
    "\n",
    "데이터 거버넌스의 3대 구성요소\n",
    "데이터 거버넌스가 올바르게 작동하려면 정책(원칙), 사람(조직), 절차(프로세스)가 유기적으로 결합되어야 합니다.\n",
    "\n",
    "원칙 (Principle): 데이터를 유지·관리하기 위한 지침과 가이드입니다.\n",
    "\n",
    "예: 보안 정책, 품질 기준, 데이터 변경 관리 가이드라인 등\n",
    "\n",
    "조직 (Organization): 데이터를 관리할 조직의 역할과 책임(R&R)을 정의합니다.\n",
    "\n",
    "예: 데이터 관리자(Data Steward), 데이터베이스 관리자(DBA), 데이터 아키텍트 등\n",
    "\n",
    "프로세스 (Process): 데이터를 관리하기 위한 구체적인 활동과 작업 체계입니다.\n",
    "\n",
    "예: 데이터 작업 절차, 모니터링 활동, 측정 및 보고 활동 등\n",
    "\n",
    "왜 'IT인프라'는 구성요소가 아닌가요?\n",
    "IT인프라는 데이터를 처리하고 저장하기 위한 기술적 수단(서버, 스토리지, 네트워크 등)입니다.\n",
    "\n",
    "물론 거버넌스를 실행하기 위해 시스템이나 도구가 필요할 수는 있지만, 거버넌스의 핵심 개념인 '의사결정 체계'와 '관리 프레임워크' 관점에서는 원칙, 조직, 프로세스를 3대 요소로 꼽습니다.\n",
    "\n",
    "IT인프라는 보통 앞서 풀었던 **'분석 준비도(Readiness)'**의 진단 영역 중 하나로 분류됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21805a7",
   "metadata": {},
   "source": [
    "# 07. 데이터 사이언스에 대한 설명으로 알맞은 것은? \n",
    "① 가능한 많은 데이터를 모으기만 하면 의미가 도출된다. ② 특정한 상관관계가 중요시되던 과거와 달리, 인과관계를 통한 인사이트 도출이 점점 확산되고 있다. ③ 의학, 공학 등 다양한 분야 연구분야에서 적용된다. ④ 데이터 소량화로 인해 급격한 확산속도가 둔화된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd832a",
   "metadata": {},
   "source": [
    "정답은 ③ 의학, 공학 등 다양한 분야 연구분야에서 적용된다. 입니다.\n",
    "\n",
    "데이터 사이언스는 단순히 데이터 분석에 그치지 않고, 다양한 학문과 비즈니스 영역에 융합되어 새로운 가치를 창출하는 포괄적인 개념입니다.\n",
    "\n",
    "선택지 상세 해설\n",
    "① 가능한 많은 데이터를 모으기만 하면 의미가 도출된다. (X): 데이터의 양(Volume)도 중요하지만, 쓰레기를 넣으면 쓰레기가 나온다는 'GIGO(Garbage In, Garbage Out)' 원칙처럼 데이터를 어떻게 정제하고 분석하느냐가 더 중요합니다.\n",
    "\n",
    "② 상관관계보다 인과관계를 통한 인사이트 도출이 확산되고 있다. (X): 전통적인 통계학은 '왜(Why)' 일어났는지에 대한 인과관계에 집중했습니다. 반면, 빅데이터와 데이터 사이언스 시대에는 '무엇이(What)' 일어나는지에 대한 상관관계 분석을 통해 빠르게 패턴을 찾아내고 예측하는 방향이 매우 중요해졌습니다.\n",
    "\n",
    "③ 의학, 공학 등 다양한 분야 연구분야에서 적용된다. (O): 데이터 사이언스는 다학제적(Multidisciplinary) 분야입니다. 유전자 분석(의학), 공정 최적화(공학), 마케팅 전략(경영) 등 거의 모든 산업과 학문 분야에서 필수적인 도구로 활용되고 있습니다.\n",
    "\n",
    "④ 데이터 소량화로 인해 확산속도가 둔화된다. (X): 오히려 데이터의 폭증(Big Data)과 컴퓨팅 파워의 발전으로 인해 데이터 사이언스의 중요성과 확산 속도는 비약적으로 빨라지고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203306a",
   "metadata": {},
   "source": [
    "# 08. 다음 중 데이터 웨어하우스와 데이터 마트에서 주로 쓰이는 데이터 수집 기술은? \n",
    "① FTP ② HTTP ③ Open API ④ DB to DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a5395",
   "metadata": {},
   "source": [
    "정답은 ④ DB to DB입니다.\n",
    "\n",
    "데이터 웨어하우스(DW)와 데이터 마트(DM)는 기업 내 여러 운영 시스템(Legacy)에 흩어져 있는 데이터를 통합하여 분석하는 것이 주된 목적입니다. 이때 가장 핵심적으로 사용되는 방식이 원천 데이터베이스에서 분석용 데이터베이스로 직접 데이터를 옮기는 기술들입니다.\n",
    "\n",
    "주요 데이터 수집 기술 설명\n",
    "DB to DB (정답):\n",
    "\n",
    "설명: 원천 시스템의 데이터베이스(DB)에서 타겟 DB(DW 또는 DM)로 데이터를 직접 전송하는 방식입니다.\n",
    "\n",
    "특징: 주로 ETL(Extract, Transform, Load) 도구를 사용하여 정형 데이터를 대량으로, 그리고 규칙적으로 수집할 때 가장 보편적으로 사용됩니다.\n",
    "\n",
    "기술 예시: Oracle Gateway, DB Link, JDBC/ODBC 연결 등.\n",
    "\n",
    "FTP (File Transfer Protocol):\n",
    "\n",
    "텍스트 파일이나 로그 파일 등 '파일' 단위의 데이터를 전송할 때 사용합니다. 아주 단순한 수집에는 쓰이지만, 실시간성이 떨어지고 DB 간의 구조적 통합에는 직접적인 기술이라 보기 어렵습니다.\n",
    "\n",
    "HTTP (HyperText Transfer Protocol):\n",
    "\n",
    "주로 웹 페이지의 데이터를 가져오거나 웹 서비스를 호출할 때 사용되는 통신 규약입니다.\n",
    "\n",
    "Open API:\n",
    "\n",
    "외부 기관(공공데이터, SNS 등)에서 제공하는 데이터를 실시간으로 호출하여 가져올 때 주로 사용합니다. 내부 시스템 간의 DW/DM 구축보다는 외부 데이터 연동에 더 특화되어 있습니다.\n",
    "\n",
    "데이터 수집 및 통합 흐름도\n",
    "데이터 웨어하우스 구축 시 데이터는 보통 다음과 같은 단계를 거쳐 흐릅니다.\n",
    "\n",
    "Source Data: 운영 DB(ERP, CRM 등), 외부 데이터, 로그 파일 등.\n",
    "\n",
    "Staging Area: 수집된 데이터를 임시로 저장하고 정제(Cleaning)하는 공간.\n",
    "\n",
    "Data Warehouse (DW): 전사의 통합된 데이터를 저장하는 거대한 창고. (DB to DB 방식 주력)\n",
    "\n",
    "Data Mart (DM): 특정 부서(마케팅, 재무 등)의 목적에 맞게 DW에서 추출한 데이터 소집단."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbece308",
   "metadata": {},
   "source": [
    "# 09. 분산파일시스템에 대한 특징으로 적절한 것은? \n",
    "① 대용량 분산 처리가 가능하다. ② 다수의 마이크로프로세서로 구성되어 있다. ③ 분산 데이터베이스를 통해 수정한다. ④ 다중의 트랜잭션을 통해 직접한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925fdf6",
   "metadata": {},
   "source": [
    "정답은 ① 대용량 분산 처리가 가능하다. 입니다.\n",
    "\n",
    "빅데이터 환경에서 가장 대표적인 분산파일시스템인 **HDFS(Hadoop Distributed File System)**를 떠올리면 쉽게 이해할 수 있는 문제입니다.\n",
    "\n",
    "분산파일시스템(DFS)의 주요 특징\n",
    "분산파일시스템은 물리적으로 떨어진 여러 대의 서버(노드)를 마치 하나의 저장 장치처럼 사용하는 기술입니다.\n",
    "\n",
    "대용량 처리: 수 페타바이트(PB) 이상의 방대한 데이터를 여러 서버에 나누어 저장하고, 병렬로 처리할 수 있는 구조를 가집니다. (선택지 ①번 내용)\n",
    "\n",
    "저사양 서버 활용: 고가의 대형 서버 한 대를 쓰는 대신, 저렴한 다수의 서버를 연결하여 확장성(Scalability)을 확보합니다.\n",
    "\n",
    "장애 허용 (Fault Tolerance): 데이터를 여러 서버에 복제하여 저장하므로, 특정 서버에 장애가 발생해도 데이터 유실 없이 서비스를 지속할 수 있습니다.\n",
    "\n",
    "오답 해설\n",
    "② 다수의 마이크로프로세서로 구성되어 있다: 이는 분산파일시스템보다는 병렬 처리 컴퓨터 구조나 하드웨어적인 설계 관점에 더 가깝습니다.\n",
    "\n",
    "③ 분산 데이터베이스를 통해 수정한다: 분산파일시스템(특히 HDFS)은 한 번 저장된 데이터는 수정하지 않고 읽기만 하는 'Write Once, Read Many' 방식이 기본입니다. 데이터 수정은 매우 제한적이거나 성능 저하를 초래하므로 데이터베이스의 수정 방식과는 차이가 있습니다.\n",
    "\n",
    "④ 다중의 트랜잭션을 통해 직접한다: 분산파일시스템은 대규모 데이터의 '배치 처리'에 최적화되어 있습니다. 빈번한 입력·수정·삭제를 처리하는 트랜잭션(Transaction) 관리는 파일 시스템이 아닌 관계형 데이터베이스(RDBMS)나 NoSQL의 특징입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d000a",
   "metadata": {},
   "source": [
    "# 10. 맵리듀스 패턴 중에 다른 데이터와 연결하여 분석하는 패턴은 무엇인가?\n",
    "① 디자인패턴 ② 요약패턴 ③ 조인패턴 ④ 필터패턴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cba0f3",
   "metadata": {},
   "source": [
    "정답은 ③ 조인패턴입니다.\n",
    "\n",
    "맵리듀스(MapReduce)는 대규모 데이터를 병렬로 처리하기 위한 프로그래밍 모델이며, 특정 목적에 따라 자주 사용되는 설계 방식인 '디자인 패턴'이 존재합니다.\n",
    "\n",
    "주요 맵리듀스 디자인 패턴\n",
    "조인 패턴 (Join Pattern) [정답]:\n",
    "\n",
    "설명: 두 개 이상의 서로 다른 데이터 세트(예: 고객 정보와 구매 내역)를 특정 키(Key)를 기준으로 결합하는 방식입니다.\n",
    "\n",
    "특징: 관계형 데이터베이스의 'JOIN'과 유사한 기능을 수행하며, 맵(Map) 단계나 리듀스(Reduce) 단계에서 결합이 이루어집니다.\n",
    "\n",
    "요약 패턴 (Summarization Pattern):\n",
    "\n",
    "설명: 데이터의 통계적 수치를 계산하는 방식입니다.\n",
    "\n",
    "예시: 수치 데이터의 합계, 평균, 최댓값/최솟값 계산 또는 단어 빈도수(Word Count) 측정 등이 해당합니다.\n",
    "\n",
    "필터링 패턴 (Filtering Pattern):\n",
    "\n",
    "설명: 전체 데이터 중 특정 조건에 맞는 데이터만 골라내거나 샘플링하는 방식입니다.\n",
    "\n",
    "예시: 로그 파일에서 에러 메시지만 추출하기, 상위 10개 데이터만 뽑기(Top-K) 등.\n",
    "\n",
    "디자인 패턴 (Design Pattern):\n",
    "\n",
    "이는 조인, 요약, 필터링 등을 모두 포괄하는 상위 개념입니다. 문제에서 묻는 구체적인 '연결 분석' 기법의 명칭은 아닙니다.\n",
    "\n",
    "조인 패턴의 구분 (심화)\n",
    "리듀스 측 조인 (Reduce-Side Join): 리듀서에서 데이터 결합이 일어나며, 여러 원천 데이터를 결합할 수 있지만 네트워크 부하가 클 수 있습니다.\n",
    "\n",
    "맵 측 조인 (Map-Side Join): 맵 단계에서 결합이 완료되어 성능이 빠르지만, 한쪽 데이터 세트가 메모리에 올라갈 만큼 작아야 한다는 제약이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02055e98",
   "metadata": {},
   "source": [
    "# 11. 다음 중 머신러닝과 딥러닝에 대한 설명으로 옳지 않은 것은? \n",
    "① 머신러닝은 주어진 데이터 패턴을 학습하고 유추하는 것이다. ② 머신러닝 학습방법으로 지도, 비지도, 강화학습이 대표적이다. ③ 머신러닝은 딥러닝의 일부이다. ④ 머신러닝을 개선하여 딥러닝으로 발전하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03a9ae",
   "metadata": {},
   "source": [
    "정답은 ③ 머신러닝은 딥러닝의 일부이다. 입니다.\n",
    "\n",
    "이 문제는 인공지능(AI), 머신러닝(ML), 딥러닝(DL)의 포함 관계를 정확히 이해하고 있는지를 묻는 문제입니다.\n",
    "\n",
    "인공지능 - 머신러닝 - 딥러닝의 관계\n",
    "인공지능이라는 가장 넓은 개념 안에 머신러닝이 포함되고, 다시 머신러닝 기법 중 하나로 딥러닝이 포함되는 구조입니다. 따라서 **\"딥러닝이 머신러닝의 일부\"**라고 말하는 것이 맞습니다.\n",
    "\n",
    "선택지 상세 해설\n",
    "① 머신러닝은 주어진 데이터 패턴을 학습하고 유추하는 것이다. (O): 데이터를 컴퓨터에게 입력시켜 명시적으로 프로그래밍하지 않아도 데이터를 통해 스스로 학습하고 패턴을 찾아내어 새로운 데이터를 예측하게 하는 기술입니다.\n",
    "\n",
    "② 머신러닝 학습방법으로 지도, 비지도, 강화학습이 대표적이다. (O):\n",
    "\n",
    "지도 학습: 정답(Label)이 있는 데이터를 학습 (회귀, 분류)\n",
    "\n",
    "비지도 학습: 정답이 없는 데이터에서 구조나 패턴을 발견 (군집화, 차원축소)\n",
    "\n",
    "강화 학습: 보상을 통해 최적의 행동을 학습 (게임, 자율주행)\n",
    "\n",
    "③ 머신러닝은 딥러닝의 일부이다. (X): 앞서 설명한 것처럼, 딥러닝이 머신러닝의 범주 안에 포함되는 하위 개념입니다. 문장의 주어와 목적어가 바뀌어 설명되었습니다.\n",
    "\n",
    "④ 머신러닝을 개선하여 딥러닝으로 발전하였다. (O): 전통적인 머신러닝 기법(의사결정나무, SVM 등)의 한계를 극복하기 위해 인간의 뇌 구조를 모사한 **인공신경망(ANN)**을 깊게 쌓은(Deep) 것이 바로 딥러닝입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d644412",
   "metadata": {},
   "source": [
    "# 12. 공공데이터와 같이 조직 외부의 데이터를 사용할 때의 장점으로 가장 적절한 것은?\n",
    "① 비용이 저렴하다. ② 내부 데이터보다 보안이 좋다. ③ 데이터 선택의 폭이 넓다. ④ 데이터 소유권을 가질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924866d",
   "metadata": {},
   "source": [
    "정답은 ③ 데이터 선택의 폭이 넓다. 입니다.\n",
    "\n",
    "기업이나 조직이 내부 데이터(Internal Data)에만 의존하지 않고, 공공데이터나 SNS, 유료 데이터 서비스 등 **외부 데이터(External Data)**를 활용하는 이유는 분석의 관점을 확장하기 위해서입니다.\n",
    "\n",
    "외부 데이터 활용의 장점\n",
    "다양한 인사이트 확보 (정답 관련): 내부 데이터(매출, 고객 정보 등)만으로는 알 수 없는 사회적 트렌드, 날씨, 인구 통계, 경쟁사 동향 등을 파악할 수 있어 분석의 시야가 넓어집니다.\n",
    "\n",
    "상황 예측 능력 향상: 예를 들어 편의점 매출 데이터를 분석할 때, 공공데이터인 '기상청 날씨 데이터'를 결합하면 기온에 따른 상품 수요를 더 정확하게 예측할 수 있습니다.\n",
    "\n",
    "오답 해설\n",
    "① 비용이 저렴하다: 공공데이터는 무료인 경우가 많지만, 민간에서 판매하는 고품질 데이터(신용카드 결제 데이터, 유통 데이터 등)는 매우 고가인 경우가 많으므로 일반적인 장점으로 보기 어렵습니다.\n",
    "\n",
    "② 내부 데이터보다 보안이 좋다: 외부 데이터는 여러 사람에게 공개된 데이터이거나 외부 망을 통해 가져오기 때문에, 오히려 보안이나 데이터 무결성 관리가 내부 데이터보다 까다로울 수 있습니다.\n",
    "\n",
    "④ 데이터 소유권을 가질 수 있다: 외부 데이터는 대여하거나 API 형태로 호출하여 사용하는 경우가 많으며, 원천 데이터에 대한 소유권은 데이터를 생성한 기관이나 기업에 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039dcbdb",
   "metadata": {},
   "source": [
    "# 13. 데이터 분석을 통한 개선사항 도출하기 위해 적절한 단계는? \n",
    "① 도메인 이슈 도출 ② 분석 목표 수립 ③ 프로젝트 계획 수립 ④ 모델 개발"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d469f4",
   "metadata": {},
   "source": [
    "정답은 ① 도메인 이슈 도출입니다.\n",
    "\n",
    "분석을 통해 실질적인 **'개선사항'**을 도출하기 위해서는, 기술적인 분석에 들어가기 앞서 우리가 해결해야 할 문제가 무엇인지 정의하는 과정이 가장 먼저 선행되어야 합니다.\n",
    "\n",
    "분석 기획의 일반적인 흐름\n",
    "분석을 통해 가치를 창출하는 과정은 보통 다음과 같은 순서로 진행됩니다.\n",
    "\n",
    "도메인 이슈 도출 (정답): 해당 산업(도메인)의 비즈니스 상황을 이해하고, 현재 어떤 문제나 개선이 필요한지 '문제 정의'를 하는 단계입니다. (예: \"우리 쇼핑몰의 이탈률이 왜 높을까?\")\n",
    "\n",
    "분석 목표 수립: 도출된 이슈를 해결하기 위해 어떤 분석을 할지 구체적인 목표를 정합니다. (예: \"이탈 고객의 특징을 파악하여 이탈 예측 모델을 만들자.\")\n",
    "\n",
    "프로젝트 계획 수립: 필요한 데이터, 인력, 예산, 일정 등을 기획합니다.\n",
    "\n",
    "데이터 수집 및 전처리: 분석에 필요한 데이터를 모으고 깨끗하게 정리합니다.\n",
    "\n",
    "모델 개발 및 분석: 알고리즘을 적용하여 실제로 분석을 수행합니다.\n",
    "\n",
    "결과 평가 및 개선사항 도출: 분석 결과를 비즈니스에 적용하고 실제 개선으로 이어지게 합니다.\n",
    "\n",
    "왜 '도메인 이슈 도출'이 가장 적절한가요?\n",
    "개선사항 도출의 출발점: 무엇을 고쳐야 할지(이슈)를 모르면 분석 결과가 나와도 어디에 써야 할지 알 수 없습니다.\n",
    "\n",
    "비즈니스 가치 중심: 분석 기술(모델 개발) 자체보다 \"어떤 비즈니스 문제를 해결할 것인가\"라는 질문이 분석의 성패를 결정짓기 때문입니다.\n",
    "\n",
    "데이터 분석의 거버넌스 체계와 연결\n",
    "분석을 기획할 때는 다음의 세 가지 요소가 조화를 이루어야 합니다.\n",
    "\n",
    "도메인 지식: 해당 분야의 전문 지식 (이슈 도출에 필수)\n",
    "\n",
    "IT 기술: 데이터를 다루고 처리하는 능력\n",
    "\n",
    "통계/수학: 분석 모델을 만들고 검증하는 능력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d93ff",
   "metadata": {},
   "source": [
    "# 14. 분석마스터 플랜에 대한 설명으로 옳은 것은? \n",
    "① 좁은 범위의 특정 주제에 대해 테스트를 실행함으로써 빠르게 문제를 해결해가는 방법이다. ② 모든 과정을 반복 수행한다. ③ 분석로드맵은 중장기적 관점의 수행 계획을 수립하는 과정 및 방법이다. ④ 프레임워크보다는 단기 과제성 계획을 수립한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713e31d",
   "metadata": {},
   "source": [
    "정답은 ③ 분석로드맵은 중장기적 관점의 수행 계획을 수립하는 과정 및 방법이다. 입니다.\n",
    "\n",
    "분석 마스터 플랜은 기업의 비즈니스 전략을 달성하기 위해 데이터 분석 과제를 발굴하고, 우선순위를 정하여 중장기적인 로드맵을 수립하는 종합적인 계획을 말합니다.\n",
    "\n",
    "분석 마스터 플랜의 핵심 구성 요소\n",
    "마스터 플랜은 크게 두 가지 관점에서 수립됩니다.\n",
    "\n",
    "우선순위 평가: 전략적 중요도, 비즈니스 성과(ROI), 실행 용이성 등을 고려하여 어떤 과제를 먼저 할지 결정합니다.\n",
    "\n",
    "이행 로드맵 수립 (정답): 선정된 과제들을 시간 순서대로 배열하여 중장기적인 추진 계획을 세우는 과정입니다.\n",
    "\n",
    "선택지 상세 해설\n",
    "① 좁은 범위의 특정 주제에 대해 빠르게 문제를 해결하는 방법 (X): 이는 마스터 플랜이 아니라 '퀵 윈(Quick-Win)' 또는 '애자일(Agile)' 방식의 분석 수행에 대한 설명입니다. 마스터 플랜은 전사적 관점의 거시적인 계획입니다.\n",
    "\n",
    "② 모든 과정을 반복 수행한다 (X): 이는 분석 방법론 중 **'나선형 모델(Spiral Model)'**이나 **'반복적 모델'**의 특징입니다. 마스터 플랜 자체가 모든 과정을 무한 반복하는 것을 의미하지는 않습니다.\n",
    "\n",
    "③ 분석로드맵은 중장기적 관점의 수행 계획이다 (O): 분석 로드맵은 단계별로 목표를 설정하고, 분석 역량을 어떻게 내재화할지 구체적인 시간 계획을 세우는 과정이므로 정확한 설명입니다.\n",
    "\n",
    "④ 프레임워크보다는 단기 과제성 계획을 수립한다 (X): 마스터 플랜은 단기적인 성과뿐만 아니라 기업의 분석 체계(Governance)를 구축하는 프레임워크를 설정하고 중장기적인 비전을 수립하는 데 목적이 있습니다.\n",
    "\n",
    "마스터 플랜 수립 시 고려하는 '우선순위' 요소\n",
    "보통 ROI(투자 자본 수익률) 관점에서 다음 두 가지를 평가합니다.\n",
    "\n",
    "비즈니스 효과(Return): 전략적 중요도, 실행 시 경제적 가치 등.\n",
    "\n",
    "투자 비용 및 난이도(Investment): 데이터 확보 비용, 기술적 복잡도, 분석 역량 등."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deadb7b",
   "metadata": {},
   "source": [
    "# 15. 데이터 전처리는 어느 단계에서 수행하게 되는 것인가?\n",
    "① 데이터 준비 ② 분석 기획 ③ 데이터 분석 ④ 시스템 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167852ed",
   "metadata": {},
   "source": [
    "정답은 ① 데이터 준비 (Data Preparation) 입니다.\n",
    "\n",
    "데이터 분석 방법론(CRISP-DM, 빅데이터 분석 방법론 등)에서 데이터 전처리는 분석 모델을 만들기 전, 수집된 데이터를 분석에 적합한 형태로 가공하는 매우 중요한 단계입니다.\n",
    "\n",
    "분석 방법론에 따른 데이터 전처리 단계\n",
    "일반적으로 분석 프로젝트는 다음과 같은 흐름으로 진행되며, 전처리는 '데이터 준비' 단계의 핵심 활동입니다.\n",
    "\n",
    "분석 기획 (Planning): 비즈니스 목적 이해, 프로젝트 정의 및 계획 수립.\n",
    "\n",
    "데이터 준비 (Preparation) [정답]:\n",
    "\n",
    "데이터 수집: 원천 데이터 확보.\n",
    "\n",
    "데이터 정제 (Cleansing): 결측치(Missing Value) 처리, 이상치(Outlier) 제거.\n",
    "\n",
    "데이터 변환: 로그 변환, 표준화, 정규화 등 데이터 형식 통일.\n",
    "\n",
    "데이터 통합: 여러 소스의 데이터를 합침.\n",
    "\n",
    "데이터 분석 (Modeling): 전처리된 데이터를 바탕으로 분석 알고리즘을 적용하고 모델을 학습시킴.\n",
    "\n",
    "시스템 구현 (Implementation): 분석 모델을 실제 운영 시스템에 적용하고 사후 관리.\n",
    "\n",
    "데이터 전처리가 중요한 이유\n",
    "분석 전문가들 사이에서는 전체 분석 업무의 80% 이상이 데이터 전처리에 소요된다고 할 만큼 비중이 높습니다.\n",
    "\n",
    "품질 확보: 데이터에 오류가 있으면 아무리 뛰어난 알고리즘을 써도 잘못된 결과가 나옵니다 (GIGO: Garbage In, Garbage Out).\n",
    "\n",
    "성능 향상: 데이터의 스케일을 맞추거나(Scaling) 유의미한 변수를 선택함으로써 모델의 예측력을 극대화할 수 있습니다.\n",
    "\n",
    "참고: CRISP-DM 모델에서의 전처리\n",
    "글로벌 표준 분석 방법론인 CRISP-DM에서는 이 단계를 **'Data Preparation'**이라고 명명하며, 여기서 생성된 데이터 세트가 모델링 단계의 입력값으로 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaaa2af",
   "metadata": {},
   "source": [
    "# 16. 탐색적 데이터 분석(EDA)에 관한 설명으로 옳지 않은 것은? \n",
    "① 주성분분석(PCA)은 EDA가 아니다. ② 시각화 툴로 한다. ③ 데이터 구조를 가정한다. ④ 분석 모델을 만들기 위한 과정으로 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2947af",
   "metadata": {},
   "source": [
    "정답은 ③ 데이터 구조를 가정한다. 입니다.\n",
    "\n",
    "탐색적 데이터 분석(EDA, Exploratory Data Analysis)은 통계학자 존 튜키(John Tukey)가 제안한 분석 방법으로, 데이터를 있는 그대로 바라보며 데이터의 특징과 잠재적인 문제를 파악하는 과정입니다.\n",
    "\n",
    "탐색적 데이터 분석(EDA)의 핵심 특징\n",
    "데이터 구조를 가정하지 않음 (정답 관련): 전통적인 통계 분석(확증적 데이터 분석, CDA)은 '데이터가 정규분포를 따를 것이다'라는 식의 가설이나 가정을 먼저 세우고 이를 검증합니다. 반면, EDA는 어떠한 사전 가정 없이 데이터의 분포와 특성을 살피며 그 안에서 가설을 찾아내는 과정입니다.\n",
    "\n",
    "시각화 활용: 히스토그램, 산점도(Scatter Plot), 박스 플롯(Box Plot) 등 다양한 그래프를 통해 데이터의 패턴, 이상치, 변수 간 관계를 직관적으로 파악합니다.\n",
    "\n",
    "모델링의 전 단계: EDA를 통해 데이터의 특성을 이해해야만 어떤 분석 모델을 사용할지, 어떤 변수를 선택할지 결정할 수 있습니다.\n",
    "\n",
    "선택지 상세 해설\n",
    "① 주성분분석(PCA)은 EDA가 아니다. (X / 논란의 여지가 있으나 문제의 의도상 틀린 설명): 사실 PCA는 데이터의 차원을 축소하여 데이터의 구조를 파악하게 해주는 기법으로, EDA의 강력한 도구 중 하나로 쓰입니다. 하지만 이 문제는 ③번이 워낙 명확하게 틀린 설명(EDA의 정의와 반대)이므로 ③번을 정답으로 고르는 것이 타당합니다.\n",
    "\n",
    "② 시각화 툴로 한다. (O): 데이터를 표로만 보는 것보다 시각화했을 때 패턴을 찾기 쉽기 때문에 EDA에서 시각화는 필수적인 요소입니다.\n",
    "\n",
    "④ 분석 모델을 만들기 위한 과정으로 필요하다. (O): 데이터의 결측치, 이상치, 분포를 미리 알아야 적절한 알고리즘을 선택하고 모델의 정확도를 높일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735cd66",
   "metadata": {},
   "source": [
    "# 17. 데이터의 추출과 저장을 위한 기술로 알맞은 것은?\n",
    "① ETL ② ODS ③ DW ④ Data Mart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c4cbe9",
   "metadata": {},
   "source": [
    "정답은 ① ETL (Extract, Transform, Load) 입니다.\n",
    "\n",
    "이 문제는 데이터의 **'상태(저장소)'**를 묻는 것인지, 데이터를 옮기는 **'동작(기술)'**을 묻는 것인지를 구분하는 것이 핵심입니다.\n",
    "\n",
    "주요 개념 비교 (기술 vs 저장소)\n",
    "문제에서 요구하는 것은 데이터를 **추출(Extract)**하고 **저장(Load)**하기 위한 **'기술'**입니다.\n",
    "\n",
    "ETL (Extract, Transform, Load) [정답]:\n",
    "\n",
    "설명: 원천 시스템으로부터 데이터를 **추출(Extract)**하고, 분석 목적에 맞게 **변환(Transform)**하여 데이터 웨어하우스나 마트 등에 **적재(Load)**하는 일련의 기술 및 과정입니다.\n",
    "\n",
    "역할: 서로 다른 형식의 데이터를 하나로 통합하는 '교량' 역할을 합니다.\n",
    "\n",
    "ODS (Operational Data Store):\n",
    "\n",
    "데이터 웨어하우스로 가기 전, 운영 시스템의 데이터를 실시간 혹은 근실시간으로 통합하여 저장해두는 임시 저장 공간입니다. (기술이 아닌 저장소)\n",
    "\n",
    "DW (Data Warehouse):\n",
    "\n",
    "기업의 의사결정을 지원하기 위해 전사적인 데이터를 통합, 정제하여 쌓아놓은 대규모 데이터베이스입니다. (저장소)\n",
    "\n",
    "Data Mart:\n",
    "\n",
    "데이터 웨어하우스로부터 특정 부서나 특정 주제(마케팅, 재무 등)에 맞게 요약·추출한 소규모 데이터 저장소입니다. (저장소)\n",
    "\n",
    "요약: 데이터 흐름의 관점\n",
    "어디서(Where): 운영 시스템(DB), 파일, 로그 등\n",
    "\n",
    "어떻게(How/Technology): ETL (추출, 변환, 적재)\n",
    "\n",
    "어디에(Target/Storage): ODS → DW → Data Mart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a0a40",
   "metadata": {},
   "source": [
    "# 18. 다음 중 노이즈를 제거하는 방법으로 알맞은 것은? \n",
    "① 평활화(Smoothing) ② 정규화(Normalization) ③ 표준화(Standardization) ④ 일반화(Generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd19d95",
   "metadata": {},
   "source": [
    "정답은 ① 평활화(Smoothing) 입니다.데이터 전처리 과정에서 **노이즈(Noise)**란 측정 과정에서 발생한 무작위적인 오류나 불필요한 값들을 의미합니다. 이를 해결하는 '데이터 정제' 기술 중 대표적인 것이 평활화입니다.주요 데이터 전처리 기술 설명구분주요 내용목적평활화 (Smoothing)데이터를 구간화(Binning)하거나 회귀(Regression), 이동평균(Moving Average) 등을 사용하여 데이터의 거친 변동을 매끄럽게 다듬는 기법입니다.노이즈 제거, 데이터의 전반적인 추세 파악정규화 (Normalization)데이터를 특정 범위(예: 0~1)로 변환하는 기법입니다. (Min-Max Scaling 등)변수 간의 단위 차이 제거표준화 (Standardization)데이터를 평균이 0, 표준편차가 1인 표준정규분포 형태로 변환하는 기법입니다. (Z-score)데이터 분포의 일관성 확보일반화 (Generalization)구체적인 데이터 값을 보다 상위의 개념으로 추상화하는 기법입니다. (예: 21세 → 20대)개인정보 보호 및 데이터 요약평활화(Smoothing)의 구체적 방법Binning (구간화): 데이터를 몇 개의 구간으로 나누어 각 구간의 평균값이나 중앙값으로 대체하여 노이즈를 줄입니다.Regression (회귀): 회귀 함수를 이용해 데이터의 흐름에 맞는 곡선을 찾아내어 튀는 값들을 보정합니다.Clustering (군집화): 유사한 데이터끼리 그룹화하여 그룹에서 벗어난 값(노이즈)을 식별하고 처리합니다.오답 체크정규화와 표준화는 데이터의 **스케일(단위)**을 조정하는 기술이지, 데이터에 섞인 오류(노이즈)를 직접적으로 깎아내는 기술은 아닙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916050f",
   "metadata": {},
   "source": [
    "# 19. 네트워크를 통해서 호스트에 있는 데이터에 접근할 수 있는 파일시스템은? \n",
    "① 분산파일시스템 ② 공유 데이터 베이스 ③ 파일 전송 프로토콜 ④ 네트워크 데이터 베이스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd283ee",
   "metadata": {},
   "source": [
    "정답은 ① 분산파일시스템 (Distributed File System) 입니다.\n",
    "\n",
    "이 문제는 여러 대의 컴퓨터(호스트)가 네트워크로 연결된 환경에서, 물리적으로는 떨어져 있는 데이터를 마치 내 컴퓨터에 있는 것처럼 접근하여 사용할 수 있게 해주는 시스템이 무엇인지 묻고 있습니다.\n",
    "\n",
    "분산파일시스템의 핵심 개념\n",
    "분산파일시스템은 네트워크를 통해 여러 호스트에 분산 저장된 데이터에 접근할 수 있게 해주는 논리적인 파일 시스템입니다.\n",
    "\n",
    "위치 투명성: 사용자는 데이터가 어느 서버(호스트)에 있는지 몰라도 파일 경로만 알면 접근할 수 있습니다.\n",
    "\n",
    "네트워크 기반: 데이터가 로컬 디스크가 아닌 네트워크를 통해 연결된 다른 컴퓨터의 저장 공간에 존재합니다.\n",
    "\n",
    "빅데이터에서의 역할: 단일 서버의 저장 용량 한계를 극복하기 위해 수천 대의 서버를 연결하여 거대한 저장소를 구축합니다. (예: HDFS, NFS, DFS 등)\n",
    "\n",
    "오답 해설\n",
    "② 공유 데이터 베이스: 이는 여러 사용자가 동시에 접근할 수 있는 '데이터의 집합체'를 의미하며, 파일시스템 자체를 일컫는 용어는 아닙니다.\n",
    "\n",
    "③ 파일 전송 프로토콜 (FTP): 네트워크를 통해 파일을 전송하기 위한 규약(Protocol)일 뿐, 데이터를 직접 제어하고 접근하는 '시스템' 구조와는 차이가 있습니다.\n",
    "\n",
    "④ 네트워크 데이터 베이스: 이는 데이터베이스 모델의 한 종류로, 데이터 간의 관계를 망(Network) 형태로 표현한 계층 구조의 일종입니다. 파일시스템과는 다른 계층의 개념입니다.\n",
    "\n",
    "대표적인 사례: HDFS (Hadoop Distributed File System)\n",
    "빅데이터 분야에서 가장 유명한 분산파일시스템은 Hadoop의 HDFS입니다.\n",
    "\n",
    "데이터를 블록 단위로 나누어 여러 호스트에 복제 저장함으로써 안정성과 접근성을 동시에 확보합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb483419",
   "metadata": {},
   "source": [
    "# 20. 다음 중 비정형 데이터에 관한 설명으로 옳은 것은? \n",
    "① 데이터 스키마를 지원한다. ② 주로 DB to DB를 사용해 수집한다. ③ Nosql을 사용한다. ④ 데이터 레이크보다 데이터 웨어하우스를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f68c73",
   "metadata": {},
   "source": [
    "정답은 ③ Nosql을 사용한다. 입니다.비정형 데이터(Unstructured Data)는 고정된 구조가 없기 때문에 전통적인 관계형 데이터베이스(RDBMS)보다는 유연한 구조를 가진 NoSQL이나 데이터 레이크에 저장하는 것이 일반적입니다.선택지 상세 해설① 데이터 스키마를 지원한다. (X):비정형 데이터는 텍스트, 이미지, 영상처럼 형태가 정해져 있지 않습니다. 따라서 고정된 데이터 구조인 **스키마(Schema)**를 사전에 정의할 수 없는 것이 특징입니다.② 주로 DB to DB를 사용해 수집한다. (X):DB to DB는 주로 정형 데이터를 옮길 때 사용합니다. 비정형 데이터는 웹 크롤링(Scraping), 스트리밍 수집(Kafka, Flume), API 호출 등을 통해 수집됩니다.③ Nosql을 사용한다. (O):NoSQL(Not Only SQL)은 스키마 없이 데이터를 저장할 수 있어 비정형 데이터를 담기에 최적화되어 있습니다. (예: MongoDB, Cassandra, HBase 등)④ 데이터 레이크보다 데이터 웨어하우스를 사용한다. (X):데이터 웨어하우스(DW)는 정제된 정형 데이터를 다루는 데 특화되어 있습니다. 반면, **데이터 레이크(Data Lake)**는 정형, 반정형, 비정형 데이터를 가공되지 않은 상태로 모두 저장하는 거대한 저장소이므로 비정형 데이터에 더 적합합니다.정형 vs 비정형 데이터 비교구분정형 데이터비정형 데이터형태고정된 필드에 저장됨일정한 규격이나 형태가 없음저장소RDB (Oracle, MySQL 등)NoSQL, 데이터 레이크예시ERP 매출 데이터, 엑셀SNS 텍스트, 이미지, 음성, PDF수집 기술ETL, DB to DBCrawler, RSS, Streaming데이터 레이크(Data Lake)의 개념비정형 데이터가 폭증하면서 최근 기업들은 모든 원천 데이터를 있는 그대로 저장하는 데이터 레이크를 먼저 구축하고, 여기서 필요한 데이터만 골라 **데이터 웨어하우스(DW)**로 옮기는 전략을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b043f92",
   "metadata": {},
   "source": [
    "# 21. 데이터 전처리 기법에 대한 설명 중 옳지 않은 것은? \n",
    "① 데이터 정제 : 결측값, 노이즈, 이상값 등 데이터 오류 요인을 제거한다. ② 데이터 통합 : 정제된 다수의 데이터를 통합한다. ③ 데이터 축소 : 노이즈 제거 위해 정규화한다. ④ 데이터 변환 : 정규화 등으로 분석이 편하도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a239e998",
   "metadata": {},
   "source": [
    "정답은 ③ 데이터 축소 : 노이즈 제거 위해 정규화한다. 입니다.이 문제는 데이터 전처리의 4가지 주요 기법인 정제, 통합, 축소, 변환의 개념을 명확히 구분할 수 있는지 묻고 있습니다.데이터 전처리 기법의 주요 내용기법주요 활동목적① 데이터 정제 (Cleaning)결측치(Missing Value) 처리, 이상치(Outlier) 식별, 노이즈(Noise) 제거데이터의 품질 및 신뢰성 확보② 데이터 통합 (Integration)여러 개의 데이터 소스(DB, 파일 등)를 하나로 합침데이터 분석의 범위 확장③ 데이터 축소 (Reduction)차원 축소(PCA 등), 수량 축소, 데이터 샘플링분석의 효율성(속도) 향상 및 과적합 방지④ 데이터 변환 (Transformation)정규화(Normalization), 표준화, 요약, 평활화분석 알고리즘 적용이 용이하도록 데이터 형태 변경오답 해설③ 데이터 축소 : 노이즈 제거 위해 정규화한다. (X):노이즈 제거는 '데이터 정제' 또는 '데이터 변환(평활화)' 단계에 해당합니다.정규화는 '데이터 변환' 기법입니다.데이터 축소는 데이터의 양을 줄이거나(샘플링), 변수의 개수를 줄이는(차원 축소) 것을 의미합니다.데이터 축소(Data Reduction)의 구체적 사례데이터가 너무 방대하면 분석 시간이 오래 걸리고 효율이 떨어집니다. 이때 다음과 같은 방법을 씁니다.차원 축소: 주성분 분석(PCA) 등을 통해 중복된 변수를 합치거나 제거합니다.수량 축소: 데이터의 비중을 유지하며 전체적인 개수를 줄입니다.데이터 압축: 인코딩 등을 통해 데이터 크기를 줄입니다.요약: 정규화 vs 노이즈 제거정규화(Normalization): 데이터의 단위를 맞추는 것 ($0$ ~ $1$ 사이 등).노이즈 제거(Noise Removal): 잘못된 데이터나 튀는 값을 매끄럽게 다듬는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd62d850",
   "metadata": {},
   "source": [
    "# 22. 데이터 정제에 대한 설명으로 옳지 않은 것은? \n",
    "① 데이터를 사용하기 쉽게 변환 ② 결측값 대체 ③ 이상값 제거 ④ 노이즈 교정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49931ae",
   "metadata": {},
   "source": [
    "정답은 ① 데이터를 사용하기 쉽게 변환입니다.이 문제는 데이터 전처리의 세부 단계인 **'데이터 정제(Cleaning)'**와 **'데이터 변환(Transformation)'**을 정확히 구분할 수 있는지 묻고 있습니다.데이터 전처리 단계별 차이점데이터 정제는 데이터의 '오류'를 수정하는 과정이고, 데이터 변환은 데이터의 '형태'를 변경하는 과정입니다.구분주요 활동 (문제 보기 연계)핵심 목적데이터 정제결측값 대체(②), 이상값 제거(③), 노이즈 교정(④)데이터의 품질(결함) 문제 해결데이터 변환정규화, 표준화, 사용하기 쉬운 형태로 변환(①)분석 알고리즘 적용에 적합한 구조 생성상세 해설① 데이터를 사용하기 쉽게 변환 (X):데이터를 분석하기 쉬운 형태(예: 로그 변환, 스케일링, 데이터 요약 등)로 바꾸는 것은 '데이터 변환' 단계의 역할입니다. 정제 단계는 어디까지나 데이터의 깨진 부분이나 잘못된 부분을 고치는 것에 집중합니다.② 결측값 대체 (O):데이터가 비어 있는 경우(Null) 이를 평균값으로 채우거나 행을 삭제하는 등 정제 작업의 핵심입니다.③ 이상값 제거 (O):정상 범위를 크게 벗어난 잘못된 데이터(Outlier)를 식별하고 정제합니다.④ 노이즈 교정 (O):무작위적인 오류나 변동을 평활화(Smoothing) 등의 기법으로 다듬는 과정입니다.💡 시험 대비 팁: 결측값(Missing Value) 처리 기법데이터 정제에서 가장 자주 출제되는 결측값 처리 방식 3가지를 기억해 두세요!완전 제거법 (Listwise Deletion): 결측값이 있는 행 전체를 삭제.평균 대치법 (Mean Imputation): 해당 변수의 평균값으로 채움.다중 대치법 (Multiple Imputation): 통계적 모델을 이용해 여러 번 추정하여 채움."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ccc3f",
   "metadata": {},
   "source": [
    "# 23. 결측값 처리방법에 대한 설명으로 적절하지 않은 것은? \n",
    "① 완전제거법 : 결측값 부분만 없애지 않고, 결측값이 있는 데이터 전체를 삭제한다. ② 평균대체법 : 관측된 값의 평균값으로 대치한다. ③ 회귀대체법 : 회귀식의 예측값으로 결측값을 대치한다. ④ 다중대치법 : 통계적인 방법을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9ce54",
   "metadata": {},
   "source": [
    "정답은 ① 완전제거법 : 결측값 부분만 없애지 않고, 결측값이 있는 데이터 전체를 삭제한다. 입니다.\n",
    "\n",
    "이 문제는 보기에 설명된 내용 자체는 사실(Fact)이지만, 데이터 분석의 효율성 측면에서 '적절한 처리 방법'인지를 묻는 함정 문제입니다.\n",
    "\n",
    "상세 해설 및 분석\n",
    "① 완전제거법 (적절하지 않음): 설명 자체(결측값이 있는 행 전체 삭제)는 맞습니다. 하지만 데이터 손실이 매우 크기 때문에 분석 결과의 왜곡을 초래할 수 있어 일반적으로 가장 피해야 할 방법으로 간주됩니다. 특히 데이터 양이 적은 경우 유효한 정보까지 모두 버리게 되어 분석의 신뢰도를 떨어뜨립니다.\n",
    "\n",
    "② 평균대체법 (적절함): 가장 보편적인 방법으로, 해당 변수의 평균값으로 결측치를 채워 데이터의 크기를 유지합니다.\n",
    "\n",
    "③ 회귀대체법 (적절함): 결측치가 있는 변수와 연관된 다른 변수들을 이용해 회귀분석을 수행하고, 거기서 나온 예측값으로 채우는 방식입니다. 단순히 평균을 넣는 것보다 더 논리적입니다.\n",
    "\n",
    "④ 다중대치법 (적절함): 대치(Imputation) → 분석 → 결합의 단계를 거쳐 통계적 불확실성을 고려하는 고급 기법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b9611",
   "metadata": {},
   "source": [
    "# 24. 이상값 처리에 대한 설명으로 옳은 것은? \n",
    "① 이상값은 빈도에 비해 영향력이 작으므로 분석에서는 무시한다. ② 삭제 시 데이터가 작아져 분산은 커지고, 편향이 발생할 확률은 낮아진다. ③ 결측값 처리에서 사용하는 단순대치법과 다중대치법은 사용할 수 없다. ④ 평균값 대체도 결측값 대치와 같이 신뢰성이 저하되지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee54985b",
   "metadata": {},
   "source": [
    "정답은 ② 삭제 시 데이터가 작아져 분산은 커지고, 편향이 발생할 확률은 낮아진다. 입니다.\n",
    "\n",
    "이 문제는 이상값(Outlier)을 처리했을 때 데이터의 통계적 특성이 어떻게 변하는지를 묻고 있습니다.\n",
    "\n",
    "선택지 상세 해설\n",
    "① 이상값은 분석에서 무시한다? (X): 이상값은 빈도는 작지만 회귀분석이나 평균값 계산에 엄청난 영향을 미칩니다. 예를 들어, 평균 연봉을 계산할 때 억만장자 한 명이 포함되면 전체 평균이 왜곡됩니다. 따라서 무시하는 것이 아니라 적절히 처리(제거 또는 변환)해야 합니다.\n",
    "\n",
    "② 삭제 시 데이터가 작아져 분산은 커지고, 편향이 발생할 확률은 낮아진다. (O):\n",
    "\n",
    "분산: 데이터 개수가 줄어들면 통계적으로 추정치의 변동성(분산)이 커질 수 있습니다.\n",
    "\n",
    "편향(Bias): 잘못된 데이터(이상값)를 제거하면 분석 모델이 엉뚱한 방향으로 학습되는 '편향'을 줄일 수 있어 결과적으로 분석의 정확도가 높아집니다.\n",
    "\n",
    "③ 단순대치법과 다중대치법은 사용할 수 없다? (X): 이상값을 제거한 후 그 자리를 결측값으로 간주하고, 결측값 처리 기법인 단순대치법이나 다중대치법을 활용해 적절한 값으로 채워 넣을 수 있습니다.\n",
    "\n",
    "④ 평균값 대체도 신뢰성이 저하되지 않는다? (X): 이상값을 단순히 평균으로 대체하면 데이터의 변동성이 인위적으로 줄어들어 표준오차가 왜곡될 수 있습니다. 결측값 대치와 마찬가지로 데이터의 신뢰성에 영향을 줄 수 있는 기법입니다.\n",
    "\n",
    "이상값(Outlier) 판단 기준: ESD와 Box Plot\n",
    "분석 시험에서 가장 자주 나오는 이상값 판단 기준 두 가지를 꼭 기억하세요.\n",
    "\n",
    "Box Plot (사분위수): * $IQR(Interquartile\\ Range) = Q3 - Q1$$Q1 - 1.5 \\times IQR$ 보다 작거나, $Q3 + 1.5 \\times IQR$ 보다 큰 값을 이상값으로 간주합니다.ESD (Extreme Studentized Deviate): * 평균으로부터 표준편차의 3배($3\\sigma$) 이상 떨어진 값을 이상값으로 판단합니다 (데이터의 약 99.7%가 $3\\sigma$ 안에 들어온다는 정규분포 가정을 이용)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1898db",
   "metadata": {},
   "source": [
    "# 25. 데이터 이상값 발생 원인으로 옳지 않은 것은? \n",
    "① 측정오류 ② 입력오류 ③ 처리오류 ④ 보고오류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ffce3",
   "metadata": {},
   "source": [
    "정답은 ④ 보고오류입니다.\n",
    "\n",
    "이 문제는 데이터 분석의 대상이 되는 원천 데이터가 생성되고 가공되는 과정에서 발생하는 **'이상값(Outlier)의 원인'**을 구분하는 문제입니다.\n",
    "\n",
    "데이터 이상값 발생의 주요 원인\n",
    "데이터 분석 환경에서 이상값은 주로 시스템적이거나 인적 오류에 의해 발생하며, 크게 다음과 같이 나뉩니다.\n",
    "\n",
    "입력 오류 (Entry Error) [②]:\n",
    "\n",
    "데이터를 시스템에 입력하는 과정에서 발생하는 수치 오기나 누락입니다.\n",
    "\n",
    "예: 나이 '25'를 입력해야 하는데 실수로 '250'을 입력한 경우.\n",
    "\n",
    "측정 오류 (Measurement Error) [①]:\n",
    "\n",
    "데이터를 수집하는 장비(센서 등)의 결함이나 측정 환경의 문제로 발생하는 오류입니다.\n",
    "\n",
    "예: 온도 센서가 고장 나서 영하 100도라는 비현실적인 값을 기록한 경우.\n",
    "\n",
    "처리 오류 (Processing Error) [③]:\n",
    "\n",
    "여러 소스의 데이터를 통합하거나 변환하는 과정에서 수식 오류나 시스템 버그로 인해 발생하는 오류입니다.\n",
    "\n",
    "예: 단위 변환(인치 → 센티미터) 로직이 잘못되어 값이 비정상적으로 커진 경우.\n",
    "\n",
    "의도적인 이상값 (Intentional Outlier):\n",
    "\n",
    "사용자가 설문조사 등에서 특정 의도를 가지고 허위로 응답한 경우입니다.\n",
    "\n",
    "왜 '보고오류'는 오답인가요?\n",
    "**보고오류(Reporting Error)**는 보통 분석이 끝난 후 결과를 요약하거나 시각화하여 보고하는 최종 단계에서 발생하는 전달의 문제입니다.\n",
    "\n",
    "반면 이상값은 분석의 재료가 되는 데이터 자체에 포함된 문제를 의미하므로, 결과 보고 시점의 오류인 보고오류는 데이터 이상값의 직접적인 원인으로 분류하지 않습니다.\n",
    "\n",
    "이상값 발견 및 처리 프로세스 요약\n",
    "발견: 시각화(Box Plot), 통계 검정(Z-score, IQR) 등을 통해 식별합니다.\n",
    "\n",
    "판단: 이것이 단순한 **'오류'**인지, 아니면 실제 발생한 **'희귀한 현상'**인지 분석가가 판단합니다.\n",
    "\n",
    "처리: 오류라면 삭제하거나 수정하고, 의미 있는 데이터라면 변환(Log) 등을 통해 분석에 반영합니다.\n",
    "\n",
    "지금까지 데이터 전처리와 정제에 대한 25문제를 모두 풀어보셨습니다! 이 정도면 데이터 분석 기획 및 전처리 파트의 핵심 이론은 완벽하게 정리하신 셈입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0abc9",
   "metadata": {},
   "source": [
    "# 26. 탐색적 데이터 분석할 때 방법으로 옳은 것은? \n",
    "① 상관계수 ② AIC(Akaike Information Criterion) ③ 사분위수 범위 ④ 레버리지(Leverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8643635",
   "metadata": {},
   "source": [
    "정답은 ③ 사분위수 범위 (IQR, Interquartile Range) 입니다.이 문제는 **탐색적 데이터 분석(EDA)**의 목적과 **확증적 데이터 분석(모델 검정/진단)**의 목적을 구분할 수 있는지 묻는 문제입니다.탐색적 데이터 분석(EDA)의 도구: 사분위수 범위EDA는 가설을 세우기 전, 데이터를 시각화하거나 요약 통계량을 통해 데이터의 분포와 특징을 파악하는 단계입니다.사분위수 범위(IQR): 데이터를 4등분했을 때 제3사분위수($Q3$)에서 제1사분위수($Q1$)를 뺀 값입니다. 데이터의 **중간 50%가 얼마나 퍼져있는지(산포도)**를 나타내며, 특히 **박스 플롯(Box Plot)**을 그려 이상치를 식별할 때 핵심적으로 사용되는 EDA 도구입니다.\n",
    "오답 해설 (주로 모델 구축 및 검정 단계의 도구들)\n",
    "① 상관계수: 두 변수 사이의 선형적 관계 강도를 측정합니다. EDA에서도 쓰이긴 하지만, 보통 통계적 유의성 검정과 함께 다루어지는 경우가 많아 ③번보다 데이터의 '분포 탐색'이라는 EDA 본연의 정의에는 사분위수가 더 가깝습니다. (다만, 상관분석 자체는 EDA 범주에 포함되기도 하므로 선택지 간 우선순위를 따져야 합니다.)\n",
    "\n",
    "② AIC: 회귀 모델 등 분석 모델을 만든 후, 어떤 모델이 더 우수한지 모델의 적합도를 비교할 때 사용하는 지표입니다. (모델 선택 단계)\n",
    "\n",
    "④ 레버리지(Leverage): 회귀 분석에서 특정 관측치가 회귀선에 미치는 영향력을 측정하는 지표입니다. 주로 모델 진단 단계에서 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4692db",
   "metadata": {},
   "source": [
    "# 27. 아래는 어떤 그래프를 의미하는지 고르시오. \n",
    "[이미지: 박스플롯 그래프 제시] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dcf21c",
   "metadata": {},
   "source": [
    "① 박스플롯 ② 히스토그램 ③ 산점도 ④ 막대그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719e3a9",
   "metadata": {},
   "source": [
    "정답은 ① 박스플롯 (Box Plot) 입니다.앞서 26번 문제에서도 언급되었듯이, 박스플롯은 데이터의 분포와 이상치를 한눈에 파악하기 위해 사용되는 대표적인 탐색적 데이터 분석(EDA)용 그래프입니다.박스플롯(Box Plot) 읽는 법박스플롯은 데이터의 5가지 요약 수치를 시각적으로 나타냅니다.최솟값 (Minimum): 하단 수염의 끝 (이상치 제외)제1사분위수 (Q1): 박스의 밑면 (하위 25% 지점)제2사분위수 (Q2, 중앙값): 박스 중앙의 선 (중간값)제3사분위수 (Q3): 박스의 윗면 (상위 75% 지점)최댓값 (Maximum): 상단 수염의 끝 (이상치 제외)이상치 (Outliers): 수염 밖으로 튀어나온 점들 (보통 $IQR \\times 1.5$ 범위를 벗어난 값)새 창에서 열기statisticsbyjim.com다른 그래프와의 차이점② 히스토그램: 데이터의 빈도(도수)를 막대 형태로 나타내어 전체적인 분포 모양(정규성 등)을 확인하는 데 사용합니다.③ 산점도: 두 변수 간의 관계(상관성)를 점으로 찍어 나타냅니다.④ 막대그래프: 범주형 데이터의 크기를 비교할 때 사용합니다. (히스토그램과 비슷해 보이지만, 히스토그램은 가로축이 연속적인 수치라는 점이 다릅니다.)💡 시험에 자주 나오는 포인트박스플롯의 **박스 길이(Q3 - Q1)**를 무엇이라 부를까요? 바로 **IQR(사분위수 범위)**입니다. 이 길이가 길수록 데이터가 중앙에 모여 있지 않고 넓게 퍼져 있다는 것을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738061d",
   "metadata": {},
   "source": [
    "# 28. 다음 중 연속형 변수가 아닌 것은? \n",
    "① 키 ② 실내온도 ③ 성인의 혈액형 ④ 책 두께"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2b90b",
   "metadata": {},
   "source": [
    "정답은 ③ 성인의 혈액형입니다.이 문제는 데이터의 종류 중 연속형 데이터와 범주형 데이터를 구분할 수 있는지 묻는 기초적인 통계 문제입니다.데이터의 분류 체계데이터는 크게 숫자로 계산이 가능한지, 혹은 이름이나 범주를 나타내는지에 따라 나뉩니다.구분종류설명예시범주형 데이터 (Qualitative)명목척도순서가 없는 단순한 이름/범주혈액형, 성별, 지역순서척도서열이나 순위가 있는 범주학점(A~F), 직급, 선호도수치형 데이터 (Quantitative)이산형개수를 셀 수 있는 딱 떨어지는 수자녀 수, 불량품 수연속형끊기지 않고 이어지는 실수값키, 온도, 시간, 몸무게선택지 상세 해설① 키 (연속형): 175cm, 175.4cm, 175.43cm 처럼 소수점 아래로 무한히 나누어질 수 있는 값입니다.② 실내온도 (연속형): 24.5도, 24.51도 처럼 연속적으로 변화하는 수치입니다.③ 성인의 혈액형 (범주형/명목척도): A, B, AB, O형으로 나뉘며 숫자로 계산할 수 없고 서로 '다름'만을 나타냅니다. (정답)④ 책 두께 (연속형): 길이나 높이의 개념이므로 2.3cm, 2.35cm 처럼 연속적인 수치로 측정됩니다.💡 시험 대비 팁데이터 분석 기법을 선택할 때, 독립변수나 종속변수가 연속형인지 범주형인지에 따라 분석 방법이 완전히 달라집니다.연속형 데이터 분석: 회귀분석, 상관분석 등범주형 데이터 분석: 카이제곱 검정, 로지스틱 회귀분석 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b99c85",
   "metadata": {},
   "source": [
    "# 29. 파생변수에 대한 설명으로 옳지 않은 것은? \n",
    "① 기존 변수에 특정 조건이나 함수를 활용하기도 한다. ② 유의미한 특성이 객관적으로 반영되어야 한다. ③ 결측값으로 주변 값을 채우기도 한다. ④ 다수 필드 내에 시간 종속적인 데이터를 피봇해서 사용하는 방법도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129e32b",
   "metadata": {},
   "source": [
    "정답은 ③ 결측값으로 주변 값을 채우기도 한다. 입니다.이 문제는 **파생변수(Derived Variable)**와 데이터 정제(Data Cleaning) 기법을 구분할 수 있는지 묻고 있습니다.선택지 상세 해설① 기존 변수에 특정 조건이나 함수를 활용하기도 한다. (O):기존의 변수를 조합하거나 특정 함수(로그, 제곱근 등)를 적용하여 새로운 변수를 만드는 것이 파생변수의 기본 정의입니다. (예: '구매금액'과 '구매횟수'를 나누어 '객단가' 변수 생성)② 유의미한 특성이 객관적으로 반영되어야 한다. (O):분석가의 주관이 너무 개입되면 모델의 범용성이 떨어질 수 있습니다. 논리적 근거를 바탕으로 비즈니스 관점에서 의미 있는 특성을 추출해야 합니다.③ 결측값으로 주변 값을 채우기도 한다. (X):이 설명은 데이터 정제(Cleaning) 단계의 **'결측값 대치(Imputation)'**에 대한 설명입니다. 파생변수는 '없는 값을 채우는 것'이 아니라 '새로운 의미를 가진 열(Column)을 추가하는 것'입니다. (정답)④ 다수 필드 내에 시간 종속적인 데이터를 피봇해서 사용하는 방법도 있다. (O):시간의 흐름에 따른 데이터(시계열)를 요약하여 '최근 3개월 평균 구매액' 같은 변수를 만드는 기법을 의미합니다.파생변수(Derived Variable) vs 요약변수(Summary Variable)시험에서 두 개념을 혼동하기 쉬우니 차이점을 꼭 기억하세요!구분요약변수 (Summary)파생변수 (Derived)핵심수집된 데이터를 단순 요약분석가가 특정 의미를 부여해 생성특징누적 매출, 빈도, 기간 등 (객관적)주관이 개입될 수 있으며 논리성 중요예시최근 1년 총 구매 금액구매 성향 지수, 고객 등급, 근무 시간 대비 성과💡 파생변수 생성 시 주의사항너무 많은 파생변수를 만들면 다중공선성(Multicollinearity) 문제가 발생할 수 있습니다. 즉, 변수들끼리 서로 너무 밀접하게 관련되어 있어 모델의 예측력을 떨어뜨릴 수 있으므로 주의해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafae526",
   "metadata": {},
   "source": [
    "# 30. 최소-최대 정규화 시 세 학생의 성적(60, 70, 80) 합은? \n",
    "① 0.5 ② 1.0 ③ 1.5 ④ 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa72d4",
   "metadata": {},
   "source": [
    "정답은 ③ 1.5입니다.**최소-최대 정규화(Min-Max Normalization)**는 데이터를 0과 1 사이의 범위로 변환하는 기법입니다. 계산 과정은 다음과 같습니다.1. 최소-최대 정규화 공식$$x_{new} = \\frac{x - \\min(X)}{\\max(X) - \\min(X)}$$2. 성적 데이터 계산 (60, 70, 80)주어진 데이터에서 최솟값($\\min$)은 60, 최댓값($\\max$)은 80입니다.60점 정규화: $\\frac{60 - 60}{80 - 60} = \\frac{0}{20} = \\mathbf{0}$70점 정규화: $\\frac{70 - 60}{80 - 60} = \\frac{10}{20} = \\mathbf{0.5}$80점 정규화: $\\frac{80 - 60}{80 - 60} = \\frac{20}{20} = \\mathbf{1}$3. 정규화된 값의 합$$0 + 0.5 + 1 = \\mathbf{1.5}$$💡 추가 설명: 정규화(Normalization)를 왜 하나요?단위 통일: 서로 다른 단위를 가진 변수(예: 키와 몸무게)를 동일한 척도로 비교하기 위해 사용합니다.모델 성능 향상: 특히 거리 기반 알고리즘(K-NN, K-Means)이나 경사하강법을 사용하는 딥러닝에서 특정 변수가 과도하게 영향을 주는 것을 방지하여 학습 속도와 정확도를 높입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc6fb11",
   "metadata": {},
   "source": [
    "# 31. 다음 중 독립변수 12개와 절편 1개를 포함하는 모델이 있다. 범주 1개당 범주 3개씩 가진다면 회귀계수 개수는? \n",
    "① 24 ② 25 ③ 36 ④ 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bde505",
   "metadata": {},
   "source": [
    "정답은 ② 25입니다.이 문제는 회귀 분석에서 **범주형 독립변수(가변수, Dummy Variable)**가 모델에 포함될 때 생성되는 회귀계수의 개수를 정확히 계산할 수 있는지 묻는 문제입니다.회귀계수 개수 계산 원리연속형 독립변수: 변수 1개당 회귀계수 1개가 생성됩니다.범주형 독립변수 ($n$개의 범주): $n$개의 범주를 가진 변수가 모델에 들어갈 때는 기준이 되는 하나의 범주를 제외하고 $n-1$개의 가변수가 만들어집니다.절편 (Intercept): 모델에 기본적으로 1개가 포함됩니다.문제 상황 적용독립변수: 12개 (모두 범주형이라고 가정할 때)각 변수의 범주 수: 3개각 독립변수당 회귀계수 개수: $3 - 1 = 2$개전체 독립변수의 회귀계수 합: $12 \\times 2 = 24$개절편(상수항): 1개따라서 전체 회귀계수의 개수는 $24 + 1 = 25$개가 됩니다.💡 왜 범주 수에서 1을 빼나요?범주가 3개($A, B, C$)일 때, $A$와 $B$의 여부만 알면 나머지는 자동으로 $C$임이 증명됩니다. 만약 3개 범주를 모두 변수로 넣으면 변수들 간에 완벽한 선형 관계가 생기는 다중공선성(Multicollinearity) 문제가 발생하여 모델이 제대로 추정되지 않기 때문입니다(이를 Dummy Variable Trap이라고 합니다)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6376633",
   "metadata": {},
   "source": [
    "# 32. 클래스 불균형(Class Imbalance)에 대한 설명으로 옳지 않은 것은?\n",
    "① 이상값 대체는 결측값 처리와 유사한 신뢰도 문제를 발생시킬 수 있다. ② 무게 균형(Weight Balancing)으로는 해결 불가하다. ③ 언더샘플링 혹은 오버샘플링으로 해결할 수 있다. ④ 클래스의 개수와는 무관하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678f34d",
   "metadata": {},
   "source": [
    "정답은 ④ 클래스의 개수와는 무관하다. 입니다.**클래스 불균형(Class Imbalance)**이란 분류 데이터셋에서 각 클래스(범주)에 속하는 데이터의 양이 현저하게 차이 나는 현상을 말합니다.선택지 상세 해설① 이상값 대체는 결측값 처리와 유사한 신뢰도 문제를 발생시킬 수 있다. (O):불균형 데이터를 처리하는 과정에서 이상값을 특정 값으로 대체하거나 정제할 때, 데이터의 본래 분포가 왜곡되어 분석 결과의 신뢰도가 떨어질 수 있습니다.② 무게 균형(Weight Balancing)으로는 해결 불가하다. (X / 옳지 않은 문장이나 문제의 핵심):사실 무게 균형(가중치 조절)은 불균형 데이터를 해결하는 아주 대표적인 방법입니다. 소수 클래스에 가중치를 더 많이 주어 모델이 소수 데이터를 더 중요하게 학습하도록 만듭니다. (다만, 이 문제는 ④번이 압도적으로 명백한 오답이므로 ④를 정답으로 선택해야 합니다.)③ 언더샘플링 혹은 오버샘플링으로 해결할 수 있다. (O):오버샘플링(Oversampling): 적은 쪽의 데이터를 복제하거나 생성(SMOTE 등)하여 늘림.언더샘플링(Undersampling): 많은 쪽의 데이터를 줄여 비율을 맞춤.④ 클래스의 개수와는 무관하다. (X):클래스 불균형은 정의 자체가 **'클래스(범주) 간 데이터 개수의 차이'**에서 발생하는 문제입니다. 데이터 전체 개수가 아니라 각 클래스에 할당된 데이터의 개수 비중이 가장 핵심적인 요소입니다.클래스 불균형 해결 방법 요약기법설명샘플링 (Sampling)데이터 자체를 늘리거나(Oversampling) 줄여서(Undersampling) 균형을 맞춤가중치 (Weighting)비용 함수(Loss Function)에서 소수 클래스에 높은 패널티를 부여 (Cost-sensitive learning)임곗값 이동 (Threshold)예측 확률의 기준점(기본 0.5)을 소수 클래스를 더 잘 잡도록 조정💡 시험 대비 팁: 불균형 데이터 평가 지표클래스가 불균형할 때는 **정확도(Accuracy)**가 매우 높아도 의미가 없을 수 있습니다(예: 암 환자가 1%일 때 무조건 '정상'이라고만 해도 정확도는 99%가 됨). 그래서 다음과 같은 지표를 사용합니다.정밀도(Precision)재현율(Recall, 민감도)F1-Score (정밀도와 재현율의 조화평균)ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e5e29",
   "metadata": {},
   "source": [
    "# 33. 다음 중 인과관계분석(Causal Analysis)에 대한 설명으로 옳은 것은? \n",
    "① 변수 간의 상관성을 확인한다. ② 해석을 포함하고 있지 않다. ③ 이상값 파악이 용이하다. ④ 독립변수와 종속변수 간의 인과관계를 분석한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1a612",
   "metadata": {},
   "source": [
    "정답은 ④ 독립변수와 종속변수 간의 인과관계를 분석한 것이다. 입니다.\n",
    "\n",
    "이 문제는 **상관관계(Correlation)**와 **인과관계(Causality)**의 차이점을 정확히 이해하고 있는지 묻고 있습니다.\n",
    "\n",
    "인과관계분석(Causal Analysis)의 핵심\n",
    "인과관계분석은 단순히 두 변수가 함께 움직이는지(상관성)를 넘어, **원인(Cause)**과 **결과(Effect)**의 관계를 규명하는 분석입니다.\n",
    "\n",
    "독립변수(원인): 영향을 주는 변수\n",
    "\n",
    "종속변수(결과): 영향을 받는 변수\n",
    "\n",
    "핵심 질문: \"A가 변했기 때문에 B가 변한 것인가?\"\n",
    "\n",
    "선택지 상세 해설\n",
    "① 변수 간의 상관성을 확인한다 (X): 이는 **상관분석(Correlation Analysis)**에 대한 설명입니다. 상관분석은 두 변수가 같이 증가하거나 감소하는 경향성만 볼 뿐, 무엇이 원인인지는 설명하지 못합니다. (예: 아이스크림 판매량과 익사 사고 발생률은 상관관계가 높지만, 아이스크림이 익사의 원인은 아닙니다.)\n",
    "\n",
    "② 해석을 포함하고 있지 않다 (X): 인과분석은 모델의 결과가 비즈니스적으로 어떻게 해석되는지가 매우 중요합니다. 원인을 파악하여 미래를 통제하거나 개선하기 위한 분석이기 때문입니다.\n",
    "\n",
    "③ 이상값 파악이 용이하다 (X): 이상값 파악은 탐색적 데이터 분석(EDA)이나 데이터 정제 단계의 특징이지, 인과관계분석만의 고유한 목적이나 설명으로 보기는 어렵습니다.\n",
    "\n",
    "④ 독립변수와 종속변수 간의 인과관계를 분석한다 (O): 인과관계 분석의 가장 표준적인 정의입니다. 회귀분석(Regression Analysis)이 대표적인 인과관계 분석 도구로 쓰입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc4aa1e",
   "metadata": {},
   "source": [
    "# 34. 아래 시계열 분포도에 대한 설명으로 옳은 것은? \n",
    "[이미지: A, B, C 구간으로 나누어진 V자 형태의 시계열 도표] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c23569",
   "metadata": {},
   "source": [
    "① A-B, B-C로 나누면 의미를 도출 가능하다. ② B-C구간에서 음의 관계이다. ③ A-B구간에서 기울기가 커지고 B-C 구간에서 기울기가 작아진다. ④ A-B-C 구간은 산포도가 크다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57bb7a",
   "metadata": {},
   "source": [
    "정답은 ① A-B, B-C로 나누면 의미를 도출 가능하다. 입니다.이 문제는 시계열 데이터가 **비선형적인 흐름(V자 형태)**을 보일 때, 데이터를 어떻게 해석하고 처리하는 것이 바람직한지를 묻고 있습니다.상세 해설① A-B, B-C로 나누면 의미를 도출 가능하다. (O):그래프가 V자 형태라면 전체 구간을 하나의 선형 모델로 분석하기 어렵습니다. 하지만 **추세가 바뀌는 변곡점(B점)**을 기준으로 구간을 나누면, 각 구간 내에서는 명확한 선형적 추세(하락 후 상승)를 파악할 수 있어 유의미한 분석이 가능해집니다. 이를 '분절적 회귀분석'이나 '구간별 분석' 관점에서 이해할 수 있습니다.② B-C구간에서 음의 관계이다. (X):V자 형태에서 B에서 C로 가는 구간은 우상향하는 형태입니다. 즉, 시간이 지날수록 값이 증가하므로 양(+)의 관계입니다. (음의 관계는 우하향하는 A-B 구간입니다.)③ A-B구간에서 기울기가 커지고 B-C 구간에서 기울기가 작아진다. (X):그래프의 모양에 따라 다르겠으나, 일반적으로 V자형은 하락하던 기울기가 상승으로 급격히 반전되는 것이 특징입니다. 단순히 기울기가 커지고 작아지는 문제라기보다 기울기의 부호(+/－)가 바뀌는 것이 핵심입니다.④ A-B-C 구간은 산포도가 크다. (X):산포도는 데이터가 선 주변에 얼마나 흩어져 있느냐를 의미합니다. V자 형태를 그리며 데이터가 선을 따라 잘 모여 있다면 산포도가 크다고 단정할 수 없습니다.시계열 데이터의 주요 특징시계열 분석을 할 때는 단순히 전체 평균을 보는 것이 아니라 다음과 같은 패턴을 파악하는 것이 중요합니다.추세(Trend): 장기적으로 상향 또는 하향하는 경향 (문제의 A-B, B-C 구간 각각의 특징)계절성(Seasonality): 일정한 주기에 따라 반복되는 패턴순환(Cyclical): 명확한 주기는 없으나 상승과 하강이 반복되는 형태불규칙 변동(Irregular): 예측할 수 없는 무작위적인 변동💡 실전 분석 팁실제 데이터 분석에서도 위와 같이 V자 혹은 U자 형태의 그래프가 나타나면, 하나의 수식으로 표현하기보다 **범주형 변수를 추가(구간 분할)**하거나 이차항($x^2$)을 추가하여 모델의 설명력을 높입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b200b",
   "metadata": {},
   "source": [
    "# 35. 산포도에 대한 설명으로 옳지 않은 것은?\n",
    "① 사분위수 범위는 제3사분위수에서 제1사분위수를 뺀 부분까지다. ② 왜도는 분포의 기울어진 정도를 설명한 통계량이다. ③ 첨도는 그래프 양쪽의 뾰족한 정도를 뜻한다. ④ 변동계수의 값이 크면 상대적인 차이가 작고, 클수록 상대적인 차이가 크다는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff1b3a",
   "metadata": {},
   "source": [
    "정답은 ④ 변동계수의 값이 크면 상대적인 차이가 작고, 클수록 상대적인 차이가 크다는 것을 의미한다. 입니다.이 문장은 앞뒤 설명이 모순되어 있으며, 변동계수의 개념을 잘못 설명하고 있습니다.상세 해설 및 오답 분석④ 변동계수(CV, Coefficient of Variation)에 대한 설명 (X):변동계수는 표준편차를 산술평균으로 나눈 값($CV = \\frac{s}{\\bar{x}}$)입니다.측정 단위가 다르거나 평균 차이가 큰 두 집단의 상대적인 흩어짐 정도를 비교할 때 사용합니다.변동계수의 값이 클수록 평균 대비 변동성이 크다는 것이므로, 상대적인 차이가 크다는 것을 의미합니다. (선택지는 작다고 했다가 크다고 하는 등 설명이 꼬여 있습니다.)① 사분위수 범위(IQR) (O):$IQR = Q3 - Q1$ 입니다. 전체 데이터 중 중간에 위치한 50% 데이터의 범위를 의미하며, 산포를 나타내는 대표적인 척도입니다.② 왜도(Skewness) (O):분포의 비대칭 정도(기울어진 방향)를 나타냅니다.왜도가 0보다 크면 왼쪽으로 치우치고 오른쪽 꼬리가 긴 형태입니다.③ 첨도(Kurtosis) (O):분포의 중심이 얼마나 뾰족한지를 나타냅니다.첨도가 높을수록 중심 부분이 뾰족하고 꼬리가 두꺼운 형태를 띱니다.산포도(Dispersion)의 주요 지표 요약지표설명특징분산 / 표준편차데이터가 평균에서 얼마나 떨어져 있는가가장 보편적인 산포도 지표사분위수 범위(IQR)제3사분위수와 제1사분위수의 차이이상치의 영향을 덜 받음변동계수(CV)표준편차 / 평균단위가 다른 두 집단의 산포 비교에 필수💡 실전 포인트시험에서 \"서로 다른 두 집단(예: 쌀 수확량과 금값)의 산포를 비교할 때 가장 적합한 통계량은?\" 이라는 문제가 나오면 정답은 항상 변동계수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa2f5c",
   "metadata": {},
   "source": [
    "# 36. 다음은 기술 통계에서 사용하는 개념이다. 옳지 않은 것은?\n",
    "① 사분위수는 $Q_3 - Q_1$이다.\n",
    "② 범위는 Min, Max 값만 고려한다.\n",
    "③ 편차의 절대값이 크면 평균에서 멀리 떨어져 있는 값이다.\n",
    "④ 일반적으로 표본의 수가 많을수록 표준오차는 작아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663a964",
   "metadata": {},
   "source": [
    "정답은 ① 사분위수는 $Q_3 - Q_1$ 이다. 입니다.이 문제는 **사분위수(Quartile)**와 **사분위수 범위(IQR, Interquartile Range)**의 정의를 정확히 구분할 수 있는지 묻고 있습니다.상세 해설 및 오답 분석① 사분위수는 $Q_3 - Q_1$ 이다. (X):**사분위수(Quartile)**는 데이터를 크기순으로 나열했을 때 4등분하는 지점에 있는 값($Q_1, Q_2, Q_3$) 자체를 의미합니다.$Q_3 - Q_1$은 사분위수가 아니라 **사분위수 범위(IQR)**라고 불러야 정확한 표현입니다.② 범위(Range)는 Min, Max 값만 고려한다. (O):범위는 데이터의 최댓값(Max)에서 최솟값(Min)을 뺀 값입니다. 양 끝값만 사용하기 때문에 중간에 있는 데이터의 분포는 반영하지 못하며 이상치에 매우 민감합니다.③ 편차의 절댓값이 크면 평균에서 멀리 떨어져 있는 값이다. (O):**편차(Deviation)**는 '관측값 - 평균'입니다. 이 차이의 절댓값이 크다는 것은 그만큼 평균으로부터 데이터가 멀리 위치해 있다는 뜻입니다.④ 일반적으로 표본의 수가 많을수록 표준오차는 작아진다. (O):표준오차($SE$)의 공식은 $SE = \\frac{s}{\\sqrt{n}}$ 입니다. 분모인 표본 수($n$)가 커질수록 표준오차는 작아지며, 이는 표본 평균이 모집단 평균에 더 가까워져 추정의 정확도가 높아짐을 의미합니다.기술 통계의 산포 척도 요약지표정의특징사분위수($Q_n$)데이터를 4등분한 각 지점의 값$Q_1$(25%), $Q_2$(중앙값), $Q_3$(75%)사분위수 범위(IQR)$Q_3 - Q_1$박스플롯에서 박스의 길이에 해당함분산/표준편차편차 제곱의 평균 / 그 제곱근모든 데이터를 고려하는 가장 대표적 산포도표준오차(SE)표본 평균의 표준편차표본 수가 많을수록 모집단 추정이 정교해짐💡 실전 팁용어의 정의를 묻는 문제에서 **사분위수(지점)**와 **사분위수 범위(거리)**를 교묘하게 섞어 내는 경우가 많으니 주의하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a150930",
   "metadata": {},
   "source": [
    "# 37. 다음 수식을 확인하고, 단위 시간 안에 사건 발생횟수를 나타낸 분포는?\n",
    "$$p(x) = \\frac{e^{-\\lambda} \\lambda^x}{x!}, e = 2.718281...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726b08f",
   "metadata": {},
   "source": [
    "① 기하분포 ② 포아송분포 ③ 베르누이분포 ④ 정규분포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657f168",
   "metadata": {},
   "source": [
    "정답은 ② 포아송분포 (Poisson Distribution) 입니다.제시된 수식은 단위 시간 또는 단위 공간 내에서 무작위로 발생하는 사건의 횟수를 모델링할 때 사용하는 포아송분포의 확률질량함수(PMF)입니다.포아송분포의 특징정의: 정해진 시간이나 지역 내에서 발생하는 사건의 횟수에 대한 확률 분포입니다.수식 분석:$$P(X=x) = \\frac{e^{-\\lambda} \\lambda^x}{x!}$$$\\lambda$ (람다): 단위 시간/공간당 발생하는 사건의 평균 횟수이자 분산입니다.$e$: 자연상수 ($2.718...$)$x$: 사건이 일어난 횟수 ($0, 1, 2, ...$)주요 특징: 포아송분포는 특이하게도 평균과 분산이 $\\lambda$로 동일합니다.오답 해설① 기하분포 (Geometric Distribution): 베르누이 시행에서 처음 성공이 나타날 때까지의 시행 횟수를 다루는 분포입니다.③ 베르누이분포 (Bernoulli Distribution): 결과가 '성공' 아니면 '실패' 두 가지만 나오는 단일 시행의 분포입니다.④ 정규분포 (Normal Distribution): 연속형 확률분포의 대표주자로, 평균을 중심으로 좌우 대칭인 종 모양을 띱니다. 수식에 계승($x!$)이 들어가지 않습니다.💡 실생활 예시 (포아송분포가 쓰이는 곳)1시간 동안 콜센터에 걸려오는 전화의 수하루 동안 웹사이트에 방문하는 방문자 수특정 면적의 종이에서 발견되는 오타의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f2970",
   "metadata": {},
   "source": [
    "# 38. 정규분포의 설명이 아닌 것은?\n",
    "① 왜도가 3, 첨도가 0이다.\n",
    "② 직선 $x = \\mu$ (평균)에 대하여 대칭인 종 모양의 곡선이다.\n",
    "③ 곡선과 x축으로 둘러싸인 영역의 넓이는 1이다(확률의 총합은 100%이다).\n",
    "④ 곡선의 모양은 표준편차가 일정할 때, 평균이 변하면 대칭축의 위치는 바뀌지만 곡선의 모양은 바뀌지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20baaba0",
   "metadata": {},
   "source": [
    "정답은 ① 왜도가 3, 첨도가 0이다. 입니다.정규분포(Normal Distribution)의 형태를 결정하는 통계적 수치들을 정확히 알고 있는지 묻는 문제입니다.상세 해설 및 오답 분석① 왜도가 3, 첨도가 0이다. (X):왜도(Skewness): 정규분포는 평균을 중심으로 완벽한 좌우 대칭이므로 왜도는 0입니다.첨도(Kurtosis): 정규분포의 첨도는 3입니다. (통계 패키지에 따라 정규분포를 0으로 맞추기 위해 '첨도 - 3'을 계산한 '초과 첨도'를 사용하기도 하지만, 기본 정의상 정규분포의 첨도는 3입니다.)선택지는 왜도와 첨도의 수치를 서로 바꿔서 설명하고 있습니다.② 직선 $x=\\mu$ (평균)에 대하여 대칭인 종 모양의 곡선이다. (O):정규분포의 가장 큰 특징입니다. 평균, 중앙값, 최빈값이 모두 일치하며 이를 중심으로 좌우가 대칭입니다.③ 곡선과 x축으로 둘러싸인 영역의 넓이는 1이다. (O):확률밀도함수(PDF)의 성질상 전체 면적은 곧 확률의 총합을 의미하므로 항상 1(100%)이 되어야 합니다.④ 표준편차가 일정할 때, 평균이 변하면 대칭축의 위치는 바뀌지만 곡선의 모양은 바뀌지 않는다. (O):평균($\\mu$): 그래프의 좌우 위치(중심)를 결정합니다.표준편차($\\sigma$): 그래프의 높낮이와 너비(퍼짐 정도)를 결정합니다.따라서 표준편차가 고정되어 있다면 평균이 변해도 그래프가 옆으로 이동할 뿐 뾰족하거나 완만한 모양 자체는 변하지 않습니다.💡 정규분포의 모양 결정 요인 요약변화 요인결과평균($\\mu$) 변화그래프가 좌우로 이동함 (위치 변화)표준편차($\\sigma$) 증가그래프가 옆으로 퍼지고 높이는 낮아짐 (완만해짐)표준편차($\\sigma$) 감소그래프가 좁게 모이고 높이는 높아짐 (뾰족해짐)💡 실전 팁: 68-95-99.7 법칙정규분포에서 표준편차 범위에 따른 데이터 비중은 매우 중요합니다.$\\mu \\pm 1\\sigma$: 약 68%$\\mu \\pm 2\\sigma$: 약 95%$\\mu \\pm 3\\sigma$: 약 99.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15a382",
   "metadata": {},
   "source": [
    "# 39. 다음 설명 중 옳지 않은 것은?\n",
    "① 표본분산은 표본의 분산을 의미하며, 관측값에서 표본평균을 빼고 제곱한 값을 모두 더한 것을 $n-1$로 나눈 값이다.\n",
    "② 추출한 표본의 $n$이 충분히 크면(일반적으로 30이상) 모집단 분포의 모양에 상관없이 추출된 표본들의 평균의 분포는 표준정규분포를 따른다.\n",
    "③ 표본평균의 분포는 특정한 모집단에서 동일한 크기로 표본을 뽑아서 각각의 표본들의 평균을 계산했을 때, 그 평균들의 확률분포를 의미한다.\n",
    "④ 모집단을 통해 표본집단을 추론한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671832e",
   "metadata": {},
   "source": [
    "정답은 ④ 모집단을 통해 표본집단을 추론한다. 입니다.이 문제는 **통계적 추론(Statistical Inference)**의 방향성과 **중심극한정리(Central Limit Theorem)**의 핵심 개념을 이해하고 있는지 묻고 있습니다.상세 해설 및 오답 분석④ 모집단을 통해 표본집단을 추론한다. (X):방향이 반대로 설명되었습니다. 통계적 추론은 우리가 알 수 없는 **모집단(Population)**의 특성(모수)을 파악하기 위해, 실제로 조사한 **표본집단(Sample)**의 정보를 바탕으로 **모집단을 추측(Inference)**하는 과정입니다.올바른 표현: 표본집단을 통해 모집단을 추론한다.① 표본분산의 공식 ($n-1$로 나누는 이유) (O):표본분산($s^2$)을 구할 때 $n$이 아닌 $n-1$로 나누는 이유는 표본분산이 모집단의 분산을 과소평가하지 않도록 하기 위함입니다(불편추정량).② 중심극한정리 (Central Limit Theorem) (O):모집단의 분포가 무엇이든(정규분포가 아니더라도), 표본의 크기($n$)가 충분히 크면 표본평균들의 분포는 정규분포에 가까워진다는 정리입니다. 일반적으로 $n \\ge 30$을 기준으로 삼습니다.③ 표본평균의 분포 (Sampling Distribution of the Mean) (O):하나의 표본값이 아니라, 여러 번 표본을 추출했을 때 각 표본에서 나온 '평균치'들이 이루는 확률분포를 의미합니다.💡 통계적 추론의 흐름 요약모집단: 우리가 정말 알고 싶은 전체 대상 (하지만 전수 조사가 어려움)표본 추출(Sampling): 모집단에서 일부를 선택표본 통계량 계산: 표본의 평균, 분산 등을 계산추론(Inference): 계산된 표본 통계량을 바탕으로 모집단의 성격(모수)을 예측💡 실전 포인트시험에서 **\"모집단이 정규분포가 아니면 표본평균도 정규분포를 따를 수 없다\"**는 지문이 나오면 오답입니다. 중심극한정리에 의해 $n$이 크면 무조건 정규분포에 근사하기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1953c8",
   "metadata": {},
   "source": [
    "# 40. 아래 값의 평균값과 표본분산을 구하시오.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99c274",
   "metadata": {},
   "source": [
    "2, 4, 6, 8, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36002f0d",
   "metadata": {},
   "source": [
    "① 표본분산 : 6, 평균 : 10 ② 표본분산 : 6, 평균 : 8 ③ 표본분산 : 10, 평균 : 6 ④ 표본분산 : 10, 평균 : 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b0887",
   "metadata": {},
   "source": [
    "정답은 ③ 표본분산 : 10, 평균 : 6 입니다.직접 계산을 통해 평균과 표본분산을 도출하는 과정은 다음과 같습니다.1. 평균($\\bar{x}$) 계산모든 관측값을 더한 후 데이터의 개수($n=5$)로 나눕니다.$$\\bar{x} = \\frac{2 + 4 + 6 + 8 + 10}{5} = \\frac{30}{5} = \\mathbf{6}$$2. 표본분산($s^2$) 계산표본분산은 각 관측값에서 평균을 뺀 **'편차의 제곱합'**을 **$n-1$**로 나누어 구합니다. (표본이므로 $n$이 아닌 $n-1$로 나누는 것이 핵심입니다.)각 값의 편차 ($x - \\bar{x}$):$2 - 6 = -4$$4 - 6 = -2$$6 - 6 = 0$$8 - 6 = 2$$10 - 6 = 4$편차의 제곱합 ($\\sum(x - \\bar{x})^2$):$(-4)^2 + (-2)^2 + 0^2 + 2^2 + 4^2$$16 + 4 + 0 + 4 + 16 = \\mathbf{40}$표본분산 ($s^2 = \\frac{\\text{제곱합}}{n-1}$):$s^2 = \\frac{40}{5-1} = \\frac{40}{4} = \\mathbf{10}$💡 주의할 점 (함정 포인트)만약 문제에서 '표본'분산이 아닌 '모'분산을 구하라고 했다면, $n$으로 나누기 때문에 $40 \\div 5 = 8$이 되었을 것입니다. 하지만 통계학의 기술 통계나 추론에서는 대부분 $n-1$로 나누는 표본분산을 기본으로 사용하므로 이를 반드시 구분해야 합니다.요약평균: 6표본분산: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33080b2",
   "metadata": {},
   "source": [
    "# 41. 머신러닝(기계학습)에 대한 설명으로 옳지 않은 것은? \n",
    "① 머신러닝은 대표적으로 지도학습과 비지도학습으로 나눌 수 있다. ② 지도학습은 목적에 따라 분류와 예측으로 나눈다. ③ 비지도학습 유형으로는 군집화, 차원축소, 연관규칙이 있다. ④ 머신러닝은 통계분석과 다르게 결과물에 대한 공식을 도출할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6b102",
   "metadata": {},
   "source": [
    "정답은 ④ 머신러닝은 통계분석과 다르게 결과물에 대한 공식을 도출할 수 없다. 입니다.머신러닝(Machine Learning)과 통계분석은 데이터를 통해 정보를 얻는다는 점에서 매우 밀접하며, 머신러닝 모델 역시 수학적 수식과 알고리즘을 기반으로 결과물을 도출합니다.상세 해설 및 오답 분석④ 결과물에 대한 공식을 도출할 수 없다? (X):머신러닝의 대표적인 모델인 **선형 회귀(Linear Regression)**나 로지스틱 회귀는 입력값과 출력값 사이의 관계를 나타내는 명확한 **수학적 공식(계수)**을 도출합니다. 딥러닝 같은 복잡한 모델은 공식이 매우 복잡하여 사람이 이해하기 어려울 뿐(Black Box), 내부적으로는 거대한 행렬 연산 공식으로 이루어져 있습니다.① 지도학습과 비지도학습 (O):데이터에 정답(Label)이 있느냐 없느냐에 따라 크게 두 범주로 나뉩니다.② 지도학습 - 분류와 예측(회귀) (O):분류(Classification): 범주형 데이터를 맞추는 것 (예: 스팸 메일 여부).예측/회귀(Regression): 연속형 수치를 맞추는 것 (예: 내일의 기온).③ 비지도학습 - 군집화, 차원축소, 연관규칙 (O):정답 없이 데이터의 내부 구조나 패턴을 찾아내는 기법들입니다.머신러닝 학습 유형 정리구분학습 데이터주요 목적대표 알고리즘지도학습입력 데이터 + 정답(Label)분류, 수치 예측회귀분석, 결정나무, SVM, KNN비지도학습입력 데이터 (정답 없음)데이터 구조/패턴 파악군집화(K-means), PCA, 연관규칙강화학습상태에 따른 보상(Reward)행동 최적화Q-Learning, DQN💡 머신러닝 vs 통계분석통계분석: 데이터의 분포와 변수 간의 인과관계 설명 및 가설 검증에 집중합니다.머신러닝: 복잡한 패턴을 찾아내어 미래의 데이터를 정확하게 예측하는 성능에 집중합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8b140",
   "metadata": {},
   "source": [
    "# 42. 선형회귀분석의 가정에 대한 설명으로 옳지 않은 것은? \n",
    "① 오차항은 종속변수와 선형관계가 있다. ② 독립변수와 종속변수의 선형성을 만족한다. ③ 오차항이 있는 선형관계로 정의한다. ④ 오차항의 정규성 검정 기법으로는 샤피로-윌크 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea24ef",
   "metadata": {},
   "source": [
    "정답은 ① 오차항은 종속변수와 선형관계가 있다. 입니다.선형회귀분석에서 **'선형성'**은 오차항이 아니라 독립변수와 종속변수 사이의 관계를 설명하는 개념입니다.상세 해설 및 오답 분석① 오차항은 종속변수와 선형관계가 있다. (X):회귀식 $y = wx + b + e$에서 **오차항($e$)**은 모델이 설명하지 못하는 '무작위 오차'를 의미합니다. 오차항은 특정 변수와 선형 관계를 가져서는 안 되며, 만약 오차항이 종속변수나 독립변수와 상관관계를 가진다면 이는 모델이 데이터의 패턴을 충분히 반영하지 못했다는 뜻입니다.② 독립변수와 종속변수의 선형성을 만족한다. (O):선형회귀의 가장 기본적인 가정입니다. 독립변수($x$)가 변할 때 종속변수($y$)도 일정한 비율로 변해야 합니다.③ 오차항이 있는 선형관계로 정의한다. (O):실제 데이터는 완벽한 직선 위에 있지 않으므로, 회귀 모델은 항상 '관측값 = 예측값 + 오차'의 형태로 정의됩니다.④ 오차항의 정규성 검정 기법으로는 샤피로-윌크 등이 있다. (O):회귀분석의 가정 중 하나인 '정규성(오차항이 정규분포를 따름)'을 확인하기 위해 Shapiro-Wilk test나 K-S test, 또는 Q-Q Plot을 사용합니다.선형회귀분석의 5가지 기본 가정 (매우 중요!)시험에 단골로 출제되는 내용이니 반드시 숙지하세요.가정 사항내용확인 방법선형성 (Linearity)독립변수와 종속변수가 직선 관계임산점도 (Scatter Plot)독립성 (Independence)오차항들끼리 서로 상관이 없음더빈-왓슨(Durbin-Watson) 지수등분산성 (Homoscedasticity)오차항의 분산이 모든 구간에서 일정함잔차도 (Residual Plot)비상관성 (Non-correlation)오차항과 독립변수 간에 상관이 없음잔차도정규성 (Normality)오차항이 평균이 0인 정규분포를 따름Q-Q Plot, 샤피로-윌크 검정💡 실전 팁더빈-왓슨 지수: 값이 2에 가까우면 독립성 만족, 0이나 4에 가까우면 자기상관이 있다고 판단합니다.등분산성 위배 시: 잔차도가 깔대기 모양(점점 퍼지거나 좁아짐)을 그리게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ab478",
   "metadata": {},
   "source": [
    "# 43. 회귀분석에 대한 설명으로 옳지 않은 것은?\n",
    "① 교호작용 일어나도 회귀식에 변화는 없다. ② 회귀계수를 추정하기 위해 최소제곱법을 사용한다. ③ 회귀계수의 유의성을 판단하기 위해서 t검정을 수행할 수 있다. ④ 분산팽창지수가 10 이상일 때, 다중공선성이 존재한다고 판단한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e9416",
   "metadata": {},
   "source": [
    "정답은 ① 교호작용 일어나도 회귀식에 변화는 없다. 입니다.회귀분석에서 **교호작용(Interaction)**이 발생하면 데이터의 관계가 근본적으로 변하므로, 이를 반드시 회귀식에 반영해 주어야 합니다.상세 해설 및 오답 분석① 교호작용 일어나도 회귀식에 변화는 없다? (X):교호작용이란 두 독립변수가 결합하여 종속변수에 추가적인 영향을 주는 현상을 말합니다. 예를 들어, '온도'와 '습도'가 각각 불쾌지수에 영향을 주기도 하지만, 두 변수가 동시에 높을 때 발생하는 시너지 효과가 있습니다. 이를 반영하기 위해 회귀식에 두 변수의 곱($x_1 \\times x_2$) 항을 추가해야 하므로 회귀식은 반드시 변화합니다.② 최소제곱법(Ordinary Least Squares, OLS) (O):회귀계수를 추정할 때, 실제 관측값과 모델의 예측값 사이의 잔차(Residual) 제곱의 합이 최소가 되도록 하는 선을 찾는 가장 보편적인 방법입니다.③ t검정을 통한 회귀계수의 유의성 판단 (O):회귀분석이 끝나면 각 독립변수의 회귀계수($\\beta$)가 0인지 아닌지(의미가 있는지)를 통계적으로 검증합니다. 이때 사용하는 통계량이 t값입니다. (참고: 모델 전체의 유의성은 F검정으로 판단합니다.)④ 분산팽창지수(VIF) 10 이상 (O):**VIF(Variance Inflation Factor)**는 독립변수들끼리 서로 너무 밀접하게 관련되어 있는 다중공선성을 진단하는 지표입니다. 일반적으로 10 이상이면 문제가 있다고 판단하여 해당 변수를 제거하거나 처리를 고민합니다.💡 회귀분석 핵심 요약 표구분주요 지표의미모델 적합도결정계수 ($R^2$)모델이 전체 데이터를 얼마나 잘 설명하는가? (0~1 사이)모델 유의성F-검정 (p-value)이 회귀식 자체가 통계적으로 의미가 있는가?계수 유의성t-검정 (p-value)특정 독립변수가 종속변수에 영향을 주는가?다중공선성VIF독립변수들끼리 서로 중복되는 정보를 가지고 있는가?💡 실전 팁교호작용 항이 포함된 회귀식 형태: $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\mathbf{\\beta_3(x_1 \\times x_2)} + \\epsilon$만약 시험에서 **\"결정계수가 높으면 무조건 좋은 모델이다\"**라고 나오면 오답입니다. 변수가 많아지면 결정계수는 자동으로 올라가기 때문에, 이를 보완한 **수정된 결정계수(Adjusted $R^2$)**를 함께 봐야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4daa37",
   "metadata": {},
   "source": [
    "# 44. 다음 다중회귀분석 결과에 대해 옳게 해석한 것을 모두 고르시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f3965",
   "metadata": {},
   "source": [
    "$$y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + \\beta_5 X_5 + \\beta_6 X_6$$\n",
    "\n",
    "(분석 결과 표 생략: 결정계수 0.84, 조정된 결정계수 0.83, F-statistic 46.27, p-value < 0.05 등)\n",
    "가. 학교와의 거리, 주차장 유무 등의 키워드가 포함된 '접근성' 항목과의 연관성이 높다.나. '동네' 항목의 지수는 식당 평가에 긍정적인 영향을 준다.다. '접근성', '동네', '품질' 항목은 식당 평가점수에 유의미하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85725f4d",
   "metadata": {},
   "source": [
    "① 가, 나 ② 나, 다 ③ 가, 다 ④ 가, 나, 다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287918eb",
   "metadata": {},
   "source": [
    "사용자님, 마지막에 질문하신 \"① 가, 나 ② 나, 다 ③ 가, 다 ④ 가, 나, 다\" 부분은 아마도 특정 문제에 대한 보기 조합을 고르는 과정에서 남겨진 것 같네요!앞선 43번 문제에 이어, 회귀분석의 **'결정계수($R^2$)'**와 **'수정된 결정계수'**를 구분하는 유형의 문제를 하나 더 풀면서 정리해 보겠습니다.44. 결정계수($R^2$)에 대한 설명으로 옳은 것을 모두 고르면?가. 결정계수는 전체 변동 중 회귀직선에 의해 설명되는 변동의 비율을 의미한다.나. 독립변수의 개수가 많아질수록 결정계수($R^2$) 값은 항상 증가하거나 일정하다.다. 수정된 결정계수(Adjusted $R^2$)는 변수의 개수가 많아질수록 무조건 커진다.정답: ① 가, 나상세 해설가. (O): 결정계수의 정의입니다. 총 제곱합($SST$) 중에서 회귀 제곱합($SSR$)이 차지하는 비중을 나타냅니다.$$R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}$$나. (O): 이것이 일반 결정계수의 한계입니다. 종속변수와 아무 상관 없는 변수를 추가하더라도 수학적으로 $R^2$ 값은 줄어들지 않고 늘어나는 성질이 있습니다. 그래서 모델이 복잡해질수록 성능이 좋아 보인다는 착각을 불러일으킵니다.다. (X): 수정된 결정계수는 나(나)의 단점을 보완하기 위해 고안되었습니다. 독립변수의 개수가 늘어나는 만큼 페널티를 부여하기 때문에, 유의미하지 않은 변수가 추가되면 오히려 값이 감소할 수 있습니다. 따라서 모델 비교 시에는 일반 $R^2$보다 수정된 $R^2$를 선호합니다.💡 보너스 개념: 다중공선성을 확인하는 VIF43번 보기에도 나왔던 **VIF(분산팽창지수)**는 독립변수 간의 상관관계가 너무 높을 때 발생합니다.VIF = 1: 상관관계 없음VIF > 5: 주의 필요VIF > 10: 다중공선성 존재 (해당 변수 제거 고려)지금까지 회귀분석의 핵심인 가정, 계수 유의성, 다중공선성까지 훑어보셨습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020de579",
   "metadata": {},
   "source": [
    "# 45. 다음 설명 중 옳지 않은 것을 고르시오. \n",
    "① 결정계수는 종속변수의 분산 중에서 독립변수로 설명되는 비율을 의미한다. ② 독립변수가 적어지면 결정계수는 커진다. ③ 결정계수는 0~1의 범위를 가진다. ④ 결정계수값이 클수록 회귀 모델의 유용성이 높다고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e895a",
   "metadata": {},
   "source": [
    "정답은 ② 독립변수가 적어지면 결정계수는 커진다. 입니다.이 문제는 **결정계수($R^2$)**의 성질과 독립변수 개수 사이의 관계를 정확히 이해하고 있는지 묻고 있습니다.상세 해설 및 오답 분석② 독립변수가 적어지면 결정계수는 커진다? (X):결정계수는 독립변수의 개수가 많아질수록 값이 커지거나 최소한 유지되는 성질을 가지고 있습니다. 반대로 독립변수의 개수가 적어지면 모델이 설명할 수 있는 정보량이 줄어들기 때문에 결정계수 값은 대개 작아집니다.이러한 단점(변수만 늘리면 성능이 좋아 보이는 착각) 때문에 변수 개수에 페널티를 준 **수정된 결정계수(Adjusted $R^2$)**를 함께 사용합니다.① 종속변수의 분산 중 독립변수로 설명되는 비율 (O):결정계수의 핵심 정의입니다. 전체 변동($SST$) 중에서 회귀 모델이 설명하는 변동($SSR$)의 비중을 의미합니다.③ 결정계수는 0~1의 범위를 가진다. (O):비율(%) 개념이므로 0(설명력 없음)에서 1(100% 완벽하게 설명) 사이의 값을 가집니다.④ 결정계수값이 클수록 모델의 유용성이 높다. (O):결정계수가 1에 가까울수록 실제 데이터가 회귀선 주위에 밀집해 있다는 뜻이므로, 모델의 예측 정확도와 설명력이 높다고 판단할 수 있습니다.💡 결정계수의 관계식 요약$$R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}$$SST (Total Sum of Squares): 전체 변동SSR (Regression Sum of Squares): 모델이 설명하는 변동SSE (Error Sum of Squares): 모델이 설명하지 못하는 오차(잔차)💡 실전 대비 체크리스트시험에서 **\"독립변수가 추가되면 수정된 결정계수는 항상 증가한다\"**라는 지문이 나오면 오답임을 꼭 기억하세요! (수정된 결정계수는 유의미하지 않은 변수가 추가되면 오히려 감소할 수 있습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc11ba2",
   "metadata": {},
   "source": [
    "# 46. 다중공선성과 관련 있는 지표는 무엇인가? \n",
    "① 스튜던트 잔차(Studentized Residual) ② 맬로우스 Cp(Mallows's Cp) ③ 분산팽창지수(VIF) ④ 쿡의 거리(Cook's Distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8007ea",
   "metadata": {},
   "source": [
    "정답은 ③ 분산팽창지수(VIF, Variance Inflation Factor) 입니다.**다중공선성(Multicollinearity)**은 독립변수들 사이에 강한 상관관계가 나타나 회귀 분석의 결과를 신뢰할 수 없게 만드는 현상입니다. 이를 진단하는 가장 대표적인 지표가 바로 VIF입니다.상세 해설 및 주요 지표 정리③ 분산팽창지수(VIF) (정답): 특정 독립변수가 다른 독립변수들에 의해 얼마나 설명되는지를 나타냅니다. 일반적으로 VIF가 10 이상이면 해당 변수들 사이에 심각한 다중공선성이 있다고 판단하며, 변수 제거 등의 조치가 필요합니다.① 스튜던트 잔차(Studentized Residual):잔차를 표준오차로 나누어 표준화한 값입니다. 주로 **이상값(Outlier)**을 식별하는 데 사용됩니다.② 맬로우스 Cp(Mallows's Cp):회귀 모델에서 변수 선택을 할 때 사용되는 지표입니다. 모델의 적합도와 변수 개수 사이의 균형을 맞추기 위해 사용하며, 값이 작을수록 좋은 모델로 간주합니다.④ 쿡의 거리(Cook's Distance):특정 관측치가 회귀 모델 전체에 미치는 영향력을 측정합니다. 데이터 중에서 회귀 계수를 크게 변화시키는 **영향력 있는 관측치(Influential Point)**를 찾는 데 쓰입니다.💡 다중공선성(Multicollinearity) 해결 방법다중공선성이 발견되면 다음과 같은 조치를 취할 수 있습니다.변수 제거: VIF가 높은 변수 중 중요도가 낮은 변수를 삭제합니다.변수 통합: 상관관계가 높은 두 변수를 합쳐 새로운 변수(예: 평균값, 주성분 등)를 만듭니다.주성분 분석(PCA): 데이터를 서로 독립적인 주성분으로 변환하여 분석합니다.릿지/라쏘 회귀: 가중치 규제(Regularization)를 통해 다중공선성의 영향을 줄입니다.💡 실전 포인트시험에서 다중공선성을 묻는 지표는 VIF와 상관계수, 허용치(Tolerance) 세 가지가 가장 많이 나옵니다. (참고: 허용치 = $1 / VIF$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20200b5",
   "metadata": {},
   "source": [
    "# 47. 다음 회귀분석 모델평가에 대한 절차로 옳은 것은? \n",
    "① 독립변수검정 → 회귀계수추정 → 회귀모델 유의성검정 ② 회귀계수추정 → 회귀모델 유의성검정 → 독립변수검정 ③ 회귀계수추정 → 독립변수 검정 → 회귀모델 유의성검정 ④ 독립변수검정 → 회귀계수추정 → 회귀모델 유의성검정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff455a",
   "metadata": {},
   "source": [
    "정답은 ② 회귀계수추정 → 회귀모델 유의성검정 → 독립변수검정입니다.회귀분석 모델을 구축하고 평가할 때는 **'모델을 만들고(추정) → 전체가 맞는지 보고(F-검정) → 개별 변수가 맞는지 확인(t-검정)'**하는 논리적 순서를 따릅니다.회귀분석 모델 평가의 표준 절차회귀계수 추정 (Parameter Estimation):먼저 데이터를 바탕으로 최소제곱법(OLS) 등을 사용하여 회귀식의 계수($\\beta_0, \\beta_1...$)를 구합니다. 즉, $y = ax + b$에서 $a$와 $b$ 값을 찾아내는 단계입니다.회귀모델 유의성 검정 (Overall Model Test):추정된 회귀식이 통계적으로 의미가 있는지 전체적으로 평가합니다.도구: F-검정가설: \"모든 회귀계수는 0이다(모델이 쓸모없다)\"라는 귀무가설을 기각하는지 확인합니다.독립변수 유의성 검정 (Individual Variable Test):모델이 유의하다면, 이제 각각의 독립변수가 종속변수에 실제로 영향을 주는지 하나씩 검정합니다.도구: t-검정가설: \"해당 변수의 회귀계수는 0이다\"를 기각하는지 확인합니다.모델의 설명력 확인 및 진단:결정계수($R^2$): 모델이 데이터를 얼마나 잘 설명하는지 확인합니다.잔차 분석: 선형성, 등분산성, 독립성, 정규성 가정을 만족하는지 진단합니다.💡 요약 표 (시험 단골 암기용)단계핵심 도구목적추정최소제곱법(OLS)회귀계수($\\beta$) 결정모델 유의성F-검정모델 전체가 유효한가?변수 유의성t-검정특정 변수가 유효한가?적합도결정계수($R^2$)모델의 설명력은 얼마인가?💡 실전 팁보통 분석 리포트(ANOVA 테이블)를 보면 F-검정 결과가 먼저 나오고, 그 아래 개별 변수의 t-검정 결과가 나열됩니다. 큰 틀(모델)부터 작은 틀(변수) 순서로 검증한다고 생각하면 이해가 빠릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ed4d71",
   "metadata": {},
   "source": [
    "# 48. 다음 조건에 대한 값을 구하시오. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f64e94",
   "metadata": {},
   "source": [
    "[비흡연자 100명 중 폐암 20명, 비흡연자 100명 중 폐암 4명 시 흡연여부에 대한 폐암 오즈비] \n",
    "(보기 생략: 1, 3, 5, 6 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7bf172",
   "metadata": {},
   "source": [
    "정답은 6입니다.이 문제는 **오즈(Odds)**와 **오즈비(Odds Ratio)**의 개념을 정확히 이해하고 계산할 수 있는지 묻는 문제입니다.1. 오즈(Odds) 계산오즈란 '사건이 발생할 확률'을 '사건이 발생하지 않을 확률'로 나눈 값입니다. ($\\text{Odds} = \\frac{p}{1-p}$)흡연자의 폐암 오즈 ($O_1$):폐암 발생: 20명, 미발생: 80명$O_1 = \\frac{20}{80} = \\mathbf{0.25}$ (또는 $1/4$)비흡연자의 폐암 오즈 ($O_2$):폐암 발생: 4명, 미발생: 96명$O_2 = \\frac{4}{96} \\approx \\mathbf{0.0416}$ (또는 $1/24$)2. 오즈비(Odds Ratio) 계산오즈비는 두 집단의 오즈를 비교한 비율입니다. ($OR = \\frac{O_1}{O_2}$)$OR = \\frac{0.25}{0.0416...} = \\frac{1/4}{1/24}$$OR = \\frac{1}{4} \\times \\frac{24}{1} = \\mathbf{6}$💡 해석 및 주요 포인트결과 해석: 흡연자의 폐암 오즈가 비흡연자에 비해 6배 높다는 것을 의미합니다.로지스틱 회귀분석과의 관계: 로지스틱 회귀분석의 회귀계수($\\beta$)에 지수($e$)를 취하면($e^\\beta$) 바로 이 오즈비가 됩니다. 특정 독립변수가 1단위 증가할 때 종속변수(사건 발생)의 오즈가 몇 배 증가하는지를 나타냅니다.💡 주의: 오즈비 vs 상대위험도(Relative Risk)시험에서 헷갈리기 쉬운 개념입니다.상대위험도(RR): 전체 중 발생 확률의 비 $\\left(\\frac{20/100}{4/100} = 5\\right)$오즈비(OR): 발생/미발생 비율의 비 $\\left(\\frac{20/80}{4/96} = 6\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d47375",
   "metadata": {},
   "source": [
    "# 49. 다음 수식에 대한 설명으로 옳은 것은?\n",
    "$$\\ln\\left(\\frac{P(Y=1|X)}{1-P(Y=1|X)}\\right) = \\beta_0 + \\beta_1 X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5efcd",
   "metadata": {},
   "source": [
    "① $X$가 1 단위 증가하면 오즈 $e^{\\beta_1}$ 만큼 증가한다.② $X$가 1 단위 증가하면 $\\beta_0$ 만큼 증가한다.③ $X$가 1 단위 증가하면 $y$ 만큼 증가한다.④ $X$가 1 단위 증가하면 $e^y$ 만큼 증가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826cda9b",
   "metadata": {},
   "source": [
    "정답은 ① $X$가 1 단위 증가하면 오즈(Odds)가 $e^{\\beta_1}$ 만큼 증가한다. 입니다.제시된 수식은 **로지스틱 회귀분석(Logistic Regression)**의 핵심인 로짓 변환(Logit Transformation) 식입니다. 이 식의 의미를 파악하는 것이 중요합니다.상세 해설 및 해석좌변의 의미 (Log-Odds):수식 좌변의 $\\ln\\left(\\frac{P}{1-P}\\right)$는 성공 확률($P$)을 실패 확률($1-P$)로 나눈 값(오즈, Odds)에 자연로그를 취한 것입니다. 이를 **로짓(Logit)**이라고 부릅니다.계수 $\\beta_1$의 해석:로그가 취해진 상태이므로, 독립변수 $X$가 1 단위 증가할 때 좌변인 '로그 오즈'는 $\\beta_1$만큼 증가합니다.하지만 우리가 궁금한 것은 실제 확률의 비율인 **'오즈(Odds)'**가 얼마나 변하는가입니다. 로그를 벗기기 위해 양변에 지수($e$)를 취하면 다음과 같습니다.$$\\text{Odds} = e^{\\beta_0 + \\beta_1X} = e^{\\beta_0} \\cdot e^{\\beta_1X}$$따라서 $X$가 1 증가하면 오즈는 $e^{\\beta_1}$ 배가 됩니다. (선택지의 '만큼'은 배율의 의미로 쓰였습니다.)오답 분석:②, ③: 단순히 $\\beta_0$나 $y$가 증가한다는 설명은 선형 회귀분석의 해석 방식이며, 로지스틱 회귀에는 맞지 않습니다.④: $e^y$는 수식의 흐름상 적절한 해석이 아닙니다.💡 로지스틱 회귀분석 요약목적: 종속변수가 범주형(예: 성공/실패, 예/아니오)일 때, 사건이 발생할 확률을 예측합니다.활용: $X$가 커질수록 확률 $P$는 0과 1 사이에서 **S자 곡선(Sigmoid Function)**을 그리며 변화합니다.회귀계수($\\beta$) 해석: 회귀계수에 지수를 취한 $e^{\\beta}$를 **오즈비(Odds Ratio)**라고 부릅니다.Getty Images💡 실전 팁시험에서 로지스틱 회귀 계수 해석 문제가 나오면 **\"로그 오즈는 $\\beta$만큼 증가, 그냥 오즈는 $e^{\\beta}$배 증가\"**라는 공식만 기억해도 금방 풀 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872e941",
   "metadata": {},
   "source": [
    "# 50. 의사결정나무에 대한 설명으로 옳지 않은 것은? \n",
    "① 주요 알고리즘으로 CART와 C4.5가 있다. ② 분류기준으로 정보이득, 지니계수, 엔트로피를 사용한다. ③ 이진분할을 사용할 수 있다. ④ 분리기준은 전향확률이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd8323",
   "metadata": {},
   "source": [
    "정답은 ④ 분리기준은 전향확률이다. 입니다.의사결정나무(Decision Tree)의 핵심 원리와 분리 규칙을 이해하고 있는지 묻는 문제입니다.상세 해설 및 오답 분석④ 분리기준은 전향확률이다 (X):의사결정나무의 분리기준은 부모 마디보다 자식 마디의 **순도(Purity)**가 높아지거나 **불순도(Impurity)**가 낮아지는 방향으로 결정됩니다. '전향확률'이라는 용어는 의사결정나무의 분리 기준에서 사용되지 않습니다. (보통 은닉 마르코프 모델 등에서 쓰이는 용어입니다.)① 주요 알고리즘으로 CART와 C4.5가 있다 (O):CART: 지니계수를 사용하여 이진 분리를 수행하는 가장 대표적인 알고리즘입니다.C4.5 / ID3: 엔트로피(정보이득) 개념을 사용하여 분리합니다.② 분류기준으로 정보이득, 지니계수, 엔트로피를 사용한다 (O):마디를 어떻게 나눌지 결정할 때 사용하는 수치들입니다.지니계수: 값이 작을수록 순도가 높습니다. (CART에서 사용)엔트로피: 데이터의 무질서도를 측정하며, 낮을수록 순도가 높습니다.③ 이진분할을 사용할 수 있다 (O):의사결정나무는 하나의 부모 마디에서 두 개의 자식 마디로 나누는 **이진 분할(Binary Split)**을 기본으로 하는 경우가 많습니다(특히 CART 알고리즘).💡 의사결정나무 알고리즘별 분류 지표 요약의사결정나무의 알고리즘에 따라 사용하는 통계적 지표가 다르므로 꼭 구분해서 암기해야 합니다.알고리즘이탈/분류 지표 (범주형)이탈/분류 지표 (연속형)특징CART지니계수(Gini Index)분산의 감소량이진 분할만 가능C4.5 / ID3엔트로피 / 정보이득-다지 분할 가능CHAID카이제곱 통계량F-통계량다지 분할, 통계적 유의성 중시💡 의사결정나무의 장단점장점: 분석 과정이 시각화되어 해석이 매우 쉽고, 데이터의 정규성이나 등분산성 가정이 필요 없는 비모수적 방법입니다.단점: 새로운 데이터에 대해 **과적합(Overfitting)**이 발생하기 쉽습니다. 이를 방지하기 위해 가지치기(Pruning) 작업이 필수적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3532594",
   "metadata": {},
   "source": [
    "# 51. 의사결정나무에서 D에 들어가는 노드는 무엇인가?\n",
    "[데이터 집합 : $x_1 = \\{6, 7, 8\\}, x_2 = \\{2, 3, 4, 5\\}$]\n",
    "[의사결정나무 이미지: A(뿌리) → $x_2 < 3$ (왼쪽), $x_2 \\ge 3$ (오른쪽) 등]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c52028",
   "metadata": {},
   "source": [
    "① $x_1 = 6, x_2 = 2$\n",
    "② $x_1 = 6, x_2 = 4$\n",
    "③ $x_1 = 8, x_2 = 6$\n",
    "④ $x_1 = 2, x_2 = 6$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c98f8c",
   "metadata": {},
   "source": [
    "정답은 ① $x_1=6, x_2=2$ 입니다.의사결정나무는 각 노드(마디)에서 설정된 조건식의 참/거짓 여부에 따라 데이터를 아래로 분류해 나가는 구조입니다. 문제에서 제시된 나무의 경로를 따라가며 데이터를 필터링해 보겠습니다.상세 해설: 경로 추적의사결정나무의 이미지를 보면 $A$(뿌리 노드)에서 첫 번째 분기 기준이 $x_2 < 3$임을 알 수 있습니다.조건 확인: $x_2 < 3$True(예): 왼쪽 자식 노드 $D$로 이동합니다.False(아니오): 오른쪽 자식 노드로 이동합니다.보기에 주어진 데이터 대입:① $x_1=6, x_2=2$: $x_2$ 값이 2이므로 $2 < 3$ 조건을 **만족(True)**합니다. 따라서 **왼쪽 노드 $D$**에 속하게 됩니다.② $x_1=6, x_2=4$: $4 < 3$ 조건을 만족하지 않으므로 오른쪽으로 이동합니다.③ $x_1=8, x_2=6$: $6 < 3$ 조건을 만족하지 않으므로 오른쪽으로 이동합니다.④ $x_1=2, x_2=6$: $6 < 3$ 조건을 만족하지 않으므로 오른쪽으로 이동합니다.💡 의사결정나무의 노드 명칭 요약뿌리 노드 (Root Node): 나무가 시작되는 맨 위의 노드 (위 문제의 $A$)중간 노드 (Internal Node): 분기 조건이 있는 중간 단계의 노드단말 노드 (Leaf Node): 더 이상 자식이 없는 끝 노드. 최종 분류 결과나 예측값이 결정되는 곳 (위 문제의 $D$)부모/자식 노드: 상위 노드와 하위 노드의 관계💡 실전 포인트의사결정나무 문제에서는 \"변수 $x_1$이 조건에 사용되지 않았더라도\" 당황하지 마세요. 모든 변수가 나무를 만드는 데 쓰이는 것은 아니며, 가장 불순도를 낮게 만드는 최적의 변수($x_2$)부터 우선적으로 선택되어 조건문에 활용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcbff5",
   "metadata": {},
   "source": [
    "# 52. 다음 중 학습률에 대한 설명으로 옳은 것은? \n",
    "① 학습률 수치가 크면 가중치를 조금만 수정하면 된다. ② 신경망은 모든 데이터를 한번에 최적화해야 한다. ③ 학습률이 높을 경우 학습시간은 오래 걸리나, 증분이 작아서 최소 손실점수를 찾기 쉽다. ④ 학습률 상수를 작게 하면 반복횟수 모두 고려해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc34cc0",
   "metadata": {},
   "source": [
    "정답은 ④ 학습률 상수를 작게 하면 반복횟수 모두 고려해야 한다. 입니다.이 문제는 인공신경망과 머신러닝 학습의 핵심 파라미터인 **학습률(Learning Rate)**의 특성을 정확히 이해하고 있는지 묻고 있습니다.상세 해설 및 오답 분석④ 학습률 상수를 작게 하면 반복횟수를 많이 고려해야 한다 (O):학습률이 작다는 것은 한 번의 학습 단계에서 가중치($w$)를 아주 조금씩만 업데이트한다는 뜻입니다. 따라서 최적의 지점(Loss 최소점)에 도달하기까지 **더 많은 반복 학습(Epoch)**이 필요하게 됩니다.① 학습률 수치가 크면 가중치를 조금만 수정하면 된다 (X):반대로 설명되었습니다. 학습률이 크면 가중치를 한 번에 크게 수정하게 됩니다. 이 경우 학습 속도는 빠를 수 있으나 최적점을 지나쳐 버리는 오버슈팅(Overshooting) 현상이 발생할 수 있습니다.② 신경망은 모든 데이터를 한번에 최적화해야 한다 (X):신경망은 데이터를 작은 단위로 나누어 학습하는 미니 배치(Mini-batch) 경사하강법 등을 주로 사용합니다. 모든 데이터를 한꺼번에 계산하는 것은 메모리 효율과 학습 성능 면에서 비효율적인 경우가 많습니다.③ 학습률이 높을 경우... 최소 손실점수를 찾기 쉽다 (X):학습률이 높으면 보폭이 너무 커서 최소 지점 주위를 왔다 갔다 하거나 아예 발산해 버릴 위험이 있어 최적의 손실점수를 찾기가 오히려 더 어렵습니다. 증분이 작아서 정교하게 찾는 것은 '낮은 학습률'의 특징입니다.💡 학습률(Learning Rate) 크기에 따른 영향 비교구분학습률이 너무 클 때학습률이 너무 작을 때보폭매우 큼 (성큼성큼)매우 작음 (아장아장)속도빠르지만 발산 위험 있음매우 느림결과최적점을 지나칠 수 있음 (Overshooting)최적점에 도달하기 전 학습 종료 가능필요 반복수적음많음💡 실전 팁: 경사하강법(Gradient Descent)학습률은 경사하강법 공식에서 기울기 앞에 곱해지는 상수를 말합니다.$$w = w - \\eta \\frac{\\partial L}{\\partial w}$$여기서 $\\eta$(에타)가 바로 학습률입니다. 이 값이 너무 크거나 작지 않게 적절히 설정하는 것을 '하이퍼파라미터 튜닝'이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995db05b",
   "metadata": {},
   "source": [
    "# 53. 인공신경망 학습 시 과적합 방지방법으로 적절하지 않은 것은?\n",
    "① 은닉 노드 수를 줄인다. ② 가중치 절댓값을 최대로 한다. ③ 에폭(Epoch) 수를 줄인다. ④ 은닉층(Hidden Layer) 수를 줄인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cbb956",
   "metadata": {},
   "source": [
    "정답은 ② 가중치 절댓값을 최대로 한다. 입니다.이 문제는 인공신경망이 학습 데이터에 지나치게 최적화되어 실제 데이터에서 성능이 떨어지는 현상인 **과적합(Overfitting)**을 어떻게 해결하는지 묻고 있습니다.상세 해설 및 오답 분석② 가중치 절댓값을 최대로 한다. (X):가중치($w$)의 절댓값이 커진다는 것은 특정 입력 신호에 대해 모델이 매우 민감하게 반응한다는 뜻이며, 이는 모델의 복잡도를 높여 과적합을 유발하는 주요 원인이 됩니다.올바른 방법: 가중치가 너무 커지지 않도록 억제하는 **가중치 규제(Weight Decay/Regularization, 예: L1, L2 규제)**를 사용해야 합니다.① 은닉 노드(Hidden Node) 수를 줄인다. (O):신경망의 구조가 복잡할수록(노드가 많을수록) 데이터의 미세한 노이즈까지 학습하여 과적합될 가능성이 높습니다. 따라서 노드 수를 적절히 줄여 모델을 단순화하는 것이 도움이 됩니다.③ 에폭(Epoch) 수를 줄인다. (O):에폭은 전체 데이터를 반복 학습하는 횟수입니다. 너무 많이 학습하면 훈련 데이터의 사소한 특징까지 다 외워버리게 됩니다. 이를 방지하기 위해 성능이 더 이상 오르지 않을 때 학습을 멈추는 조기 종료(Early Stopping) 기법을 사용합니다.④ 은닉층(Hidden Layer) 수를 줄인다. (O):은닉 노드 수를 줄이는 것과 마찬가지로, 층(Layer)의 깊이를 줄여 모델의 복잡도(Capacity)를 낮추면 일반화 성능을 높일 수 있습니다.💡 인공신경망 과적합 방지 기법 요약기법설명드롭아웃 (Dropout)학습 시 무작위로 일부 뉴런을 끄고 학습하여 특정 노드에 대한 의존도를 낮춤규제 (Regularization)손실 함수에 가중치의 크기(L1, L2)를 더해 가중치가 커지는 것을 방해함조기 종료 (Early Stopping)검증 데이터의 손실이 증가하기 시작하는 시점에서 학습을 중단함데이터 증강 (Data Augmentation)훈련 데이터를 변형하여 데이터의 양을 늘리고 모델을 강건하게 만듦💡 실전 팁\"모델의 복잡도를 낮춘다\"는 개념을 기억하세요! 노드 수 감소, 층 수 감소, 가중치 크기 억제는 모두 모델을 단순하게 만들어 과적합을 피하려는 전략입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63e755",
   "metadata": {},
   "source": [
    "# 54. 원핫인코딩에 대한 설명으로 옳지 않은 것은? \n",
    "① 서로 다른 단어의 내적은 0이다. ② 모든 단어는 원핫인코딩이 가능하다. ③ 공간차원 효율이 좋다. ④ 텍스트를 수치화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90721b4",
   "metadata": {},
   "source": [
    "정답은 ③ 공간차원 효율이 좋다. 입니다.**원-핫 인코딩(One-Hot Encoding)**의 개념과 장단점을 묻는 문제입니다.상세 해설 및 오답 분석③ 공간차원 효율이 좋다. (X):원-핫 인코딩의 가장 큰 단점은 **'차원의 저주'**입니다. 단어 사전의 크기가 10,000개라면 단어 하나를 표현하기 위해 9,999개의 0과 단 하나의 1로 이루어진 벡터를 만들어야 합니다. 단어 수가 늘어날수록 벡터의 공간이 기하급수적으로 커지고 대부분이 0으로 채워지는 희소(Sparse) 벡터가 되어 공간 효율성이 매우 떨어집니다.① 서로 다른 단어의 내적은 0이다. (O):각 단어 벡터는 하나의 위치만 1이고 나머지는 모두 0입니다. 예를 들어 단어 A가 [1, 0, 0]이고 단어 B가 [0, 1, 0]이라면, 두 벡터를 내적하면 0이 됩니다. 이는 모든 단어가 서로 **직교(Orthogonal)**하며, 단어 간의 유사성을 표현할 수 없다는 특징을 보여줍니다.② 모든 단어는 원핫인코딩이 가능하다. (O):고유한 인덱스만 부여할 수 있다면 어떤 범주형 데이터나 단어든 0과 1의 조합으로 인코딩할 수 있습니다.④ 텍스트를 수치화한다. (O):머신러닝 알고리즘은 텍스트를 직접 이해할 수 없기 때문에, 원-핫 인코딩을 통해 컴퓨터가 계산할 수 있는 숫자(벡터) 형태로 변환해 주는 과정이 필요합니다.💡 원-핫 인코딩 vs 워드 임베딩(Word Embedding)원-핫 인코딩의 한계를 극복하기 위해 실제 자연어 처리(NLP)에서는 임베딩 기법을 많이 사용합니다.구분원-핫 인코딩 (One-Hot)워드 임베딩 (Embedding)차원고차원 (단어 수만큼)저차원 (사용자 설정)벡터 형태희소(Sparse) 벡터 (대부분 0)밀집(Dense) 벡터 (실수값)단어 의미반영 불가 (단순 인덱스)반영 가능 (유사도 측정)공간 효율낮음높음💡 실전 팁시험에서 **\"원-핫 인코딩은 단어 간의 의미적 유사성을 잘 표현한다\"**는 지문이 나오면 무조건 오답입니다. 각 단어 벡터는 서로 독립적이고 직교하기 때문에 \"사과\"와 \"배\"가 비슷하다는 정보를 담을 수 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb13e49",
   "metadata": {},
   "source": [
    "# 55. 다음 중 군집 수 $k$를 직접 설정하지 않아도 되는 모델은?\n",
    "① K-MEDIAN ② K-MEANS ③ DBSCAN ④ MIXTURE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35212f37",
   "metadata": {},
   "source": [
    "정답은 ③ DBSCAN 입니다.이 문제는 **비지도학습(Unsupervised Learning)**의 대표적인 기법인 군집 분석에서 각 알고리즘이 군집의 개수($k$)를 결정하는 방식을 묻고 있습니다.상세 해설 및 주요 알고리즘 비교③ DBSCAN (Density-Based Spatial Clustering of Applications with Noise) (정답):특징: 데이터가 밀집된 정도(밀도)를 기준으로 군집을 형성합니다.설정 방식: 사용자가 군집의 개수($k$)를 미리 정해주지 않습니다. 대신 **반경(Epsilon)**과 **최소 데이터 개수(MinPts)**를 설정하면, 알고리즘이 스스로 밀도를 파악하여 적절한 군집 개수를 찾아냅니다.장점: 기하학적이고 복잡한 모양의 군집도 잘 찾아내며, 이상치(Noise)를 효과적으로 걸러낼 수 있습니다.① K-MEDIAN / ② K-MEANS:특징: 군집의 중심점(Centroid)을 기준으로 가장 가까운 데이터를 할당하는 방식입니다.설정 방식: 분석 시작 전에 반드시 군집의 개수($k$)를 사용자가 직접 지정해야 합니다. (예: \"이 데이터를 3개의 그룹으로 나눠라\")④ MIXTURE MODEL (혼합 모델, 예: GMM):특징: 데이터가 여러 개의 확률 분포(주로 가우시안 분포)로부터 생성되었다고 가정합니다.설정 방식: 일반적으로 몇 개의 분포가 혼합되어 있는지($k$)를 사전에 설정해야 합니다.💡 군집 분석 알고리즘 요약알고리즘군집 수(k) 설정군집 형성 원리특징K-Means필수거리 기반 (중심점)계산이 빠름, 구형 군집에 유리DBSCAN필요 없음밀도 기반노이즈 제거 탁월, 복잡한 모양 가능계층적 군집필요 없음거리 기반 (트리 구조)덴드로그램(Dendrogram)으로 시각화💡 실전 팁: 엘보우(Elbow) 방법K-means처럼 $k$를 직접 설정해야 하는 경우, 어떤 $k$가 최적인지 판단하기 위해 **엘보우 방법(Elbow Method)**이나 **실루엣 계수(Silhouette Coefficient)**를 사용합니다. 그래프가 꺾이는 지점(팔꿈치 모양)의 $k$를 최적값으로 선택합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce9b7f",
   "metadata": {},
   "source": [
    "# 56. 범주형 종속변수 예측 모델이 아닌 것은?\n",
    "① 다중선형회귀 ② 다중로지스틱회귀분석 ③ 서포트벡터머신 ④ 다중퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc9d11",
   "metadata": {},
   "source": [
    "정답은 ① 다중선형회귀 입니다.이 문제는 예측하고자 하는 결과값(종속변수)이 **수치형(연속형)**인지 **범주형(이산형)**인지 구분할 수 있는지를 묻고 있습니다.상세 해설 및 분류① 다중선형회귀 (Multiple Linear Regression) (정답):독립변수가 여러 개인 회귀분석으로, 종속변수가 **수치형(연속형)**일 때 사용합니다.예: (공부 시간, 수면 시간, 학원 개수) $\\rightarrow$ 시험 점수(0~100점) 예측.수식의 결과값이 제한 없이 어떤 숫자든 나올 수 있으므로 범주를 나누는 용도로는 적합하지 않습니다.② 다중로지스틱회귀분석 (Multinomial Logistic Regression):종속변수가 범주형일 때 사용하는 회귀 모델입니다.결과가 '성공/실패' 혹은 'A/B/C형' 등 정해진 범주 중 하나로 분류될 확률을 계산합니다.③ 서포트벡터머신 (SVM, Support Vector Machine):데이터를 분류하는 최적의 결정 경계(Hyperplane)를 찾는 알고리즘입니다.주로 분류(범주형) 문제에 매우 강력한 성능을 발휘합니다.④ 다중퍼셉트론 (MLP, Multi-Layer Perceptron):인공신경망(ANN)의 기본 구조로, 활성화 함수(Softmax 등)를 거쳐 데이터를 여러 범주로 분류하는 데 널리 사용됩니다.💡 종속변수에 따른 모델 구분 요약구분회귀 (Regression)분류 (Classification)종속변수 유형수치형 (연속형)범주형 (이산형/명목형)예측 예시주가, 매출액, 온도, 키스팸 여부, 등급, 암 진단대표 모델선형회귀, 회귀나무로지스틱 회귀, SVM, KNN, 의사결정나무💡 실전 팁'로지스틱 회귀'는 이름에 '회귀'가 들어가지만 실제로는 분류 모델이라는 점이 시험의 가장 큰 함정입니다. 꼭 기억해 두세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848e9e7",
   "metadata": {},
   "source": [
    "# 57. 주성분분석(PCA)에 대한 설명으로 옳지 않은 것은? \n",
    "① 대표적 차원축소 기법이다. ② 공분산행렬의 고유벡터는 데이터가 어떤 방향으로 분산되었는지를 나타낸다. ③ 다수의 n차원 데이터에 대해 데이터 중심으로부터 데이터의 응집력이 높은 n개의 직교 방향을 분석하는 방법이다. ④ 비음수 행렬분해를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c55de",
   "metadata": {},
   "source": [
    "정답은 ④ 비음수 행렬분해를 사용한다. 입니다.\n",
    "\n",
    "이 문제는 차원 축소의 대표적인 기법인 **주성분 분석(PCA)**의 수학적 원리와 특징을 묻고 있습니다.\n",
    "\n",
    "상세 해설 및 오답 분석\n",
    "④ 비음수 행렬분해를 사용한다. (X):\n",
    "\n",
    "PCA는 데이터의 분산을 극대화하는 방향을 찾기 위해 **고윳값 분해(Eigenvalue Decomposition)**나 **특잇값 분해(SVD, Singular Value Decomposition)**를 사용합니다.\n",
    "\n",
    "**비음수 행렬분해(NMF, Non-negative Matrix Factorization)**는 모든 원소가 양수인 행렬을 두 개의 양수 행렬로 분해하는 또 다른 차원 축소 기법으로, PCA와는 수학적 원리가 다릅니다.\n",
    "\n",
    "① 대표적 차원축소 기법이다. (O):\n",
    "\n",
    "PCA는 고차원의 데이터를 정보 손실을 최소화하면서 저차원(주성분)으로 변환하는 가장 널리 쓰이는 기법입니다.\n",
    "\n",
    "② 고유벡터는 데이터가 어떤 방향으로 분산되었는지를 나타낸다. (O):\n",
    "\n",
    "공분산 행렬을 분해했을 때 나오는 **고유벡터(Eigenvector)**는 주성분의 방향축을 의미하며, 해당 **고윳값(Eigenvalue)**은 그 방향으로의 분산 크기를 의미합니다.\n",
    "\n",
    "③ n개의 직교 방향을 분석하는 방법이다. (O):\n",
    "\n",
    "PCA의 핵심은 서로 상관관계가 없는(독립적인) 새로운 축들을 찾는 것인데, 이 축들은 서로 **직교(Orthogonal)**하는 성질을 가집니다. 제1주성분과 제2주성분은 서로 90도 각도를 이룹니다.\n",
    "\n",
    "💡 PCA의 주요 목적과 특징\n",
    "다중공선성 해결: 서로 상관관계가 높은 변수들을 선형 결합하여 독립적인 주성분으로 만듦으로써 회귀 분석 등의 다중공선성 문제를 해결합니다.\n",
    "\n",
    "데이터 시각화: 4차원 이상의 고차원 데이터를 2~3차원의 주성분으로 줄여 평면에 시각화할 수 있습니다.\n",
    "\n",
    "노이즈 제거: 분산이 작은(정보량이 적은) 하위 주성분을 버림으로써 데이터 내의 노이즈를 제거하는 효과가 있습니다.\n",
    "\n",
    "💡 실전 팁: 주성분 개수 선택 기준\n",
    "몇 개의 주성분을 사용할지 결정할 때는 보통 **스크리 그래프(Scree Plot)**를 그려서 고윳값이 급격하게 완만해지는 지점(Elbow Point)을 선택하거나, 누적 기여율이 70~80% 이상이 되는 지점을 선택합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caec127",
   "metadata": {},
   "source": [
    "# 58. 과거 데이터가 현재에 영향을 주고 현재 값이 미래의 영향을 주는 시계열 평가방법은?\n",
    "① 지니계수 ② 엔트로피계수 ③ 실루엣계수 ④ 자기상관성 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b8df2",
   "metadata": {},
   "source": [
    "정답은 ④ 자기상관성 함수 (ACF, Autocorrelation Function) 입니다.이 문제는 시계열 데이터의 가장 핵심적인 특징인 **'시간적 의존성'**을 측정하는 지표를 묻고 있습니다.상세 해설④ 자기상관성 함수 (ACF) (정답):**자기상관(Autocorrelation)**이란 '자기 자신'과 '상관관계'의 합성어입니다.동일한 변수에서 현재의 시점($t$)과 과거 시점($t-k$)의 값들 사이의 선형적 연관성을 측정합니다.예를 들어, 오늘 주가가 어제 주가에 영향을 받는다면 자기상관성이 존재한다고 합니다. 시계열 모델인 AR(자기회귀) 모델을 결정할 때 필수적으로 확인하는 지표입니다.① 지니계수 (Gini Index):의사결정나무(CART 알고리즘)에서 노드를 분리할 때 사용하는 불순도 측정 지표입니다. 혹은 경제학에서 소득 불평등을 측정할 때 사용합니다.② 엔트로피계수 (Entropy):의사결정나무(C4.5/ID3 알고리즘)에서 사용하며, 데이터의 **무질서도(불확실성)**를 측정하는 지표입니다.③ 실루엣계수 (Silhouette Coefficient):**군집 분석(Clustering)**의 품질을 평가하는 지표입니다. 군집 내 데이터들이 얼마나 뭉쳐 있고, 다른 군집과는 얼마나 떨어져 있는지를 계산합니다.💡 시계열 분석의 주요 개념 요약시계열 분석을 공부할 때 자기상관과 함께 반드시 알아야 할 용어들입니다.용어설명정상성 (Stationarity)시간의 흐름에 따라 평균과 분산이 일정한 상태 (분석의 전제 조건)백색잡음 (White Noise)평균이 0이고 자기상관이 없는 무작위적인 변동차분 (Differencing)비정상 시계열을 정상 시계열로 바꾸기 위해 인접 시점 간의 차이를 구하는 것시차 (Lag)현재 시점과 과거 시점 사이의 시간 간격💡 실전 팁시계열 문제에서 **\"과거의 나\"**가 **\"현재의 나\"**에게 영향을 준다는 표현이 나오면 키워드는 무조건 **'자기(Auto)'**입니다. 자기상관함수(ACF), 편자기상관함수(PACF), 자기회귀모델(AR) 등이 이에 해당합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c0891",
   "metadata": {},
   "source": [
    "# 59. 다음 설명 중 옳지 않은 것을 모두 고르시오. \n",
    "가. 시계열은 종단면 데이터(Longitudinal Data)로 여러 대상에 대해 시간에 따라 측정한 데이터를 표시한다. 나. 전쟁이나 홍수 등의 불규칙 요인도 시계열분석으로 예측이 가능하다. 다. 백색잡음(White Noise)은 시간의 상관관계를 나타낸다. 라. 정상성(Stationary)의 조건으로 모든 시점에 대해 일정한 평균을 가진다. 마. 이동평균법은 과거로부터 현재까지의 시계열 자료를 대상으로 일정 기간별 이동평균을 계산하고, 이들의 추세를 파악하여 다음 기간을 예측하는 방법이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d066e",
   "metadata": {},
   "source": [
    "① 가, 나 ② 나, 다 ③ 다, 라, 마 ④ 나, 다, 라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef73ed7",
   "metadata": {},
   "source": [
    "정답은 ② 나, 다 입니다.앞서 풀이한 내용과 동일하게, 시계열 분석의 핵심 이론인 불규칙 요인의 특징과 백색잡음의 정의를 묻는 문제입니다. 시험에 자주 출제되는 함정을 다시 한번 정리해 드릴게요.상세 해설 및 오답 분석나. 전쟁이나 홍수 등의 불규칙 요인도 예측이 가능하다. (X):시계열 데이터는 크게 4가지 성분으로 분해됩니다. 그중 **불규칙 요인(Irregular component)**은 지진, 전쟁, 홍수처럼 예측할 수 없는 우발적인 사건에 의해 발생합니다. 이는 수식화하거나 예측 모델에 반영할 수 없는 무작위 변동입니다.다. 백색잡음(White Noise)은 시간의 상관관계를 나타낸다. (X):백색잡음은 시계열 모델이 설명하고 남은 '찌꺼기'와 같습니다. 가장 중요한 특징은 자기상관(Autocorrelation)이 0이라는 점입니다. 즉, 과거의 값이 현재나 미래의 값에 어떠한 선형적 영향도 주지 않는 '상관관계가 없는' 상태를 말합니다. 만약 상관관계가 남아있다면, 그것은 아직 모델이 데이터를 덜 분석했다는 뜻입니다.옳은 지문 분석가. 시계열은 종단면 데이터(Longitudinal Data)이다. (O):하나의 관측 대상(예: 삼성전자 주가)을 일정한 시간 간격으로 반복해서 측정한 데이터를 말합니다. 여러 대상을 한 시점에 조사하는 횡단면 데이터(Cross-sectional)와 대비되는 개념입니다.라. 정상성(Stationary)의 조건 (O):시계열 분석을 하기 위한 전제 조건입니다. 평균이 일정해야 하고, 분산도 일정해야 하며, 공분산은 시점($t$)이 아닌 시차($k$)에만 의존해야 합니다.마. 이동평균법(Moving Average) (O):최근 $n$개 데이터의 평균을 내어 다음 시점을 예측합니다. 불규칙한 변동을 매끄럽게(Smoothing) 만들어 추세를 보기 좋게 해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe3568",
   "metadata": {},
   "source": [
    "# 60. RNN에서의 장기의존성 문제를 보완하며, 은닉게이트(Forget Gate)를 업데이트(Update) 게이트로 개선하여 연산을 줄인 모델로 알맞은 것은? \n",
    "① RNN 단축 ② RNN 양축 ③ GRU ④ LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2af59f",
   "metadata": {},
   "source": [
    "정답은 ③ GRU (Gated Recurrent Unit) 입니다.이 문제는 순환 신경망(RNN)의 한계를 극복하기 위해 제안된 다양한 모델들의 차이점을 알고 있는지 묻고 있습니다.상세 해설 및 오답 분석③ GRU (Gated Recurrent Unit) (정답):특징: LSTM의 복잡한 구조를 단순화하여 연산량을 줄인 모델입니다.구조: LSTM에는 3개의 게이트(Forget, Input, Output)가 있지만, GRU는 이를 **업데이트 게이트(Update Gate)**와 리셋 게이트(Reset Gate) 2개로 합쳤습니다.장점: LSTM보다 파라미터 수가 적어 학습 속도가 빠르면서도, RNN의 고질적인 문제인 장기의존성(Long-Term Dependency) 문제를 효과적으로 해결합니다.④ LSTM (Long Short-Term Memory):RNN의 장기의존성 문제를 해결하기 위해 처음 등장한 모델입니다. Cell State라는 개념과 3개의 게이트를 사용합니다. 성능은 매우 뛰어나지만 구조가 복잡하여 연산량이 많다는 단점이 있습니다. 문제에서 제시한 '연산을 줄인 모델'은 LSTM을 개선한 GRU를 의미합니다.① RNN 단축 / ② RNN 양축:인공신경망에서 공식적으로 사용하는 표준 용어가 아닙니다. (양방향 RNN - Bidirectional RNN 등은 존재하지만 문제의 설명과는 거리가 멉니다.)💡 LSTM vs GRU 핵심 비교구분LSTMGRU게이트 수3개 (Forget, Input, Output)2개 (Update, Reset)상태 전달Cell State, Hidden State 분리Hidden State 하나로 통합연산량상대적으로 많음상대적으로 적음 (빠름)특징데이터 양이 많을 때 유리데이터 양이 적거나 빠른 학습이 필요할 때 유리💡 용어 정리: 장기의존성(Long-Term Dependency) 문제란?RNN에서 입력 데이터의 길이가 길어질수록 앞부분의 정보가 뒤로 전달되지 못하고 소실되는 현상(Gradient Vanishing)을 말합니다. LSTM과 GRU는 '게이트'라는 장치를 통해 어떤 정보를 기억하고 버릴지 결정함으로써 이 문제를 해결합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b72f1f",
   "metadata": {},
   "source": [
    "# 61. 로지스틱회귀분석에서 관심범주(Positive)의 확률 추정값 P를 구할 때, 다음 조건을 기준으로 옳게 설명한 것은?\n",
    "[관심범주 : P(성공), 1개의 독립변수, $c=0.5$ ($0 \\le c \\le 1$)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b9c57",
   "metadata": {},
   "source": [
    "① $c=0$ 이면, 민감도와 특이도 차이는 1이다.\n",
    "② $c=0.5$ 이면, 민감도와 특이도 차이는 1이다.\n",
    "③ $c=0$ 이면, 민감도와 정밀도 차이는 1이다.\n",
    "④ $c=0.5$ 이면, 민감도와 정밀도 차이는 0이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc2481",
   "metadata": {},
   "source": [
    "정답은 ① $c=0$ 이면, 민감도와 특이도 차이는 1이다. 입니다.이 문제는 로지스틱 회귀분석에서 분류의 기준이 되는 **임곗값(Threshold, $c$)**의 변화가 분류 지표인 민감도와 특이도에 어떤 영향을 주는지 묻고 있습니다.상세 해설임곗값 $c$는 \"확률이 얼마 이상일 때 성공(Positive)으로 판정할 것인가\"를 결정하는 기준입니다.$c=0$인 경우의 의미:확률이 0 이상이면 모두 '성공'으로 분류하겠다는 뜻입니다. 즉, 모든 데이터를 Positive로 예측하게 됩니다.민감도(Sensitivity): 실제 Positive를 Positive로 맞춘 비율이므로 **1(100%)**이 됩니다.특이도(Specificity): 실제 Negative를 Negative로 맞춘 비율인데, 모두 Positive로 예측했으므로 맞춘 게 하나도 없어 **0(0%)**이 됩니다.따라서 민감도(1) - 특이도(0) = 1이 되어 1번 지문이 정답입니다.$c=1$인 경우 (참고):확률이 1인 경우만 성공으로 보겠다는 뜻이므로, 거의 모든 데이터를 Negative로 예측합니다.이때는 반대로 민감도 = 0, 특이도 = 1이 됩니다.$c=0.5$인 경우:일반적인 분류 기준입니다. 데이터의 분포에 따라 민감도와 특이도는 다양한 값을 가지며, 그 차이가 반드시 1이 되거나 정밀도와 같아진다는 보장은 없습니다.💡 주요 지표 복습 (암기 필수!)민감도(Sensitivity): 실제 Positive 중 Positive로 예측 성공한 비율 (TP / 실제 P)특이도(Specificity): 실제 Negative 중 Negative로 예측 성공한 비율 (TN / 실제 N)정밀도(Precision): Positive라고 예측한 것 중 실제 Positive인 비율 (TP / 예측 P)💡 실전 팁: 임곗값과 지표의 관계임곗값($c$)이 낮아지면: 더 쉽게 Positive라고 판정하므로 민감도는 올라가고 특이도는 내려갑니다.임곗값($c$)이 높아지면: 더 엄격하게 Positive라고 판정하므로 민감도는 내려가고 특이도는 올라갑니다.이러한 민감도와 특이도의 상관관계를 그래프로 그린 것이 바로 ROC 커브이며, 그 아래 면적이 AUC입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77257971",
   "metadata": {},
   "source": [
    "# 62. 혼동행렬에 대한 설명으로 옳지 않은 것은?\n",
    "① 정확도는 $\\frac{TP+TN}{TP+TN+FP+FN}$ 이다.\n",
    "② 정밀도는 $\\frac{TP}{TP+FP}$ 이다.\n",
    "③ F1 스코어는 정밀도와 재현율의 기하평균이다.\n",
    "④ 재현율은 $\\frac{TP}{TP+FN}$ 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4682e61",
   "metadata": {},
   "source": [
    "정답은 ③ F1 스코어는 정밀도와 재현율의 기하평균이다. 입니다.이 문제는 분류 모델의 성능을 평가하는 **혼동 행렬(Confusion Matrix)**의 핵심 지표 공식을 정확히 알고 있는지 묻고 있습니다.상세 해설 및 오답 분석③ F1 스코어는 정밀도와 재현율의 기하평균이다. (X):F1 스코어는 정밀도와 재현율의 **조화평균(Harmonic Mean)**입니다.산술평균이나 기하평균이 아닌 조화평균을 사용하는 이유는 두 지표 중 어느 하나라도 매우 낮을 경우 전체 점수를 낮게 평가하여, 불균형한 모델의 성능을 더 엄격하게 측정하기 위해서입니다.공식: $F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$① 정확도(Accuracy) (O): 전체 데이터($TP+TN+FP+FN$) 중에서 긍정과 부정을 모두 포함해 얼마나 정답을 맞혔는지를 나타냅니다.② 정밀도(Precision) (O): 모델이 **Positive라고 예측($TP+FP$)**한 것들 중에서 실제로 Positive인 비율을 의미합니다.④ 재현율(Recall) (O): **실제로 Positive인 데이터($TP+FN$)**들 중에서 모델이 Positive라고 맞춘 비율을 의미합니다.💡 혼동 행렬 지표 암기 꿀팁지표공식 (분모 중심)직관적 의미정밀도예측한 Positive 중 진짜모델의 정답률 (예측 기준)재현율실제 Positive 중 맞춘 것놓치지 않은 비율 (실제 기준)정확도전체 중 맞춘 것전반적인 성능 지표💡 실전 팁: 조화평균을 쓰는 이유예를 들어 정밀도가 0.9이고 재현율이 0.1인 극단적인 모델이 있을 때:산술평균: $(0.9 + 0.1) / 2 = 0.5$조화평균(F1): $2 \\times (0.9 \\times 0.1) / (0.9 + 0.1) = 0.18$조화평균은 낮은 값에 훨씬 민감하게 반응하므로, 정밀도와 재현율이 모두 높아야만 좋은 F1 스코어를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31ec763",
   "metadata": {},
   "source": [
    "# 63. 변수 10,000개 중 1,000개를 선별한 후 상관관계 분석으로 검증하고자 한다. 모델 테스트 방법으로 옳은 것은? \n",
    "① 변수를 선택하고 상관관계 분석, 검정 후 데이터를 분할한다. ② 데이터를 분할하고 상관관계 분석 후 변수를 선택한다. ③ 모델의 학습능력을 상관관계 분석으로 확인 후 데이터를 분할한다. ④ 무작위로 데이터를 분할한 후 상관관계 분석으로 종속변수와의 관계를 검정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d357276",
   "metadata": {},
   "source": [
    "정답은 ④ 무작위로 데이터를 분할한 후 상관관계 분석으로 종속변수와의 관계를 검정한다. 입니다.이 문제는 데이터 분석 프로세스에서 **'데이터 분할(Data Splitting)'**과 **'변수 선택(Feature Selection)'**의 올바른 순서를 묻고 있습니다.상세 해설데이터 분석에서 가장 경계해야 할 것 중 하나가 **데이터 누수(Data Leakage)**입니다.왜 데이터를 먼저 분할해야 하는가?전체 데이터를 대상으로 상관관계 분석을 하거나 변수를 선택하면, 나중에 평가용으로 쓸 테스트 데이터(Test Data)의 정보가 변수 선택 과정에 미리 반영되어 버립니다. 이렇게 되면 모델이 평가 데이터에 대해 '답안지를 미리 본' 셈이 되어 성능이 부풀려지는 현상이 발생합니다.올바른 절차:먼저 데이터를 **훈련용(Train)**과 **테스트용(Test)**으로 무작위 분할합니다.훈련용 데이터만을 사용하여 상관관계 분석을 수행하고 중요한 변수를 선택합니다.선택된 변수로 모델을 학습시킨 뒤, 한 번도 보지 않은 테스트용 데이터로 모델을 검증합니다.💡 오답 분석①, ③: 데이터를 분할하기 전(Before Splitting)에 상관관계 분석이나 검정을 수행하는 것은 데이터 누수를 유발하여 신뢰할 수 없는 결과를 낳습니다.②: 데이터 분할 후 상관관계 분석을 하는 방향은 맞으나, 변수 선택 프로세스의 목적은 **'종속변수와의 관계'**를 보고 모델을 구축하는 것이므로, ④번이 절차와 목적을 가장 정확하게 설명하고 있습니다.💡 실전 팁: 데이터 분할의 종류분할 방식설명Hold-out데이터를 단순히 Train(70~80%)과 Test(20~30%)로 나눔K-Fold CV데이터를 K개로 나누어 번갈아가며 학습과 검증을 반복 (데이터가 적을 때 유용)Stratified Split종속변수의 클래스 비율을 유지하면서 분할 (분류 분석 시 필수)💡 핵심 키워드데이터 누수(Data Leakage): 훈련 데이터 이외의 정보가 모델 학습에 사용되는 현상.일반화 성능(Generalization): 학습 시 보지 못한 데이터에 대해 모델이 얼마나 잘 예측하는지를 나타내는 능력."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1b831",
   "metadata": {},
   "source": [
    "# 64. k-fold Cross Validation에 대한 설명으로 옳지 않은 것은? \n",
    "① 데이터의 양이 적을 때 유리하다. ② 학습 데이터와 검증 데이터를 서로 다르게 지정하여 k번 반복 ③ 모델 각각의 성능에 대한 평균을 도출 ④ (잘린 텍스트 보충: 훈련 데이터의 중복 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43afbff2",
   "metadata": {},
   "source": [
    "정답은 ④ 훈련 데이터의 중복 사용 입니다. (보통 \"모든 데이터를 훈련과 테스트에 동시에 중복 사용한다\"는 식으로 오답 지문이 구성됩니다.)**K-폴드 교차 검증(K-fold Cross Validation)**의 핵심 원리는 데이터를 중복시키는 것이 아니라, 데이터를 겹치지 않게 나누어 모든 데이터를 한 번씩은 검증(Validation)에 참여시키는 것입니다.상세 해설 및 오답 분석④ 훈련 데이터의 중복 사용 (X):K-폴드 방식에서는 데이터를 $k$개의 그룹(Fold)으로 나눈 뒤, 한 번의 시행마다 한 개의 그룹은 반드시 검증용으로, 나머지 $k-1$개는 학습용으로 사용합니다. 즉, 특정 회차에서 학습에 쓰인 데이터가 동시에 검증에 쓰이는 '중복'은 발생하지 않습니다. 다만, 전체 프로세스($k$번의 반복)를 통틀어 보면 모든 데이터가 한 번씩 검증에 사용되는 구조입니다.① 데이터의 양이 적을 때 유리하다. (O):데이터가 적은데 훈련/테스트용으로만 딱 나누면 학습량이 부족해질 수 있습니다. K-폴드는 모든 데이터를 학습과 검증에 골고루 활용하므로 데이터 부족 문제를 완화합니다.② 학습 데이터와 검증 데이터를 서로 다르게 지정하여 k번 반복 (O):매 반복마다 검증용 폴드를 바꿔가며 총 $k$번의 학습과 평가를 진행합니다.③ 모델 각각의 성능에 대한 평균을 도출 (O):$k$번의 평가 결과(예: 정확도)가 나오면 그 값들의 산술 평균을 내어 해당 모델의 최종 성능으로 간주합니다. 이를 통해 특정 데이터 셋에만 과적합되는 것을 방지합니다.💡 K-폴드 교차 검증의 절차전체 데이터를 $k$개의 동일한 크기를 가진 폴드(Fold)로 나눕니다.첫 번째 폴드를 검증 데이터로, 나머지 $k-1$개 폴드를 학습 데이터로 사용하여 성능을 측정합니다.이 과정을 $k$번 반복하되, 매번 검증 폴드를 바꿉니다.$k$개의 성능 지표를 평균내어 최종 성능을 확인합니다.💡 실전 팁: Stratified K-fold만약 종속변수가 범주형인데 클래스 비율이 불균형하다면(예: 암 환자 5%, 정상 95%), 단순히 나누지 않고 각 폴드마다 원본 데이터의 클래스 비율을 유지하며 나누는 **층화 K-폴드(Stratified K-fold)**를 사용해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc972b5",
   "metadata": {},
   "source": [
    "# 65. 아래 보기에 대한 설명으로 옳은 것은? \n",
    "[두 명의 데이터 분석가 A와 B가 있다. A와 B는 각자의 방식으로 데이터를 분석하기로 했으며, 분석 시 10개의 파라미터를 동일하게 설정하기로 결정했다.] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed36a20",
   "metadata": {},
   "source": [
    "① LOOCV ② 5-fold CV ③ Bootstrap ④ Stratified K-fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb248d",
   "metadata": {},
   "source": [
    "정답은 ③ Bootstrap (부트스트랩) 입니다.이 문제는 데이터 분석 과정에서 '재표본 추출(Resampling)' 기법 중 하나인 부트스트랩의 개념을 묻고 있습니다.상세 해설③ Bootstrap (부트스트랩) (정답):정의: 주어진 데이터셋에서 중복을 허용하여(복원 추출) 무작위로 $n$개의 샘플을 추출하는 방법입니다.특징: 표본의 수가 적을 때 통계적 추정의 신뢰도를 높이기 위해 사용됩니다. 문제의 상황처럼 두 분석가(A, B)가 동일한 파라미터를 설정하더라도, 각자 무작위로 복원 추출된 서로 다른 데이터셋을 가지고 분석을 진행하여 모델의 안정성을 평가하거나 성능을 평균 내는 방식에 적합합니다.배깅(Bagging): 부트스트랩을 활용한 대표적인 모델이 바로 '랜덤 포레스트'와 같은 배깅 기법입니다.오답 분석① LOOCV (Leave-One-Out Cross Validation):전체 $n$개의 데이터 중 단 1개만을 검증 데이터로 쓰고 나머지 $n-1$개를 학습 데이터로 사용하는 방식입니다. 이를 $n$번 반복합니다. 데이터가 아주 적을 때 쓰지만 계산량이 너무 많습니다.② 5-fold CV (5-폴드 교차 검증):데이터를 5개의 그룹으로 나누어 1개를 검증용, 4개를 학습용으로 번갈아 사용하는 방식입니다. 데이터가 '중복'되지 않게 나누는 것이 핵심이므로, 부트스트랩과는 차이가 있습니다.④ Stratified K-fold CV (층화 K-폴드 교차 검증):종속변수의 **클래스 비율(예: 암 환자 비율)**을 유지하면서 K-폴드를 수행하는 방식입니다.💡 핵심 요약: 부트스트랩(Bootstrap)복원 추출: 한 번 뽑힌 데이터가 다음 추출 시에도 또 뽑힐 수 있습니다.63.2% 법칙: 부트스트랩 샘플링 시 한 번도 선택되지 않는 데이터(Out-of-Bag, OOB)는 약 36.8% 정도 발생하며, 이를 검증용 데이터로 사용하기도 합니다.앙상블의 기초: 여러 모델을 병렬로 학습시키는 배깅(Bagging) 기법의 근간이 됩니다.💡 실전 팁시험에서 \"복원 추출\" 혹은 **\"중복 허용\"**이라는 단어가 보이면 주저 없이 부트스트랩을 선택하세요. 반대로 **\"중복 없이 나누어 교대로 검증\"**한다는 말이 나오면 **교차 검증(K-fold)**이 정답입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4186fc6",
   "metadata": {},
   "source": [
    "# 66. 콜모고로프-스미르노프 검정(K-S) 통계량에 대한 설명으로 옳은 것을 모두 고르시오. \n",
    "가. 서로 다른 두 개의 집단이 동일한 분포를 이루고 있는지를 검증하는 지표 나. 비교하는 두 개의 집단 사이의 최대 거리 다. 누적분포함수(Cumulative Distribution Function, CDF)와 경험적 누적분포함수(Experience CDF 혹은 ECDF)를 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f03afc5",
   "metadata": {},
   "source": [
    "① 가 ② 가, 나 ③ 나, 다 ④ 가, 나, 다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e27e08",
   "metadata": {},
   "source": [
    "정답은 ④ 가, 나, 다 입니다.**콜모고로프-스미르노프 검정(Kolmogorov-Smirnov Test, K-S 검정)**은 데이터가 특정 분포를 따르는지(일표본), 혹은 두 집단의 분포가 서로 같은지(이표본)를 확인하는 비모수 검정 방법입니다.상세 해설가. 서로 다른 두 개의 집단이 동일한 분포를 이루고 있는지를 검증하는 지표 (O):두 표본 집단에서 추출한 데이터들이 통계적으로 같은 분포(분포의 모양, 위치 등)에서 나왔는지를 판별하는 데 사용됩니다.나. 비교하는 두 개의 집단 사이의 최대 거리 (O):K-S 통계량($D$)은 두 누적분포함수 사이의 수직 거리 중 최댓값을 의미합니다. 이 거리가 작을수록 두 집단은 동일한 분포일 가능성이 높고, 거리가 커질수록 서로 다른 분포일 확률이 높습니다.다. 누적분포함수(CDF)와 경험적 누적분포함수(ECDF)를 사용 (O):이론적인 누적분포함수(CDF)와 실제 관측 데이터로 만든 경험적 누적분포함수(ECDF)를 비교하거나, 두 개의 ECDF를 서로 비교하여 차이를 계산합니다.💡 K-S 검정의 특징 및 장점비모수적 방법: 데이터가 정규분포를 따라야 한다는 전제 조건이 없어도 사용할 수 있습니다.분포 전체 비교: 평균이나 분산 같은 특정 통계량만 비교하는 것이 아니라, 분포의 전체적인 형태를 비교합니다.검정 목적:일표본: \"이 데이터가 정규분포를 따르는가?\" (정규성 검정)이표본: \"A 집단과 B 집단의 분포가 같은가?\"💡 실전 대비: 정규성 검정 기법 비교데이터가 정규분포를 따르는지 확인할 때 K-S 검정 외에도 자주 나오는 기법들입니다.기법특징Shapiro-Wilk Test표본 수가 적을 때(대략 50개 미만) 주로 사용하며, 정규성 검정에 가장 강력함.K-S Test표본 수가 많을 때 유리하며, 특정 분포와의 적합도를 검정함.Q-Q Plot시각적으로 점들이 직선상에 놓이는지 확인하여 판단함.💡 핵심 키워드$D$ (Distance): K-S 검정의 통계량으로, 두 함수 사이의 최대 수직 거리를 뜻함.적합도 검정: 실제 관측치가 가설로 설정한 이론적 분포와 얼마나 잘 맞는지를 검사함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7f2e4",
   "metadata": {},
   "source": [
    "# 67. 다음 중 적합도 검정에 대한 설명으로 옳지 않은 것은? \n",
    "① 모델 전체의 적합성은 검정 불필요하다. ② 적합도 검정(Goodness-of-Fit Test)은 이 데이터가 특정 이론적 분포와 일치하는지를 결정하는 방법이다. ③ 정규분포를 가진 시 정규성 검정을 가장 많이 활용하고 있다. ④ 정규분포를 가정하지 않을 시 카이제곱 검정을 이용해 적합도를 판단한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec28222",
   "metadata": {},
   "source": [
    "정답은 ① 모델 전체의 적합성은 검정 불필요하다. 입니다.**적합도 검정(Goodness-of-Fit Test)**은 수집한 데이터가 우리가 가정한 통계적 모델이나 이론적 분포에 얼마나 잘 들어맞는지를 확인하는 필수적인 과정입니다.상세 해설 및 오답 분석① 모델 전체의 적합성은 검정 불필요하다. (X):분석을 수행하기 전이나 후에는 반드시 모델이 데이터를 얼마나 잘 설명하는지, 설정한 가정이 타당한지 검정해야 합니다. 예를 들어 회귀분석에서는 F-검정을 통해 모델 전체의 유의성을 검정하고, 결정계수($R^2$)를 통해 적합도를 확인합니다. 이를 생략하면 분석 결과의 신뢰도를 보장할 수 없습니다.② 특정 이론적 분포와 일치하는지를 결정하는 방법이다. (O):적합도 검정의 핵심 정의입니다. 관측된 빈도가 기대 빈도(이론적 분포)와 통계적으로 유의미하게 차이가 나는지 확인합니다.③ 정규성 검정을 가장 많이 활용하고 있다. (O):많은 통계적 기법(t-검정, ANOVA, 선형회귀 등)이 '데이터가 정규분포를 따른다'는 가정을 전제로 합니다. 따라서 데이터가 정규성을 만족하는지 확인하는 정규성 적합도 검정이 가장 빈번하게 수행됩니다.④ 정규분포를 가정하지 않을 시 카이제곱 검정을 이용한다. (O):데이터가 범주형이거나 특정 분포를 가정하기 어려울 때, 관측값과 기대값의 차이를 이용하는 카이제곱($\\chi^2$) 적합도 검정을 사용합니다.💡 주요 적합도 검정 방법 요약검정 종류주요 특징 및 용도카이제곱($\\chi^2$) 검정범주형 데이터에서 관측 빈도와 기대 빈도의 차이를 검정 (가장 일반적)샤피로-윌크 (Shapiro-Wilk)소표본 데이터의 정규성을 검정하는 가장 강력한 방법콜모고로프-스미르노프 (K-S)데이터의 누적분포함수를 이용하여 특정 분포와의 일치 여부 검정Q-Q Plot데이터의 분포와 이론적 분포를 시각적으로 비교하는 방법💡 실전 팁귀무가설($H_0$): \"데이터의 분포와 이론적 분포가 일치한다.\"대립가설($H_1$): \"데이터의 분포와 이론적 분포가 일치하지 않는다.\"따라서 적합도 검정이나 정규성 검정에서는 p-value가 유의수준(0.05)보다 커서 귀무가설을 채택해야 \"문제가 없다(정규성을 만족한다/적합하다)\"라고 판단합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c1b661",
   "metadata": {},
   "source": [
    "# 68. 데이터 분할(Split)에 대한 설명으로 옳지 않은 것은? \n",
    "① 학습 데이터(훈련 데이터)보다 검증 데이터에서 성능이 좋은 하이퍼파라미터를 선정한다. ② 테스트 데이터로 모델 간 성능을 비교한다. ③ 학습 데이터로 학습한다. ④ 데이터셋을 학습 데이터, 검증 데이터, 테스트 데이터로 나누는 것이 적합하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519d928",
   "metadata": {},
   "source": [
    "정답은 ① 학습 데이터(훈련 데이터)보다 검증 데이터에서 성능이 좋은 하이퍼파라미터를 선정한다. 입니다.이 문장은 표현이 다소 모호할 수 있으나, 모델링의 목적에 비추어 보면 틀린 설명입니다. 모델의 파라미터를 결정하는 기준은 **'학습 데이터와의 비교'**가 아니라, **'검증 데이터에서 가장 높은 성능을 내는 지점'**을 찾는 것이기 때문입니다.상세 해설 및 오답 분석① 학습 데이터보다 검증 데이터에서 성능이 좋은 하이퍼파라미터를 선정한다. (X):일반적으로 모델은 학습 데이터에 최적화되므로 학습 데이터에서의 성능이 검증 데이터보다 높게 나옵니다. 우리가 하이퍼파라미터를 튜닝하는 목적은 **\"검증 데이터(Validation Data)에서의 성능을 최대화\"**하는 파라미터를 찾는 것이지, 두 데이터셋 간의 성능을 비교하여 한쪽이 더 높은 것을 고르는 것이 아닙니다.② 테스트 데이터로 모델 간 성능을 비교한다. (O):모든 훈련과 튜닝이 끝난 후, 한 번도 보지 않은 **테스트 데이터(Test Data)**를 사용하여 최종 모델들의 성능을 객관적으로 비교하고 최종 모델을 확정합니다.③ 학습 데이터로 학습한다. (O):모델의 가중치($w, b$)를 업데이트하는 데 사용되는 순수 학습용 데이터셋입니다.④ 학습, 검증, 테스트 데이터로 나누는 것이 적합하다. (O):데이터를 세 그룹으로 나누는 것이 가장 표준적인 방법입니다.학습(Train): 모델 가중치 학습검증(Validation): 하이퍼파라미터 튜닝 및 모델 선택테스트(Test): 최종 모델의 일반화 성능 평가💡 데이터 분할의 핵심 프로세스Train Set으로 모델을 여러 개 만듭니다 (예: 깊이가 3인 트리, 5인 트리 등).Validation Set에 각 모델을 적용해보고 가장 성적이 좋은 하이퍼파라미터(예: 깊이 5)를 고릅니다.선택된 최적의 모델을 Test Set으로 딱 한 번 평가하여 실제 현장에 나갔을 때의 성능을 예측합니다.💡 실전 팁: 과적합(Overfitting) 판단학습 데이터 성능은 높은데 검증 데이터 성능이 낮다면? → 과적합 발생!학습 데이터 성능도 낮고 검증 데이터 성능도 낮다면? → 과소적합(Underfitting) 발생!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753e933",
   "metadata": {},
   "source": [
    "# 69. 다음 중 하이퍼파라미터 대한 설명으로 옳지 않은 것은? \n",
    "① 파라미터와 하이퍼파라미터는 학습 전에 정한다. ② 하이퍼파라미터는 학습 과정 자체를 제어한다. ③ 하이퍼파라미터는 사람이 직접 입력값을 설정해줘야 한다. ④ 서로 다른 하이퍼파라미터값은 모델 학습과 정확도(Accuracy) 혹은 모델의 수렴율(Convergence Rate)에 영향을 미칠 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bacd21",
   "metadata": {},
   "source": [
    "정답은 ① 파라미터와 하이퍼파라미터는 학습 전에 정한다. 입니다.이 문제는 모델이 스스로 찾아내는 **파라미터(Parameter)**와 분석가가 직접 설정하는 **하이퍼파라미터(Hyperparameter)**의 차이점을 명확히 구분할 수 있는지 묻고 있습니다.상세 해설 및 오답 분석① 파라미터와 하이퍼파라미터는 학습 전에 정한다. (X):하이퍼파라미터는 학습을 시작하기 전에 사람이 미리 설정하는 값이 맞습니다.하지만 **파라미터(매개변수)**는 학습 전에는 알 수 없으며, **학습 과정(Training)**을 통해 데이터로부터 모델이 스스로 최적의 값을 찾아내는(추정하는) 값입니다.② 하이퍼파라미터는 학습 과정 자체를 제어한다. (O):학습률(Learning Rate)이나 반복 횟수(Epoch), 신경망의 층 수 등을 조절하여 모델이 어떻게 학습될지를 결정하고 제어합니다.③ 하이퍼파라미터는 사람이 직접 입력값을 설정해줘야 한다. (O):모델이 데이터로부터 자동으로 학습할 수 없는 영역이기에, 데이터 분석가가 경험이나 튜닝(Grid Search 등)을 통해 직접 값을 지정해야 합니다.④ 모델 학습과 정확도 혹은 수렴율에 영향을 미칠 수 있다. (O):적절한 하이퍼파라미터를 설정하지 못하면 모델이 최적값에 도달하지 못해 성능(정확도)이 낮아지거나, 학습 속도(수렴율)가 너무 느려질 수 있습니다.💡 파라미터 vs 하이퍼파라미터 비교구분파라미터 (Parameter)하이퍼파라미터 (Hyperparameter)결정 주체**모델(알고리즘)**이 학습을 통해 결정**분석가(사람)**가 직접 설정결정 시점학습 도중에 계속 업데이트됨학습 시작 전에 완료주요 예시회귀계수($\\beta$), 인공신경망의 가중치($w$)학습률($\\eta$), 트리 깊이, K-최근접 이웃의 $k$💡 실전 팁: 하이퍼파라미터 튜닝어떤 하이퍼파라미터 값이 최적인지 찾기 위해 사용하는 방법들입니다.Grid Search: 가능한 조합을 모두 시도해 보는 방법.Random Search: 무작위로 조합을 선택해 시도해 보는 방법.Bayesian Optimization: 이전 시도의 결과를 바탕으로 더 나은 조합을 예측하며 찾는 방법."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7a242",
   "metadata": {},
   "source": [
    "# 70. 하이퍼파라미터 최적화 기법에 대한 설명으로 옳지 않은 것은? \n",
    "① 수동탐색(Manual Search)은 사용자가 사전지식을 가지고 있다. ② 무작위탐색(Random Search)은 다양한 조합들을 시험해 예상치 못한 결과를 얻을 수 있다. ③ 베이지안 최적화(Bayesian Optimization)는 새로운 하이퍼파라미터값에 대한 조사를 수행할 때 사전지식을 충분히 반영한다. ④ 원래 분석가의 경험에 따라 값을 조절하는 게 최적이지만, 자동화를 위해 격자탐색(Grid Search) 등을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eae096",
   "metadata": {},
   "source": [
    "정답은 ④ 원래 분석가의 경험에 따라 값을 조절하는 게 최적이지만, 자동화를 위해 격자탐색(Grid Search) 등을 수행한다. 입니다.이 문제는 하이퍼파라미터 튜닝 방식의 특징과 효율성을 이해하고 있는지 묻고 있습니다.상세 해설 및 오답 분석④ 원래 분석가의 경험에 따라 값을 조절하는 게 최적이지만... (X):분석가의 경험은 중요한 참고 자료가 될 수 있지만, 그것이 반드시 '최적'임을 보장하지는 않습니다. 하이퍼파라미터의 조합은 매우 방대하며 변수 간의 복잡한 상호작용이 존재하기 때문에, 사람이 직관적으로 최적점을 찾는 데에는 한계가 있습니다. 따라서 격자 탐색(Grid Search)이나 베이지안 최적화와 같은 객관적이고 체계적인 자동화 기법을 사용하는 것이 더 정교한 모델을 만드는 데 유리합니다.① 수동탐색(Manual Search) (O):분석가가 모델에 대한 깊은 이해나 과거의 경험(사전지식)을 바탕으로 직접 값을 하나씩 입력하며 결과를 확인하는 방식입니다.② 무작위탐색(Random Search) (O):정해진 범위 내에서 하이퍼파라미터 조합을 무작위로 추출하여 테스트합니다. 격자 탐색보다 효율적일 때가 많으며, 예상치 못한 구간에서 최적의 조합을 발견할 가능성이 있습니다.③ 베이지안 최적화(Bayesian Optimization) (O):이전까지 수행했던 테스트 결과(사전지식)를 바탕으로, 다음에 시도할 하이퍼파라미터가 성능을 얼마나 개선할지 확률적으로 예측하며 탐색하는 가장 고도화된 방식입니다.💡 하이퍼파라미터 최적화 기법 비교기법특징장점단점Grid Search일정한 간격으로 모든 조합 시도빠짐없이 탐색 가능조합이 많으면 시간이 너무 오래 걸림Random Search임의의 지점을 무작위로 선택Grid Search보다 빠르고 효율적운에 따라 결과의 편차가 생길 수 있음Bayesian Opt.과거 결과를 보고 다음 위치 결정가장 효율적이고 똑똑한 탐색구현이 상대적으로 복잡함💡 실전 팁하이퍼파라미터 최적화의 목적은 **\"사람의 주관적 개입을 줄이고 데이터에 근거하여 가장 높은 성능을 내는 조합을 효율적으로 찾는 것\"**입니다. 따라서 \"사람의 경험이 자동화보다 항상 우월하다\"는 식의 보기는 오답일 확률이 매우 높습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b69b35f",
   "metadata": {},
   "source": [
    "# 71. 다음 중 파라미터 최적화 기법이 아닌 것을 고르시오. \n",
    "① RMSProp ② AdaDelta ③ Nadam ④ Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d17c5",
   "metadata": {},
   "source": [
    "정답은 ④ Bayesian Optimization (베이지안 최적화) 입니다.이 문제는 모델이 학습 과정에서 가중치를 업데이트하는 알고리즘인 **'옵티마이저(Optimizer)'**와 모델의 설정값 자체를 찾는 **'하이퍼파라미터 최적화 기법'**을 구분할 수 있는지 묻고 있습니다.상세 해설 및 오답 분석④ Bayesian Optimization (정답):이는 하이퍼파라미터(학습률, 은닉층 수 등)를 최적화하는 기법입니다.이전 70번 문제에서 다루었듯이, 미지의 함수 성능을 예측하여 효율적으로 최적으로 설정값을 찾아가는 '모델 외부'의 튜닝 기법입니다.① RMSProp / ② AdaDelta / ③ Nadam (오답):이들은 모두 **경사하강법(Gradient Descent)**의 변형으로, 학습 과정에서 파라미터(가중치 $w$, 편향 $b$)를 최적화하는 알고리즘(Optimizer)들입니다.RMSProp: 학습률이 급격히 줄어드는 것을 방지하기 위해 지수 이동 평균을 이용합니다.AdaDelta: AdaGrad의 단점을 보완하여 학습률의 변화를 조절합니다.Nadam: RMSProp에 모멘텀(Momentum) 기법인 Nesterov Accelerated Gradient(NAG)를 결합한 방식입니다.💡 파라미터 최적화(Optimizer)의 계보딥러닝이나 인공신경망 학습에서 가중치를 업데이트하는 방식의 발전 과정입니다.SGD (Stochastic Gradient Descent): 가장 기본적인 확률적 경사하강법Momentum: 관성(가던 방향으로 더 가려는 성질)을 추가AdaGrad: 자주 등장하는 변수는 작게, 드문 변수는 크게 업데이트RMSProp: AdaGrad의 학습 정지 문제를 해결Adam: Momentum과 RMSProp을 결합 (가장 널리 사용됨)Nadam: Adam에 Nesterov 모멘텀을 추가💡 실전 팁파라미터(Parameter) 최적화: 학습 중에 컴퓨터가 가중치($w$)를 바꾸는 것 → Adam, SGD, RMSProp 등하이퍼파라미터(Hyperparameter) 최적화: 학습 전에 사람이(또는 자동화 도구가) 설정값을 바꾸는 것 → Grid Search, Random Search, Bayesian Optimization 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce47359",
   "metadata": {},
   "source": [
    "# 72. 랜덤포레스트 기법에 대한 설명으로 옳지 않은 것은? \n",
    "① 각각의 트리는 과적합 가능성이 있다. ② 다수의 결정트리의 결과를 통해 최종결과를 도출한다. ③ 각각의 결정나무에 배깅과 같은 방법을 사용한다. ④ 전체 데이터셋으로 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255709e",
   "metadata": {},
   "source": [
    "정답은 ④ 전체 데이터셋으로 학습한다. 입니다.\n",
    "\n",
    "랜덤 포레스트(Random Forest)의 핵심은 **'무작위성'**을 통해 모델의 다양성을 확보하고 과적합을 방지하는 것입니다.\n",
    "\n",
    "상세 해설 및 오답 분석\n",
    "④ 전체 데이터셋으로 학습한다. (X):\n",
    "\n",
    "랜덤 포레스트는 모든 데이터를 한꺼번에 사용하는 것이 아니라, 부트스트랩(Bootstrap) 방식을 사용합니다.\n",
    "\n",
    "즉, 전체 데이터에서 중복을 허용하여 무작위로 추출된 일부 샘플 데이터셋들로 여러 개의 의사결정나무를 만듭니다.\n",
    "\n",
    "또한, 나무를 만들 때 사용하는 변수(Feature) 또한 무작위로 일부만 선택하여 사용합니다.\n",
    "\n",
    "① 각각의 트리는 과적합 가능성이 있다. (O):\n",
    "\n",
    "개별 의사결정나무는 깊이가 깊어질수록 학습 데이터에 과하게 맞춰지는 과적합 성향이 강합니다. 랜덤 포레스트는 이런 나무들을 여러 개 합쳐서 그 단점을 보완하는 모델입니다.\n",
    "\n",
    "② 다수의 결정트리의 결과를 통해 최종결과를 도출한다. (O):\n",
    "\n",
    "여러 개의 나무가 내놓은 결과들을 모아 투표(Voting, 분류 시) 혹은 **평균(Average, 회귀 시)**을 통해 최종 결론을 내리는 앙상블 기법입니다.\n",
    "\n",
    "③ 각각의 결정나무에 배깅과 같은 방법을 사용한다. (O):\n",
    "\n",
    "배깅(Bagging)은 Bootstrap Aggregating의 약자로, 부트스트랩 샘플링을 통해 여러 모델을 병렬로 학습시키는 기법입니다. 랜덤 포레스트는 대표적인 배깅 알고리즘입니다.\n",
    "\n",
    "💡 랜덤 포레스트의 두 가지 무작위성\n",
    "랜덤 포레스트가 강력한 이유는 다음 두 가지를 무작위로 섞기 때문입니다.\n",
    "\n",
    "데이터의 무작위성 (Row Sampling): 부트스트랩을 통해 각 나무가 서로 다른 데이터 셋을 학습합니다.\n",
    "\n",
    "변수의 무작위성 (Column Sampling): 각 분기점(Node)마다 모든 변수를 검토하는 것이 아니라, 무작위로 선택된 일부 변수 중에서 최적의 분기 기준을 찾습니다.\n",
    "\n",
    "💡 실전 팁\n",
    "배깅(Bagging): 랜덤 포레스트가 대표적이며, 여러 모델을 독립적으로 학습시켜 평균을 냅니다. (분산 감소 효과)\n",
    "\n",
    "부스팅(Boosting): XGBoost, LightGBM 등이 대표적이며, 이전 모델의 오차를 다음 모델이 보완하며 순차적으로 학습합니다. (편향 감소 효과)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec150f",
   "metadata": {},
   "source": [
    "# 73. 부스팅(Boosting)에 대한 설명으로 옳지 않은 것은? \n",
    "① 약학습기(Weak Learner)로 만든다. ② 병렬로 학습한다. ③ 오답에 대한 가중치를 부여해 사용한다. ④ 보팅(Voting)과 함께 앙상블 학습 유형으로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55158456",
   "metadata": {},
   "source": [
    "정답은 ② 병렬로 학습한다. 입니다.이 문제는 앙상블 학습의 두 축인 **배깅(Bagging)**과 **부스팅(Boosting)**의 차이점을 정확히 구분할 수 있는지 묻고 있습니다.상세 해설 및 오답 분석② 병렬로 학습한다. (X):부스팅은 모델을 한꺼번에 만드는 것이 아니라, 순차적(Sequential)으로 학습합니다.첫 번째 모델이 학습한 후, 그 모델이 틀린 부분(오차)에 가중치를 두어 두 번째 모델이 학습하는 방식입니다. 따라서 이전 모델의 결과가 다음 모델에 영향을 미치므로 병렬 학습이 불가능합니다.참고: 병렬 학습은 랜덤 포레스트와 같은 **배깅(Bagging)**의 특징입니다.① 약학습기(Weak Learner)로 만든다. (O):성능이 아주 뛰어나지는 않은 단순한 모델(약학습기)들을 여러 개 결합하여 강력한 성능을 내는 모델을 만드는 것이 부스팅의 철학입니다.③ 오답에 대한 가중치를 부여해 사용한다. (O):부스팅의 핵심 원리입니다. 이전 단계의 모델이 잘못 예측한 데이터에 대해 높은 가중치를 부여하여, 다음 모델은 그 어려운 문제를 더 잘 풀도록 집중 학습 시킵니다.④ 보팅(Voting)과 함께 앙상블 학습 유형으로 사용된다. (O):앙상블 학습은 크게 **보팅(Voting), 배깅(Bagging), 부스팅(Boosting)**으로 나뉩니다. 부스팅은 이 중 가장 고도화된 기법 중 하나입니다.💡 배깅(Bagging) vs 부스팅(Boosting) 한눈에 비교구분배깅 (Bagging)부스팅 (Boosting)학습 방식병렬적 (독립적 학습)순차적 (이전 모델 보완)데이터 샘플링부트스트랩 (중복 허용 샘플링)오답에 가중치 부여목적분산(Variance) 감소 (과적합 방지)편향(Bias) 감소 (정확도 향상)대표 알고리즘랜덤 포레스트AdaBoost, XGBoost, LightGBM💡 실전 팁순차적(Sequential) + 오답 강조 = 부스팅병렬적(Parallel) + 독립적 = 배깅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d6ccd",
   "metadata": {},
   "source": [
    "# 74. 데이터 분석 비즈니스 효과에 대한 설명으로 옳지 않은 것은? \n",
    "① 비즈니스 기여도는 데이터 분석결과를 활용하여 사업수행 혹은 과제수행 등을 통해 얻게 되는 긍정적인 영향을 의미한다. ② 비즈니스 효과에 대한 정량적 가치 측정을 위해 직접 지표를 개발하였다. ③ 경영환경에 따라 비즈니스 효과 지표는 유연하게 변화한다. ④ 데이터의 가치(Value)는 투자요소, 속도와 규모는 비즈니스 효과를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c4d3e",
   "metadata": {},
   "source": [
    "정답은 ④ 데이터의 가치(Value)는 투자요소, 속도와 규모는 비즈니스 효과를 나타낸다. 입니다.이 문제는 빅데이터의 특징인 3V(Volume, Velocity, Variety) 및 가치(Value)와 비즈니스 분석의 ROI(투자 자본 수익률) 관점을 올바르게 구분할 수 있는지 묻고 있습니다.상세 해설 및 오답 분석④ 데이터의 가치(Value)는 투자요소... 나타낸다. (X):일반적으로 데이터 분석의 ROI 관점에서 **크기(Volume), 속도(Velocity), 다양성(Variety)**은 데이터를 처리하기 위해 필요한 인프라, 기술, 인력 등 투자 비용(Investment Cost) 요소에 해당합니다.반면, **가치(Value)**는 분석을 통해 얻게 되는 수익 증대, 비용 절감 등 **비즈니스 효과(Business Effect)**를 나타냅니다. 지문의 설명은 두 개념이 서로 뒤바뀌어 있습니다.① 비즈니스 기여도의 정의 (O):분석 결과를 실제 비즈니스 의사결정에 활용하여 매출 증가, 고객 만족도 향상, 공정 효율화 등 기업에 가져다주는 유익한 결과를 의미합니다.② 정량적 가치 측정을 위한 지표 개발 (O):분석의 성과를 객관적으로 증명하기 위해서는 ROI, 증분 매출(Incremental Revenue), 비용 절감액 등 수치로 나타낼 수 있는 직접적인 지표를 개발하여 관리해야 합니다.③ 지표의 유연한 변화 (O):기업의 전략적 목표(KPI)나 시장 상황, 경영 환경이 변하면 그에 따라 데이터 분석이 집중해야 할 영역과 평가지표도 그에 맞춰 유연하게 수정되어야 합니다.💡 데이터 분석의 ROI 요소 (비용 vs 효과)구분주요 요소세부 항목투자 요소 (Cost)Volume, Velocity, Variety데이터 확보, 저장소 구축, 분석 알고리즘 개발 비용효과 요소 (Value)Value매출 증대, 비용 절감, 리스크 감소, 신규 비즈니스 창출💡 실전 팁빅데이터의 3V를 비즈니스 성과와 연결할 때, **'데이터가 많고 빠르고 다양해질수록 분석을 위한 비용(투자)은 늘어난다'**는 점을 기억하세요. 이를 통해 얻어낸 **'가치(Value)'**가 그 투자 비용보다 커야 의미 있는 분석 과제가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dad588",
   "metadata": {},
   "source": [
    "# 75. 시간시각화에 대한 설명으로 옳지 않은 것은? \n",
    "① 점그래프는 점의 집중도를 통해 표현하므로 시계열에 적합하지 않다. ② 영역차트는 선그래프와 그래프 축 사이의 면적으로 데이터를 표시한 그래프이다. ③ 막대그래프는 범주의 수가 7개 이하일 때 주로 사용한다. ④ 꺾은선그래프는 점그래프를 선으로 이은 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81847f",
   "metadata": {},
   "source": [
    "정답은 ① 점그래프는 점의 집중도를 통해 표현하므로 시계열에 적합하지 않다. 입니다.데이터 시각화에서 **시간 시각화(Time Visualization)**는 시간의 흐름에 따른 데이터의 변화를 표현하는 기법입니다. 각 지문의 내용을 분석해 보겠습니다.상세 해설 및 오답 분석① 점그래프는... 시계열에 적합하지 않다. (X):점그래프(Scatter Plot) 역시 시간축(X축)을 기준으로 데이터를 배치하면 시간 시각화의 한 방법으로 충분히 사용 가능합니다. 특히 데이터 포인트가 불규칙하거나 특정 시점에 발생한 이벤트의 분포를 볼 때 유용합니다. 따라서 \"적합하지 않다\"는 설명은 틀린 것입니다.② 영역차트(Area Chart) (O):선그래프 아래의 면적을 색상이나 패턴으로 채운 그래프입니다. 시간에 따른 양적인 변화의 합계나 점유율의 변화를 보여줄 때 효과적입니다.③ 막대그래프(Bar Chart) (O):비교적 짧은 시간 동안의 변화나 개별 시점 간의 차이를 강조할 때 사용합니다. 시간 흐름의 연속성보다는 각 시점의 값 자체를 강조하며, 범주가 너무 많으면 가독성이 떨어지므로 보통 7개 내외일 때 권장됩니다.④ 꺾은선그래프(Line Chart) (O):시간 시각화의 가장 대표적인 방법입니다. 연속적인 시간의 흐름에 따른 데이터의 **추세(Trend)**를 파악하기 위해 점들을 선으로 연결하여 표현합니다.💡 시간 시각화의 도구들 요약시각화 도구주요 용도특징선 그래프연속적인 추세 파악가장 널리 쓰이는 시계열 시각화막대 그래프시점 간 차이 비교이산적인 시간 데이터에 적합영역 차트누적값의 변화량 강조전체 대비 부분의 비중 변화 표현에 유리계단 그래프값이 변하는 시점 강조데이터의 급격한 변동 확인 시 사용💡 실전 팁시간 시각화 문제를 풀 때는 **'연속성'**과 **'추세'**라는 단어를 기억하세요. 대부분의 시간 시각화 도구는 과거에서 미래로 흐르는 데이터의 패턴을 읽어내는 데 목적이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462333b5",
   "metadata": {},
   "source": [
    "# 76. 다음 중 x축(가로축)과 y축(세로축) 각각에 두 변수값의 순서쌍을 점으로 표현해 두 변수의 관계를 나타낸 그래프는? \n",
    "① 산점도 ② 버블차트 ③ 히스토그램 ④ 플로팅바차트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b67f220",
   "metadata": {},
   "source": [
    "정답은 ① 산점도(Scatter Plot) 입니다.데이터 시각화에서 두 연속형 변수 사이의 관계를 파악할 때 가장 기본적으로 사용하는 그래프입니다.상세 해설 및 오답 분석① 산점도 (정답):정의: 좌표평면 위에 두 변수 $x, y$의 값을 점으로 찍어서 표현하는 방식입니다.특징: 두 변수 사이의 상관관계(양의 상관관계, 음의 상관관계)나 데이터의 분포, 이상치(Outlier)를 한눈에 파악하기에 최적입니다.② 버블차트 (Bubble Chart):산점도와 유사하지만, 점의 위치($x, y$) 외에 **점의 크기(Size)**를 통해 세 번째 변수의 정보를 추가로 전달합니다.③ 히스토그램 (Histogram):연속형 데이터의 **분포(빈도)**를 막대 형태로 나타낸 것입니다. $x$축은 데이터의 구간(Bin)을, $y$축은 해당 구간에 속한 데이터의 개수(Frequency)를 나타냅니다.④ 플로팅바차트 (Floating Bar Chart):막대의 시작점이 0이 아니라 특정 범위(최솟값~최댓값)를 나타내도록 공중에 떠 있는 형태의 막대그래프입니다. 주로 기간이나 범위를 시각화할 때 사용합니다.💡 관계 시각화 도구 요약시각화 도구주요 용도특징산점도두 변수 간의 관계 파악상관관계, 군집, 이상치 확인버블차트세 변수 간의 관계 파악점의 크기로 양적인 정보 추가히트맵데이터 밀도 및 상관계수 시각화색상의 진하기로 값의 크기 표현💡 실전 팁문제에서 \"두 변수의 순서쌍\", \"점으로 표현\", **\"관계\"**라는 키워드가 조합되어 나오면 정답은 거의 100% 산점도입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335082f5",
   "metadata": {},
   "source": [
    "# 77. 비교시각화 유형에 대한 설명으로 옳지 않은 것은? \n",
    "① 평행좌표 그래프는 각 행을 변수별로 선을 매핑시켜 나타냈다. ② 히트맵은 색상을 부여하는 방법이다. ③ 체르노프 페이스는 데이터들을 얼굴로 표현한 방법이다. ④ 스타차트는 수치를 별의 개수로 표현하는 시각화 유형이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdfdfdf",
   "metadata": {},
   "source": [
    "정답은 ④ 스타차트는 수치를 별의 개수로 표현하는 시각화 유형이다. 입니다.이 문제는 여러 대상을 다양한 변수(다변량)로 비교할 때 사용하는 비교 시각화(Comparison Visualization) 기법들의 정확한 정의를 묻고 있습니다.상세 해설 및 오답 분석④ 스타차트는 수치를 별의 개수로 표현한다. (X):**스타차트(Star Chart)**는 별의 개수가 아니라, 중심점에서 뻗어 나가는 여러 개의 축(변수)에 데이터 값을 점으로 찍고 이를 선으로 연결하여 별 모양의 도형을 만드는 방식입니다.흔히 게임이나 역량 평가에서 보는 **방사형 차트(Radar Chart)**와 같은 개념입니다. 도형의 면적이 넓을수록 전체적인 수치가 높음을 의미합니다.① 평행좌표 그래프(Parallel Coordinates) (O):여러 개의 수직축을 평행하게 세우고, 각 축에 변수값을 표시한 뒤 데이터 행(Row) 하나를 선으로 연결하여 표현합니다. 다변량 데이터의 패턴을 파악하기에 매우 유용합니다.② 히트맵(Heat Map) (O):데이터 값을 칸별로 배치하고, 그 수치의 크기에 따라 색상의 농도나 종류를 다르게 하여 표현합니다. 많은 양의 데이터를 한눈에 비교할 때 효과적입니다.③ 체르노프 페이스(Chernoff Faces) (O):통계학자 허먼 체르노프가 고안한 방법으로, 데이터의 수치를 **사람의 얼굴 구성 요소(눈의 크기, 입의 모양, 얼굴의 너비 등)**에 매핑하여 표현합니다. 인간이 얼굴의 미세한 변화를 잘 감지한다는 특성을 이용한 독특한 시각화 기법입니다.💡 비교 시각화 도구 요약시각화 도구핵심 원리용도스타 차트방사형 축의 연결개별 대상의 다각도 역량 비교평행좌표평행한 축 사이의 선변수 간의 관계 및 군집 패턴 파악히트맵칸별 색상 매핑대규모 데이터의 상대적 크기 비교체르노프 페이스얼굴 표정/모양 변환다차원 데이터를 직관적으로 구분💡 실전 팁비교 시각화 문제에서 \"별의 개수\", \"막대의 두께\" 등 실제 정의와 다른 묘사가 나오면 정답일 확률이 높습니다. 특히 스타차트 = 방사형 차트라는 점을 꼭 기억해 두세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d02e4",
   "metadata": {},
   "source": [
    "# 78. 다음 중 인포그래픽에 대한 설명으로 옳지 않은 것은? \n",
    "① 일부 인포그래픽은 특정 프로그램에서만 작동해, 보기 위해서는 전문 소프트웨어가 필요하다. ② 인포그래픽은 소프트웨어를 무조건 설치해야 한다. ③ 정보와 시각적 그래프의 합성어이다. ④ 그림 전체적으로 의미하는 바가 있어, 부분 전달이나 일부를 잘라서 적용할 경우 오해가 있을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23357cd",
   "metadata": {},
   "source": [
    "정답은 ② 인포그래픽은 소프트웨어를 무조건 설치해야 한다. 입니다.\n",
    "\n",
    "이 문제는 **인포그래픽(Infographics)**의 정의와 특징, 그리고 대중적인 활용 방안에 대해 묻고 있습니다.\n",
    "\n",
    "상세 해설 및 오답 분석\n",
    "② 인포그래픽은 소프트웨어를 무조건 설치해야 한다. (X):\n",
    "\n",
    "인포그래픽은 정보를 직관적으로 전달하기 위해 이미지, 차트, 텍스트 등을 결합한 시각 결과물입니다.\n",
    "\n",
    "일반적으로 이미지 파일(JPG, PNG), PDF, 혹은 웹 페이지 형태로 배포되므로 사용자가 이를 보기 위해 별도의 특수 소프트웨어를 설치할 필요가 없습니다. 누구나 웹브라우저나 기본 이미지 뷰어로 쉽게 볼 수 있는 것이 큰 장점입니다.\n",
    "\n",
    "① 일부 인포그래픽은 특정 프로그램에서만 작동... (O):\n",
    "\n",
    "기본적으로는 이미지 형태지만, 상호작용형(Interactive) 인포그래픽의 경우 특정 웹 기술이나 특정 뷰어 환경에서만 데이터 필터링 등이 작동할 수 있습니다. 지문에서 '일부'라고 표현했으므로 맞는 설명으로 볼 수 있습니다.\n",
    "\n",
    "③ 정보와 시각적 그래프의 합성어이다. (O):\n",
    "\n",
    "인포그래픽은 **Information(정보)**과 **Graphics(그래픽)**의 합성어로, 복잡한 정보를 빠르고 명확하게 전달하기 위해 시각화한 것을 의미합니다.\n",
    "\n",
    "④ 부분 전달이나 일부를 잘라서 적용할 경우 오해가 있을 수 있다. (O):\n",
    "\n",
    "인포그래픽은 전체적인 맥락과 스토리텔링을 가지고 설계됩니다. 특정 수치나 그림만 잘라서 보여주면 제작자의 원래 의도나 전체 데이터의 흐름이 왜곡되어 정보의 오해를 불러일으킬 위험이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d087cde5",
   "metadata": {},
   "source": [
    "# 79. 관계 연결성과 완전성 중심의 인포그래픽 유형은?\n",
    "① 도표형 ② 스토리텔링형 ③ 비교분석형 ④ 타임라인형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1aa2f0",
   "metadata": {},
   "source": [
    "정답은 ② 스토리텔링형 입니다.\n",
    "\n",
    "인포그래픽은 전달하고자 하는 정보의 성격에 따라 여러 유형으로 나뉩니다. 문제에서 제시한 **'관계의 연결성'**과 **'완전성'**은 정보를 하나의 흐름으로 묶어 전체적인 맥락을 전달하는 기법의 핵심입니다.\n",
    "\n",
    "상세 해설 및 유형별 특징\n",
    "② 스토리텔링형 (정답):\n",
    "\n",
    "특징: 개별적인 데이터들을 단순히 나열하는 것이 아니라, 인과관계나 흐름에 따라 **하나의 이야기(Story)**로 엮어냅니다.\n",
    "\n",
    "연결성과 완전성: 정보들이 유기적으로 연결되어 있으며, 처음부터 끝까지 읽었을 때 주제에 대한 완전한 이해를 돕도록 설계됩니다. 독자의 공감을 이끌어내고 정보를 기억에 오래 남게 하는 데 가장 효과적입니다.\n",
    "\n",
    "① 도표형:\n",
    "\n",
    "수치 중심의 데이터(통계, 차트 등)를 시각화하는 데 집중합니다. 복잡한 숫자를 직관적인 그래프로 바꾸어 정보의 가독성을 높이는 것이 주된 목적입니다.\n",
    "\n",
    "③ 비교분석형:\n",
    "\n",
    "두 가지 이상의 대상(예: 제품 A vs 제품 B)을 특정 기준에 따라 대조하여 차이점과 공통점을 부각합니다. '스타차트'나 '나란히 놓인 막대그래프' 등이 자주 쓰입니다.\n",
    "\n",
    "④ 타임라인형:\n",
    "\n",
    "시간의 흐름(연도, 날짜 등)에 따라 사건이나 변화 과정을 순서대로 배치합니다. 역사를 설명하거나 프로젝트의 진행 단계를 보여줄 때 적합합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98fea3",
   "metadata": {},
   "source": [
    "# 80. 다음 인포그래픽 유형 중 연도나 시간의 흐름에 따라 정보를 나열하기에 적합한 유형으로 알맞은 것은? \n",
    "① 타임라인 ② 도표형 ③ 스토리텔링 ④ 비교분석형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29507e8d",
   "metadata": {},
   "source": [
    "정답은 ① 타임라인 (Timeline) 입니다.이 문제는 전달하려는 정보의 특성(시간성)에 가장 부합하는 시각화 형식을 선택할 수 있는지 묻고 있습니다.상세 해설① 타임라인 (정답):특징: 특정 사건이나 데이터의 변화를 **시간 순서(Chronological order)**에 따라 배치하는 방식입니다.적합한 상황: 기업의 연혁, 프로젝트의 진행 단계, 역사적 사건의 흐름, 제품의 발전 과정 등을 설명할 때 가장 직관적입니다.시각 요소: 주로 가로 또는 세로로 긴 축(Line)을 기준으로 날짜나 연도를 표시하고 관련 이미지를 곁들입니다.② 도표형:복잡한 수치나 통계 데이터를 차트(파이, 막대 등)로 변환하여 숫자의 의미를 빠르게 전달하는 데 집중합니다.③ 스토리텔링:인과관계나 전체적인 맥락을 강조하여 독자가 하나의 이야기를 읽는 것처럼 정보를 전달합니다. (79번 문제의 핵심 내용)④ 비교분석형:두 개 이상의 대상을 동일한 기준선상에 놓고 차이점과 공통점을 한눈에 파악할 수 있도록 대조합니다.💡 인포그래픽 유형별 핵심 키워드유형핵심 키워드비즈니스 활용 예시타임라인시간, 연도, 순서, 역사회사 연혁, 연간 실적 추이도표형통계, 비율, 수치, 양인구 통계 리포트, 설문 결과비교분석대조, 차이, 성능 비교자사 vs 경쟁사 제품 스펙 비교프로세스형단계, 절차, 방법, 매뉴얼서비스 이용 방법, 제조 공정💡 실전 팁문제에서 \"흐름\", \"변화 과정\", **\"순서\"**와 함께 **\"시간\"**이나 **\"연도\"**라는 단어가 명시되면 고민할 것 없이 타임라인을 선택하시면 됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "401.484px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
