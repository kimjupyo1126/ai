{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b77abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:99% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
       "div.text_cell_render.rendered_html{font-size:20pt;}\n",
       "div.text_cell_render li, div.text_cell_render p, code{font-size:22pt; line-height:30px;}\n",
       "div.output {font-size:24pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:24pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
       "table.dataframe{font-size:24px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:99% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
    "div.text_cell_render.rendered_html{font-size:20pt;}\n",
    "div.text_cell_render li, div.text_cell_render p, code{font-size:22pt; line-height:30px;}\n",
    "div.output {font-size:24pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:24pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
    "table.dataframe{font-size:24px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99cdbb",
   "metadata": {},
   "source": [
    "# 01. 분석성숙도 평가항목에 속하지 않는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0a2e7",
   "metadata": {},
   "source": [
    "1. 비즈니스\n",
    "2. 조직, 역량\n",
    "3. 법/제도\n",
    "4. IT자원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eea2d5",
   "metadata": {},
   "source": [
    "일반적으로 기업의 데이터 분석 수준을 측정하는 분석성숙도(Analytic Maturity) 평가항목은 크게 세 가지 영역(비즈니스 부문, 조직·역량 부문, IT 자원 부문)으로 구성됩니다. 각 항목의 세부 내용은 다음과 같습니다.\n",
    "\n",
    "분석성숙도 주요 평가 항목 비즈니스 부문: 분석이 비즈니스 가치 창출에 얼마나 기여하는지, 분석 과제가 얼마나 체계적으로 관리되고 있는지를 평가합니다.\n",
    "\n",
    "조직·역량 부문: 분석 전담 조직 유무, 인력의 전문성, 분석 문화의 확산 정도 등을 평가합니다.\n",
    "\n",
    "IT 자원 부문: 데이터 저장 및 관리 인프라, 분석 도구(Tool), 데이터의 품질과 통합 수준 등을 평가합니다.\n",
    "\n",
    "참고하세요! '법/제도'는 분석을 수행할 때 준수해야 할 외부적인 제약 사항이나 가이드라인이 될 수는 있지만, 기업 내부의 분석 성숙도(역량 수준)를 직접적으로 평가하는 지표에는 포함되지 않는 것이 일반적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62030c2",
   "metadata": {},
   "source": [
    "# 02. 다음에서 설명하는 내용으로 가장 적절한 것을 보기에서 고르시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1f68d",
   "metadata": {},
   "source": [
    "()는 데이터 분석을 기없의 문화로 정착하고 데이터의 분석 업무를 지속적으로 고도화하기 위해서 조직 재분석 및 관리하는\n",
    "체계를 의미한다. 전사 차원의 모든 데이터에 대해 정책 및 지침, 표준화 운영 조직 및 책임 등의 표준화된 관리체계를 수립\n",
    "하고 운영을 위한 프레임워크의 구축을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91794f",
   "metadata": {},
   "source": [
    "1. BCP(bussiness Continuty Planning)\n",
    "2. ERP(Enterprise Resource Planning)\n",
    "3. KPI/MBO\n",
    "4. 데이터 거버넌스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b0ad6",
   "metadata": {},
   "source": [
    "데이터 거버넌스는 단순히 데이터를 저장하는 기술적인 단계를 넘어, 기업 전체의 자산으로서 데이터를 효율적이고 안전하게 관리하기 위한 의사결정 체계와 프로세스를 의미합니다.\n",
    "\n",
    "데이터 거버넌스의 3대 요소\n",
    "데이터 거버넌스를 체계적으로 구축하기 위해서는 아래 세 가지 요소가 유기적으로 맞물려야 합니다.\n",
    "\n",
    "조직(Organization): 데이터 관리 권한과 책임을 가진 조직(예: 데이터 거버넌스 위원회, 데이터 관리자 등)을 구성합니다.\n",
    "\n",
    "프로세스(Process): 데이터를 수집, 저장, 활용, 폐기하는 일련의 표준 활동과 지침을 수립합니다.\n",
    "\n",
    "기술(Technology): 메타데이터 관리 시스템, 데이터 사전(Data Dictionary) 등 거버넌스를 지원하는 시스템을 도입합니다.\n",
    "\n",
    "데이터 거버넌스의 주요 체계\n",
    "보통 다음과 같은 흐름으로 관리 체계가 구축됩니다.\n",
    "\n",
    "데이터 표준화: 데이터 명칭, 정의, 형식 등을 통일하여 혼선을 방지합니다.\n",
    "\n",
    "데이터 관리 체계: 데이터 라이프사이클에 따른 관리 원칙을 수립합니다.\n",
    "\n",
    "데이터 저장소 관리: 메타데이터 및 데이터 품질을 실시간으로 모니터링합니다.\n",
    "\n",
    "표준화 활동: 주기적인 점검과 교육을 통해 거버넌스를 내재화합니다.\n",
    "\n",
    "이 내용은 데이터 분석 준전문가(ADSP) 시험의 분석 거버넌스 체계 수립 파트에서 매우 자주 출제되는 핵심 개념입니다.\n",
    "\n",
    "혹시 데이터 거버넌스의 4대 구성 요소인 데이터 표준화, 데이터 관리 체계, 데이터 저장소 관리, 표준화 활동에 대해 더 자세한 설명이 필요하신가요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f61fd",
   "metadata": {},
   "source": [
    "# 03. 심층신경망 분석기법 중, 분류 모델과 희귀 보델은 어떤 분석 방법에 속하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb06a0",
   "metadata": {},
   "source": [
    "1. 비지도학습\n",
    "2. 지도학습\n",
    "3. 강화학습\n",
    "4. 준지도학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc1f81",
   "metadata": {},
   "source": [
    "1. 지도 학습 (Supervised Learning)\n",
    "지도 학습은 '문제(Data)'와 '정답(Label)'이 모두 주어진 상태에서 컴퓨터를 학습시키는 방법입니다. 마치 학생이 문제집을 풀고 뒤에 있는 해설지(정답)를 보며 공부하는 것과 같습니다.\n",
    "\n",
    "목표: 새로운 데이터가 들어왔을 때 그 데이터의 정답을 예측하는 것.\n",
    "\n",
    "주요 작업:\n",
    "\n",
    "분류(Classification): 데이터를 정해진 범주에 따라 나눔 (예: 스팸 메일 분류, 개/고양이 사진 구분).\n",
    "\n",
    "회귀(Regression): 연속적인 수치를 예측함 (예: 내일의 기온 예측, 주가 예측).\n",
    "\n",
    "2. 비지도 학습 (Unsupervised Learning)\n",
    "비지도 학습은 정답(Label) 없이 '문제(Data)'만 주어진 상태에서 학습합니다. 데이터 내부에 숨겨진 특성이나 패턴, 구조를 스스로 찾아내는 것이 특징입니다.\n",
    "\n",
    "목표: 데이터의 유의미한 패턴이나 그룹을 발견하는 것.\n",
    "\n",
    "주요 작업:\n",
    "\n",
    "군집화(Clustering): 유사한 특성을 가진 데이터끼리 그룹으로 묶음 (예: 고객 소비 패턴에 따른 타겟팅).\n",
    "\n",
    "차원 축소(Dimensionality Reduction): 핵심 정보는 유지하면서 데이터의 변수를 줄임.\n",
    "\n",
    "연관 규칙(Association Rule): 데이터 간의 상관관계를 찾음 (예: 맥주를 사는 사람은 기저귀도 함께 산다).\n",
    "\n",
    "3. 준지도 학습 (Semi-supervised Learning)\n",
    "지도 학습과 비지도 학습의 중간 형태입니다. 대부분 정답이 없는 데이터이고, 아주 일부분만 정답이 있는 데이터일 때 사용합니다.\n",
    "\n",
    "배경: 정답(Label)을 달아주는 작업(Labeling)은 비용과 시간이 많이 들기 때문에, 적은 양의 정답 데이터를 활용해 전체 데이터의 패턴을 파악한 뒤 성능을 높이는 방식입니다.\n",
    "\n",
    "예시: 구글 포토에서 이름표가 붙은 사진 몇 장을 토대로 이름이 없는 수만 장의 사진 속 인물을 자동으로 분류하는 기술.\n",
    "\n",
    "4. 강화 학습 (Reinforcement Learning)\n",
    "정답 대신 **'보상(Reward)'**이라는 개념을 사용하여 학습합니다. 어떤 환경 안에서 에이전트(컴퓨터)가 행동을 하고, 그 결과에 따라 상이나 벌을 받으며 보상을 최대화하는 방향으로 최적의 행동을 찾아가는 과정입니다.\n",
    "\n",
    "목표: 보상을 가장 많이 받을 수 있는 최적의 행동 방책(Policy)을 학습하는 것.\n",
    "\n",
    "핵심 요소: 에이전트(Agent), 상태(State), 행동(Action), 보상(Reward).\n",
    "\n",
    "예시: 알파고(AlphaGo), 자율주행 자동차, 게임 AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ae2bb",
   "metadata": {},
   "source": [
    "# 04. 다음 중 인공지능, 머신러닝, 딥러닝에 대한 설명으로 옳지 않은 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a91371",
   "metadata": {},
   "source": [
    "1. 인공지능 : 컴퓨터가 인간의 지능적인 행동을 모방할 수 있도록 하는 소프트웨어\n",
    "2. 머신러닝 : 명시적인 프로그랭밍 없이 컴퓨터가 학습하는 능력을 갖추게 하는 연구 분야\n",
    "3. 딥러닝 : 사람의 개입이 필요한 지도학습, 능동적인 비지도학습이 결합되어 컴퓨터가 마치 사라처럼 스스로 학습할 수 있는 인공지능 기술\n",
    "4. 머신러닝 : 딥러닝 보다 인간의 사고방식과 더 유사한 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0b9f2",
   "metadata": {},
   "source": [
    "1번 (인공지능): 가장 넓은 개념으로, 기계가 지능적인 행동을 하도록 만드는 모든 기술을 통칭합니다. 정확한 설명입니다.\n",
    "\n",
    "2번 (머신러닝): 전통적인 프로그래밍은 사람이 일일이 규칙(If-then)을 정해주어야 하지만, 머신러닝은 데이터로부터 컴퓨터가 스스로 규칙을 찾아내도록 합니다. 정확한 설명입니다.\n",
    "\n",
    "3번 (딥러닝): 딥러닝은 비정형 데이터(이미지, 음성 등)에서 특징을 스스로 추출하는 능력이 탁월하며, 복잡한 신경망 구조를 통해 인간처럼 고도의 추론이 가능하게 합니다. (문맥상 인공지능의 심화 기술로 이해하시면 됩니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae90a6",
   "metadata": {},
   "source": [
    "# 05.  개인정보처리자는 정보주체의 동의를 받은 경우 개인정보의 수집, 이용이 가능하지만, 개인정보보호법에서는 수집 목적에 한하여 동의 없이 처리할 수 있는 경우를 정의하고 있다. 다음 중 동의 없이 가능한 경우가 아닌 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e92df6",
   "metadata": {},
   "source": [
    "1. 대학교에서 학교의 시설 유지 관리를 위해 차주의 휴대번호를 교내 재학 정보시스템에서 조회하여 주자민원을 해결하는 경우\n",
    "2. 보험 회사가 계약 체결을 위해 청약자의 자동차 사고 이력, 다른 유사 보험 가입 여부등의 정보를 수집하는 경우\n",
    "3. 회사가 업무 효율성을 높이기 위해 임직원의 업무 처리 내역과 인터넷 접속 기록을 모니터링하는 시스템을 설치하는 경우\n",
    "4. 아파트에 화재가 발생하여 집 안에 있는 자녀를 구조하기 위해 부모의 휴대번호를 수집하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372048ac",
   "metadata": {},
   "source": [
    "❌ 3번이 정답(옳지 않은 것)인 이유\n",
    "3번: 회사가 업무 효율성을 높이기 위해 임직원의 인터넷 접속 기록을 모니터링하는 경우\n",
    "\n",
    "이유: '업무 효율성 증진'은 개인정보보호법 제15조 제1항에서 규정하는 '법령상 의무 준수'나 '계약의 이행', '급박한 생명·신체·재산의 이익' 등에 해당하지 않습니다.\n",
    "\n",
    "특히 근로자의 인터넷 접속 기록이나 업무 내역 모니터링은 사생활 침해의 소지가 크기 때문에, 단지 효율성만을 이유로 동의 없이 수집하는 것은 법 위반일 가능성이 매우 높습니다. 반드시 근로자의 별도 동의를 받거나 근로자참여법 등에 따른 협의가 필요합니다.\n",
    "\n",
    "✅ 동의 없이 처리가 가능한 이유 (나머지 보기)\n",
    "1번 (대학교 주차 민원): 개인정보보호법 제15조 제1항 제6호(개인정보처리자의 정당한 이익)에 해당할 수 있습니다. 학교 시설의 원활한 운영과 주차 민원 해결은 학교 측의 정당한 이익이며, 이는 정보주체(학생)의 권리보다 우선한다고 판단될 수 있는 합리적인 범위 내의 처리입니다.\n",
    "\n",
    "2번 (보험 계약 체결): 제15조 제1항 제4호(계약의 체결 및 이행을 위하여 불가피하게 필요한 경우)에 해당합니다. 보험사가 사고 이력 등을 확인하는 것은 보험 계약 체결 여부를 결정하기 위한 필수적인 과정이므로 동의 없이도 수집이 가능합니다.\n",
    "\n",
    "4번 (화재 구조): 제15조 제1항 제5호(정보주체 또는 그 법정대리인의 생명, 신체, 재산의 이익을 위하여 급박하게 필요하다고 인정되는 경우)에 해당합니다. 화재 상황에서 인명 구조를 위해 연락처를 수집하는 것은 생명의 안전을 위한 긴급한 상황이므로 예외가 허용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedcd0c1",
   "metadata": {},
   "source": [
    "# 06. 다음 중 민감정보에 처리에 해당하지 않는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ea6cb",
   "metadata": {},
   "source": [
    "1. LP 음반을 판매하는 회사가 고객 동의를 받아 정기적으로 카탈로그를 보내오다가 오디오 테이프, CD, DVD 형태의 음악 카탈로그도 보내는 경우\n",
    "2. 약국에서 고객이 다른 고객의 의약품을 잘못 가져간 경우 약국이 해당 고객에 이 사실을 알리가 위해 처방 병원에 휴대폰번호를 제공받아 전화하는 경우\n",
    "3. 병원진료 시 수집한 환자의 주민등록번호를 추가적인 질병 여부 파악을 위해 연계한 병원에 문의하여 병력조회를 하는 경우\n",
    "4. 홈쇼핑 고객에게 동의를 받아 매월 DM(Direct Message) 발송을 위해 상품안내를 진해하였으나 할인쿠폰이 추가 발급되어 DM과 함께 발송한는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72533c40",
   "metadata": {},
   "source": [
    "제시해주신 문제에서 민감정보 처리에 해당하지 않는 것을 고르려면, 먼저 **'민감정보'**의 정의를 명확히 알아야 합니다.\n",
    "\n",
    "민감정보란? 사상·신념, 노동조합·정당의 가입·탈퇴, 정치적 견해, 건강(병력), 성생활, 유전자 정보, 범죄경력 자료 등 주체의 사생활을 현저히 침해할 우려가 있는 정보를 말합니다.\n",
    "\n",
    "따라서 정답은 3번입니다.\n",
    "\n",
    "내용: 병원진료 시 수집한 환자의 정보를 다른 병원에 문의하여 **'병력조회'**를 하는 경우입니다.\n",
    "\n",
    "이유: **'병력(과거 질병 기록)'**은 개인정보보호법에서 규정하는 대표적인 민감정보입니다. 민감정보를 수집하거나 제3자에게 제공(문의)하기 위해서는 일반 개인정보와 별도로 반드시 정보주체에게 별도의 동의를 받아야 하거나 법령상 근거가 있어야 합니다. 따라서 이 행위는 민감정보 처리에 해당합니다.\n",
    "\n",
    "개인정보보호법 제23조에 명시된 민감정보는 다음과 같습니다.\n",
    "\n",
    "사상·신념 (종교 등)\n",
    "\n",
    "노동조합·정당의 가입 및 탈퇴\n",
    "\n",
    "정치적 견해\n",
    "\n",
    "건강 및 성생활 (질병, 병력 등)\n",
    "\n",
    "유전정보\n",
    "\n",
    "범죄경력자료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5923da3",
   "metadata": {},
   "source": [
    "# 07. 다음 중 분석 유형을 구분할 때 데이터 분석방법은 충분히 이해하고 있지만 조직내 분석대상을 인지하지 못하는 유형은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c6b4c",
   "metadata": {},
   "source": [
    "1. 최적화\n",
    "2. 발견\n",
    "3. 통찰\n",
    "4. 솔루션"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44d0b8",
   "metadata": {},
   "source": [
    "1. 솔루션 (Solution) — 정답\n",
    "분석 대상(What): 모름 (Unknown)\n",
    "\n",
    "분석 방법(How): 알음 (Known)\n",
    "\n",
    "특징: 분석 방법(알고리즘, 도구 등)은 잘 갖춰져 있으나, 정작 우리 조직에서 무엇을 분석해야 성과가 날지 대상을 찾지 못한 경우입니다. 따라서 해결책(Solution)을 찾는 과정이 중심이 됩니다.\n",
    "\n",
    "2. 최적화 (Optimization)\n",
    "분석 대상(What): 알음 (Known)\n",
    "\n",
    "분석 방법(How): 알음 (Known)\n",
    "\n",
    "특징: 대상과 방법을 모두 알고 있을 때, 분석 효율을 극대화하여 가장 좋은 결과를 내는 과정입니다.\n",
    "\n",
    "3. 통찰 (Insight)\n",
    "분석 대상(What): 알음 (Known)\n",
    "\n",
    "분석 방법(How): 모름 (Unknown)\n",
    "\n",
    "특징: 분석 대상은 명확하지만 기존에 사용하던 방식으로는 해결이 안 될 때, 새로운 분석 기법을 통해 숨겨진 의미를 찾아내는 과정입니다.\n",
    "\n",
    "4. 발견 (Discovery)\n",
    "분석 대상(What): 모름 (Unknown)\n",
    "\n",
    "분석 방법(How): 모름 (Unknown)\n",
    "\n",
    "특징: 대상과 방법을 모두 모르는 상태에서 데이터 자체를 탐색하며 새로운 비즈니스 기회나 가치를 찾아내는 가장 창의적인 단계입니다.\n",
    "\n",
    "분석 대상 (What)\t분석 방법 (How)\t분석 유형\n",
    "Known (앎)\tKnown (앎)\t최적화 (Optimization)\n",
    "Unknown (모름)\tKnown (앎)\t솔루션 (Solution)\n",
    "Known (앎)\tUnknown (모름)\t통찰 (Insight)\n",
    "Unknown (모름)\tUnknown (모름)\t발견 (Discovery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8954a268",
   "metadata": {},
   "source": [
    "# 08. 분석 문제 정의의 상향식접근법의 절차를 맞게 설명한 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7abec1",
   "metadata": {},
   "source": [
    "1. 프로세스 분류 -> 프로세스 흐름 분석 -> 분석요건 정의 -> 분석요건 식별\n",
    "2. 프로세스 흐름 분석 -> 프로세스 분류 -> 분석요건 식별 -> 분석요건 정의\n",
    "3. 프로세스 분류 -> 프로세스 흐름 분석 -> 분석요건 식별 -> 분석요건 정의\n",
    "4. 프로세스 흐름 분석 -> 프로세스 분류 -> 분석요건 정의 -> 분석요건 식별"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196de35b",
   "metadata": {},
   "source": [
    "✅ 정답 설명\n",
    "3번: 프로세스 분류 -> 프로세스 흐름 분석 -> 분석요건 식별 -> 분석요건 정의\n",
    "\n",
    "상향식 접근법 중 시행착오를 통한 문제 해결 방식의 일반적인 절차는 다음과 같습니다.\n",
    "\n",
    "프로세스 분류: 먼저 분석 대상이 되는 전체 업무 프로세스를 체계적으로 나눕니다.\n",
    "\n",
    "프로세스 흐름 분석: 분류된 각 프로세스가 어떻게 흘러가는지, 어떤 데이터가 발생하는지 상세히 분석합니다.\n",
    "\n",
    "분석요건 식별: 분석을 통해 해결할 수 있는 구체적인 요구사항이나 아이디어를 찾아냅니다.\n",
    "\n",
    "분석요건 정의: 식별된 요건을 구체화하여 분석 데이터, 분석 모델링 기법 등을 확정합니다.\n",
    "\n",
    "하향식(Top-Down): 비즈니스 모델이나 전략에서부터 문제를 도출하는 논리적인 방식입니다.\n",
    "\n",
    "상향식(Bottom-Up): 현장의 데이터나 상세 프로세스에서부터 가치를 발견하는 실무적인 방식입니다. 현대 분석에서는 이 두 방식을 반복적으로 혼합하여 사용하는 것이 일반적입니다.\n",
    "\n",
    "구분\t하향식 접근법 (Top-Down)\t상향식 접근법 (Bottom-Up)\n",
    "\n",
    "중심\t문제(Problem) 중심\t데이터(Data) 중심\n",
    "\n",
    "특징\t문제가 주어지고 해답을 찾음\t데이터를 탐색하며 문제를 발견함\n",
    "\n",
    "과정\t문제 식별 → 솔루션 탐색 → 타당성 검토\t프로세스 분석 → 요건 식별 → 요건 정의\n",
    "\n",
    "핵심\t\"무엇을 해결해야 하는가?\"\t\"데이터에서 무엇을 찾을 수 있는가?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb680d1",
   "metadata": {},
   "source": [
    "# 09. 빅데이터 분석절차는 빅데이터 분석방법론을 토대로 5단계 절차로 수행된다. 다음 중 빅데이터 분석절차로 옳은것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ab431",
   "metadata": {},
   "source": [
    "1. 분석기획 -> 데이터 준비 -> 시스템 구현 -> 데이터 분석 -> 평가 및 전개\n",
    "2. 분석기획 -> 데이터 분석 -> 시스템 구현 -> 데이터 준비 -> 평가 및 전개\n",
    "3. 데이터 준비 -> 분석기획 -> 데이터 분석 -> 시스템 구현 -> 평가 및 전개\n",
    "4. 분석기획 -> 데이터 준비 -> 데이터 분석 -> 시스템 구현 -> 평가 및 전개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab04534",
   "metadata": {},
   "source": [
    "✅ 4번: 분석기획 -> 데이터 준비 -> 데이터 분석 -> 시스템 구현 -> 평가 및 전개\n",
    "빅데이터 분석은 체계적인 방법론을 바탕으로 하며, 각 단계는 다음과 같은 핵심 과업을 수행합니다.\n",
    "\n",
    "분석 기획 (Planning): 비즈니스 이해 및 범위 설정, 프로젝트 정의, 데이터 획득 계획 등을 수립하는 단계입니다.\n",
    "\n",
    "데이터 준비 (Preparation): 데이터 정의, 비정형/정형 데이터 수집, 데이터 정제(Cleaning) 및 품질 점검을 수행하는 단계입니다.\n",
    "\n",
    "데이터 분석 (Analysis): 탐색적 데이터 분석(EDA), 모델링, 모델 평가 및 검증을 통해 유의미한 결과를 도출하는 핵심 단계입니다.\n",
    "\n",
    "시스템 구현 (Development): 분석된 알고리즘을 시스템화하기 위해 설계, 구현, 테스트를 거쳐 실제 운영 환경에 적용하는 단계입니다.\n",
    "\n",
    "평가 및 전개 (Evaluation & Deployment): 분석 결과의 비즈니스 영향력을 평가하고, 모델 업데이트 및 최종 보고서를 작성하여 프로젝트를 마무리하는 단계입니다.\n",
    "\n",
    "💡 헷갈리기 쉬운 포인트!\n",
    "준비 vs 분석: 데이터를 분석하기 전에는 반드시 데이터 수집 및 정제(준비) 단계가 선행되어야 합니다.\n",
    "\n",
    "분석 vs 구현: 모델을 먼저 만들고(분석), 그 모델을 실제 돌아가는 프로그램이나 서비스로 만드는 것(구현)이 논리적인 순서입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a286a5a",
   "metadata": {},
   "source": [
    "# 10. 데이터 분석 단계에서는 분석용 데이터 준비, 분석, 모델링, 평가 및 검증을 수행한다. 다음에서 설명하는 데이터 분석 단계의 수행내용에 알맞는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6895c41f",
   "metadata": {},
   "source": [
    "()는 벨연구소의 수학자 '존튜키'가 개발한 데이터 분석과정에 대한 개념으로 데이터를 분석하고 결과를 내는 과정에 있어서 지속적으로 해당 데이터에 대한 탐색과 이해를 기본으로 가져야 한단는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6477ae",
   "metadata": {},
   "source": [
    "1. 모델링\n",
    "2. 정합성 검증\n",
    "3. 시스템 테스트\n",
    "4. 탐색적 분석(EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de54cf4",
   "metadata": {},
   "source": [
    "✅ 정답 설명: 탐색적 데이터 분석 (EDA, Exploratory Data Analysis)\n",
    "개념: 통계학자 **존 튜키(John Tukey)**가 제안한 방식으로, 데이터의 가설을 세우기 전에 데이터 자체를 있는 그대로 탐색하여 데이터의 구조, 특성, 이상치 등을 파악하는 과정입니다.\n",
    "\n",
    "핵심 내용: * 데이터에 대한 탐색과 이해를 기본으로 합니다.\n",
    "\n",
    "주로 **시각화(그래프, 차트)**와 **요약 통계(평균, 중앙값 등)**를 활용하여 데이터의 숨겨진 패턴을 찾아냅니다.\n",
    "\n",
    "모델링에 앞서 데이터가 가진 잠재적인 문제를 발견하고, 어떤 분석 기법을 쓸지 결정하는 나침반 역할을 합니다.\n",
    "\n",
    "💡 오답 체크 (다른 단계의 의미)\n",
    "모델링(Modeling): EDA를 통해 파악된 데이터를 바탕으로 실제 예측이나 분류를 수행할 알고리즘(회귀분석, 랜덤포레스트 등)을 선택하고 구축하는 단계입니다.\n",
    "\n",
    "정합성 검증: 데이터가 서로 모순 없이 일관되게 일치하는지, 혹은 분석 결과가 논리적으로 타당한지 확인하는 작업입니다.\n",
    "\n",
    "시스템 테스트: 분석 모델을 실제 IT 시스템에 적용했을 때 오류 없이 정상적으로 작동하는지 확인하는 공학적인 단계입니다.\n",
    "\n",
    "📌 EDA의 4가지 주요 테마\n",
    "존 튜키가 강조한 EDA의 핵심 요소는 다음과 같습니다.\n",
    "\n",
    "저항성(Resistence): 데이터의 일부 파손(이상치)에 영향을 덜 받는 통계량 확인.\n",
    "\n",
    "잔차(Residual): 관찰값에서 모델의 예측값을 뺀 나머지 부분을 분석하여 추가 패턴 탐색.\n",
    "\n",
    "재표현(Re-expression): 데이터를 로그 등으로 변환하여 분석하기 쉬운 형태로 바꿈.\n",
    "\n",
    "현시성(Revelation): 그래프 등을 통해 데이터의 특징을 시각적으로 나타냄."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff66a72",
   "metadata": {},
   "source": [
    "# 11. 가설 설정을 통해 통계 모델을 만들거나 기계학습(지도학습, 비지도학습)을 이용하여 모델을 만드는 과정은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfd9e9d",
   "metadata": {},
   "source": [
    "1. 모델링\n",
    "2. 텍스트 분석\n",
    "3. 모델 평가\n",
    "4. 탐색적 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f8601",
   "metadata": {},
   "source": [
    "✅ 정답 설명: 모델링 (Modeling)\n",
    "정의: 준비된 데이터를 바탕으로 통계적 기법이나 머신러닝 알고리즘(지도학습, 비지도학습 등)을 적용하여 데이터의 패턴을 수치화하거나 예측 가능한 수식(모델)으로 만드는 과정입니다.\n",
    "\n",
    "핵심 활동:\n",
    "\n",
    "가설 설정: 분석 목적에 맞는 독립변수와 종속변수를 설정합니다.\n",
    "\n",
    "알고리즘 선택: 회귀 분석, 결정 트리, 신경망 등 적절한 기법을 고릅니다.\n",
    "\n",
    "학습: 데이터를 알고리즘에 입력하여 최적의 파라미터를 찾아냅니다.\n",
    "\n",
    "💡 오답 체크 (다른 선택지 설명)\n",
    "2번 (텍스트 분석): 비정형 텍스트 데이터에서 유의미한 정보를 추출하는 기술(자연어 처리 등)로, 모델링의 한 분야이거나 데이터 전처리 단계에 해당합니다.\n",
    "\n",
    "3번 (모델 평가): 구축된 모델이 얼마나 정확한지 테스트 데이터(Test Data)를 통해 성능을 측정(정확도, 정밀도 등)하는 다음 단계입니다.\n",
    "\n",
    "4번 (탐색적 분석/EDA): 모델링을 하기 전, 데이터의 분포와 특징을 파악하여 가설을 세우고 변수를 선택하는 사전 단계입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574e513",
   "metadata": {},
   "source": [
    "# 12. 다음 중 반정형 데이터의 수집기술로 가장 적절하지 않는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5095195e",
   "metadata": {},
   "source": [
    "1. Open API\n",
    "2. HTTP\n",
    "3. ETL\n",
    "4. Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869935a4",
   "metadata": {},
   "source": [
    "❌ 3번이 정답(부적절한 것)인 이유\n",
    "ETL은 주로 **정형 데이터(Structured Data)**를 대상으로 하는 기술입니다.\n",
    "\n",
    "설명: 기업의 운영 시스템(DB)에 저장된 정형 데이터를 추출(Extract)하여, 목적에 맞게 변환(Transform)한 뒤, 데이터 웨어하우스(DW)에 로드(Load)하는 일련의 과정을 의미합니다.\n",
    "\n",
    "특징: 데이터의 구조가 미리 정의된(Schema-on-write) 정형 데이터를 처리하는 데 최적화되어 있습니다.\n",
    "\n",
    "✅ 나머지 보기가 반정형 데이터 수집 기술인 이유\n",
    "**반정형 데이터(Semi-structured Data)**란 데이터의 고정된 형태는 없으나 XML, JSON, HTML처럼 메타데이터나 스키마 정보를 포함하고 있는 데이터를 말합니다.\n",
    "\n",
    "1번 (Open API): 주로 JSON이나 XML 형식으로 데이터를 주고받는 통로입니다. 기상청 정보, 주식 시세 등을 반정형 형태로 가져올 때 가장 많이 사용됩니다.\n",
    "\n",
    "2번 (HTTP): 인터넷상에서 웹 페이지나 데이터를 주고받는 기본 프로토콜입니다. 웹상의 반정형 데이터를 전송받기 위한 통신 수단으로 사용됩니다.\n",
    "\n",
    "4번 (Crawling/Scraping): 웹 사이트의 HTML 소스(반정형 데이터)를 수집하여 필요한 정보를 추출하는 기술입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df782d",
   "metadata": {},
   "source": [
    "# 13. 데이터 변환에 대한 설명으로 옳은 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aabbea4",
   "metadata": {},
   "source": [
    "1. 데이터 유형을 변환하거나 데이터 분석이 용이한 형태로 변환하는 과정\n",
    "2. 결측값 변환, 이상값 제거, 노이즈 데이터를 교정하는 과정\n",
    "3. 데이터 분석이 용이하도록 기존 또는 유사 데이블을 연계, 통합하는 과정\n",
    "4. 레거시 시스템의 데이터와 함께 분석이 필요한 경우 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b4e2e3",
   "metadata": {},
   "source": [
    "✅ 1번이 정답인 이유: \n",
    "\n",
    "데이터 변환 (Data Transformation)정의: 수집된 데이터를 분석 알고리즘이 처리하기 적합한 형식으로 형상을 바꾸는 과정입니다.\n",
    "\n",
    "주요 작업: * 데이터 유형 변환: 문자형을 숫자형으로 바꾸거나, 날짜 형식을 통일함.정규화(Normalization): \n",
    "\n",
    "데이터의 범위를 $0$~$1$ 사이로 변환함.\n",
    "\n",
    "표준화(Standardization): 평균을 $0$, 표준편차를 $1$로 변환함.이산화(Discretization): 연속형 수치를 범주형(예: 점수 → 등급)으로 바꿈.\n",
    "\n",
    "💡 오답 체크 (다른 보기는 어떤 과정일까요?)\n",
    "\n",
    "2번 (데이터 정제 - Data Cleaning): 결측값(Missing value) 처리, 이상값(Outlier) 제거, 잡음(Noise) 교정 등 데이터의 품질을 높이는 작업입니다.\n",
    "\n",
    "3번 (데이터 통합 - Data Integration): 여러 곳에 흩어져 있는 데이터 소스를 하나로 합치는 과정입니다. (예: 고객 테이블 + 구매 이력 테이블 결합)\n",
    "\n",
    "4번 (데이터 연계/통합): 주로 레거시 시스템(기존 시스템)의 데이터를 분석 환경으로 가져와 합치는 과정을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f939a5",
   "metadata": {},
   "source": [
    "# 14. 다음 중 데이터 비식별화 방법에 대한 설명으로 가장 적절하지 않은 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8970fa",
   "metadata": {},
   "source": [
    "1. 데이터 마스킹: 개인식별이 가능한 데이터에 직접적으로 식별할 수 없는 다른값으로 대체\n",
    "2. 데이터 삭제 : 개인정보 식별이 가능한 특정 데이터값으로 삭제\n",
    "3. 데이터 범주화 : 단일 식별 정보를 해당 그룹의 대표값으로 변화하거나 구간값으로 변화\n",
    "4. 총계처리 : 개인정보에 대해 통계값을 적용해 특정 개인을 판단할 수 없도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aec01f",
   "metadata": {},
   "source": [
    "제시해주신 데이터 비식별화 기술 중에서 정의가 적절하지 않은 것은 1번입니다.\n",
    "\n",
    "❌ 1번이 정답(부적절한 것)인 이유\n",
    "1번: 데이터 마스킹(Data Masking)\n",
    "\n",
    "설명: \"식별할 수 없는 다른 값으로 대체\"하는 것은 마스킹이 아니라 **가명처리(Pseudonymization)**에 해당합니다.\n",
    "\n",
    "올바른 정의: 데이터 마스킹은 데이터의 전부 또는 일부를 공백, 별표(*), 노이즈 등으로 덮어씌워 보이지 않게 하는 기술입니다. (예: 홍길동 → 홍동, 900101-1234567 → 900101-1*****)\n",
    "\n",
    "✅ 나머지 보기에 대한 올바른 설명\n",
    "2번 (데이터 삭제): 식별 가능한 데이터(이름, 주소, 상세 주소 등)를 완전히 삭제하는 방식입니다. 비식별화 중 가장 강력한 방법입니다.\n",
    "\n",
    "3번 (데이터 범주화): 구체적인 수치나 명칭을 범주(그룹)로 묶는 방식입니다. (예: 28세 → 20대, 서울시 강남구 역삼동 → 서울시 거주)\n",
    "\n",
    "4번 (총계처리): 개별 데이터의 값을 보여주지 않고 집합의 평균이나 합계 등 통계치만 보여주어 개인을 식별하지 못하게 합니다. (예: 임직원 개별 연봉 → 임직원 평균 연봉)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f2a55",
   "metadata": {},
   "source": [
    "# 15. 비식별화 프라이버시 모델 중 특정인 추론이 안 된다고 해도 민감한 정보의 다양성을 높여 추론 가능성을 낮추는 기법은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c0365b",
   "metadata": {},
   "source": [
    "1. k-익명성\n",
    "2. l-다양성\n",
    "3. t-근접성\n",
    "4. s-민감성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e4cad",
   "metadata": {},
   "source": [
    "✅ 정답 설명: l-다양성 (l-diversity)핵심 개념: $k$-익명성 모델이 가진 취약점(동질성 공격 등)을 보완하기 위해 제안되었습니다.\n",
    "\n",
    "특징: 비식별 처리된 데이터 집합에서 특정인으로 추론되는 레코드들이 있더라도, 그들이 가진 민감 정보(질병, 소득 등)의 종류가 적어도 $l$개 이상이어야 한다는 원리입니다.\n",
    "\n",
    "효과: 민감 정보의 종류를 다양하게 유지함으로써, 특정인이 어떤 민감한 속성을 가졌는지 확률적으로 추론하는 것을 방지합니다.\n",
    "\n",
    "k-익명성 (k-anonymity):자신과 같은 속성을 가진 데이터를 $k$개 이상으로 만들어 누구인지 식별할 수 없게 합니다. \n",
    "\n",
    "하지만 그룹 내 모든사람이 같은 병명을 가졌다면(동질성 공격) 개인 정보가 노출될 수 있는 단점이 있습니다.\n",
    "\n",
    "t-근접성 (t-closeness):$l$-다양성이 정보의 분포를 고려하지 못하는 단점(왜곡 공격 등)을 보완합니다.\n",
    "\n",
    "전체 데이터의 민감 정보 분포와 특정 그룹의 민감 정보 분포 차이를 $t$ 이하로 유지하여 정보 노출을 최소화합니다.\n",
    "\n",
    "s-민감성 (s-sensitivity):표준적으로 널리 쓰이는 프라이버시 모델 용어는 아니며, 보통 $k$, $l$, $t$ 모델이 핵심 시험 범위에 해당합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd8d99f",
   "metadata": {},
   "source": [
    "# 16. 다음을 설명하는 품질 특성을 고르시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cd5a9",
   "metadata": {},
   "source": [
    "필수항목에 누락이 없어야 한다.\n",
    "예) 고객의 아이디는 NULL일 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d0073",
   "metadata": {},
   "source": [
    "1. 완전성\n",
    "2. 유일성\n",
    "3. 일관성\n",
    "4. 신뢰성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6785bd5c",
   "metadata": {},
   "source": [
    "정답: 1번 (완전성)\n",
    "\n",
    "설명: 완전성이란 데이터의 구성 요소 중 누락된 것이 없이 모두 기록되어 있는 상태를 의미합니다. 문제에서 언급된 \"필수항목에 누락이 없어야 한다\"는 조건과 \"NULL일 수 없다(Not Null)\"는 제약 조건은 데이터의 완전성을 보장하기 위한 대표적인 방법입니다.\n",
    "\n",
    "2. 유일성\t데이터가 중복되지 않고 고유해야 함을 의미합니다.\t기본키(PK), 중복 가입 방지\n",
    "\n",
    "3. 일관성\t서로 다른 곳에서 관리되는 데이터가 일치해야 함을 의미합니다.\t주소 변경 시 모든 시스템에 반영됨\n",
    "\n",
    "4. 신뢰성\t데이터가 출처가 명확하고 믿을 수 있어야 함을 의미합니다.\t정확한 측정값, 권한 있는 사용자의 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c21ca0",
   "metadata": {},
   "source": [
    "# 17. 정형 데이터의 품질 기준에 대한 설명으로 가장 적절하지 않은 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2056c",
   "metadata": {},
   "source": [
    "1. 필수항목에서는 누락이 없어야 하는 특성\n",
    "2. 규정된 조건에서 사용될 때, 사용자에 의해 이해되고 선호될 수 있게 하는 정도\n",
    "3. 데이터 항목은 정해진 데이터 유효범위 및 도메인을 충족해야 하는 특성\n",
    "4. 데이터가 지켜야 할 구조, 값 등이 일관되게 정의 되어야 하며 일치해야 하는 특성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abfb320",
   "metadata": {},
   "source": [
    "제시된 보기들은 데이터 품질 관리 및 데이터 품질 진단의 주요 지표들을 설명하고 있습니다. 각 항목이 의미하는 품질 특성은 다음과 같습니다.\n",
    "\n",
    "1번: 완전성 (Completeness)\n",
    "\n",
    "설명: 필수항목에 누락(Null)이나 결측치가 없어야 한다는 특성입니다.\n",
    "\n",
    "2번: 사용성 (Usability) - [부적절]\n",
    "\n",
    "설명: 제시된 내용은 주로 UI/UX나 소프트웨어 품질(ISO 9126 등) 측면에서의 사용성을 설명하고 있습니다. 정형 데이터 품질의 핵심 지표(완전성, 유효성, 일관성, 정확성 등)와는 거리가 멉니다.\n",
    "\n",
    "3번: 유효성 (Validity)\n",
    "\n",
    "설명: 데이터가 정해진 유효 범위(Range), 데이터 타입, 도메인(Domain) 내에 존재해야 한다는 특성입니다. (예: 성별 항목에 '남', '여' 외의 값은 허용 안 함)\n",
    "\n",
    "4번: 일관성 (Consistency)\n",
    "\n",
    "설명: 데이터 구조나 값이 서로 모순되지 않고 일관되게 정의되어야 한다는 특성입니다. (예: 고객 ID가 서로 다른 테이블에서 동일한 형식을 유지함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da231369",
   "metadata": {},
   "source": [
    "# 18. 데이터의 품질진단으로 도출할 수 있는 질문은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f433a1",
   "metadata": {},
   "source": [
    "1. 왜 발생했는가?\n",
    "2. 언제할 것인가?\n",
    "3. 사건이 일어날 것인가?\n",
    "4. 어떻게 대응할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e19de",
   "metadata": {},
   "source": [
    "데이터 품질진단은 현재 데이터의 상태를 점검하여 오류 데이터(Outlier)를 찾아내고 그 원인을 분석하는 과정입니다. 각 보기가 어떤 분석 단계에 해당하는지 정리해 드립니다.\n",
    "\n",
    "분석의 단계와 질문의 유형\n",
    "데이터 품질 관리와 분석 성숙도 모델(Analytic Maturity Model) 관점에서 질문을 분류하면 다음과 같습니다.\n",
    "\n",
    "1번: 왜 발생했는가? (Why did it happen?) [정답]\n",
    "\n",
    "품질진단/진단 분석(Diagnostic Analytics) 단계입니다.\n",
    "\n",
    "데이터에 오류가 있다면, 그 오류가 시스템 결함인지, 입력 실수인지, 혹은 프로세스의 문제인지 원인을 파악하는 질문입니다.\n",
    "\n",
    "2번: 언제할 것인가? (When will it happen?)\n",
    "\n",
    "예측 분석(Predictive Analytics) 단계의 변형된 질문입니다. 특정 이벤트의 발생 시점을 묻는 것입니다.\n",
    "\n",
    "3번: 사건이 일어날 것인가? (Will it happen?)\n",
    "\n",
    "예측 분석(Predictive Analytics) 단계입니다. 과거 데이터를 바탕으로 미래의 발생 가능성을 가늠하는 질문입니다.\n",
    "\n",
    "4번: 어떻게 대응할 것인가? (How can we make it happen? / What should we do?)\n",
    "\n",
    "처방 분석(Prescriptive Analytics) 단계입니다. 분석 결과를 바탕으로 최적의 해결책이나 행동 방안을 결정하는 질문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b0b2a",
   "metadata": {},
   "source": [
    "# 19. 전통적 분석 환경에서의 ETL에 대한 설명으로 알맞지 않는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0d6fd",
   "metadata": {},
   "source": [
    "1. ETL은 Extract, Transform, Load 세 단어의 축약어로 데이터 소스시스템 및 환경으로부터 데이터를 추출하야 비즈니스 데이터로 변환한 후 데이터 마트, 데이터 웨어하우스, ODS로 적재한다.\n",
    "2. ETL 구현을 위해 일괄 ETL, 실시간 ETL로 구분할 수 있다.\n",
    "3. 대용량 데이터 처리를 위해 MPP(작업 단계에서 다수의 프로세서가 동시 처리할 수 있게 하는 병렬처리 프로세스)를 지원한다.\n",
    "4. ETL은 중간 단계에서 저장하는 역활을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b0efb",
   "metadata": {},
   "source": [
    "정답은 **4번 \"ETL은 중간 단계에서 저장하는 역할을 한다\"**입니다.\n",
    "\n",
    "이 문제는 데이터 통합의 핵심인 ETL(Extract, Transform, Load) 프로세스의 정의와 특징을 묻고 있습니다. 각 보기가 왜 맞거나 틀린지 상세히 설명해 드리겠습니다.\n",
    "\n",
    "1번: ETL의 기본 정의 [적절]\n",
    "\n",
    "ETL은 데이터를 추출(Extract)하고, 가공/변환(Transform)하여 목적지(DW, DM, ODS 등)에 적재(Load)하는 일련의 과정을 의미합니다.\n",
    "\n",
    "2번: 구현 방식에 따른 구분 [적절]\n",
    "\n",
    "데이터 처리 주기에 따라 정해진 시간에 대량의 데이터를 옮기는 Batch(일괄) ETL과 데이터 발생 즉시 반영하는 Real-time(실시간) ETL로 구분됩니다.\n",
    "\n",
    "3번: 대용량 데이터 처리 기술 [적절]\n",
    "\n",
    "전통적인 데이터 웨어하우스(DW) 환경에서도 대용량 데이터를 빠르게 처리하기 위해 MPP(Massively Parallel Processing) 아키텍처를 활용하여 병렬 처리를 수행합니다.\n",
    "\n",
    "4번: 중간 저장 역할 [부적절]\n",
    "\n",
    "ETL은 기본적으로 데이터를 소스에서 목적지로 **'전송 및 변환'하는 프로세스(흐름)**를 의미합니다. 데이터를 '중간 저장'하는 물리적인 장소는 **ODS(Operational Data Store)**나 Staging Area라고 부르며, ETL은 이 장소들을 거쳐가는 행위 자체를 뜻합니다. 즉, ETL 자체가 저장소 역할을 하는 것은 아닙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12703f58",
   "metadata": {},
   "source": [
    "# 20. 비정형 데이터 저장방식으로 알맞지 않는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc6f2a",
   "metadata": {},
   "source": [
    "1. 단어, 문장\n",
    "2. 음성 데이터는 정적으므로 y = f(x)의 형태\n",
    "3. 이미지는 RGB픽셀\n",
    "4. 동영상은 스트리밍 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ccce9",
   "metadata": {},
   "source": [
    "정답은 **2번 \"음성 데이터는 정적으므로 $y = f(x)$의 형태\"**입니다.\n",
    "\n",
    "1번: 단어, 문장 [적절]텍스트 데이터는 대표적인 비정형 데이터입니다. \n",
    "\n",
    "이는 자연어 처리(NLP) 과정을 거쳐 벡터화되거나, NoSQL 데이터베이스(Document Store 등)에 \n",
    "\n",
    "저장됩니다.\n",
    "\n",
    "2번: 음성 데이터의 특성 [부적절]이유:\n",
    "\n",
    "음성 데이터는 '정적'인 데이터가 아니라 시간의 흐름에 따라 변하는 '동적(Dynamic)'인 시계열 데이터입니다.\n",
    "\n",
    "음성은 공기의 진동을 전기 신호로 변환한 것이며, 주로 $f(t)$(시간에 따른 진폭의 변화) 형태로 표현됩니다. \n",
    "\n",
    "이를 저장할 때는 샘플링 과정을 거쳐 디지털 신호로 변환하여 저장합니다.\n",
    "\n",
    "3번: 이미지 데이터 [적절]이미지는 각 픽셀의 밝기나 색상 값을 행렬(Matrix) 형태로 저장합니다. \n",
    "\n",
    "컬러 이미지의 경우 RGB(Red, Green, Blue) 3개의 채널을 가진 3차원 텐서 형태로 수치화되어 저장됩니다.\n",
    "\n",
    "4번: 동영상 데이터 [적절]동영상은 연속된 이미지 프레임과 오디오의 결합체입니다. \n",
    "\n",
    "용량이 매우 크기 때문에 파일을 통째로 내려받기보다, 데이터를 잘게 나누어 실시간으로 전송하는 스트리밍(Streaming) 방식이나 압축 코덱을 활용한 저장 방식이 일반적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb1e76",
   "metadata": {},
   "source": [
    "# 21. 이상값 탐지를 위한 상자그림에서 측정 불가능한 항목은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2300018",
   "metadata": {},
   "source": [
    "1. 제1사분의수\n",
    "2. 사분위수 범위(IQR)\n",
    "3. 평균값\n",
    "4. 중앙값(Median)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d59ae6",
   "metadata": {},
   "source": [
    "정답은 **3번 \"평균값\"**입니다.\n",
    "\n",
    "최솟값 (Minimum): 제1사분위수에서 $1.5 \\times IQR$을 뺀 값보다 큰 데이터 중 가장 작은 값.\n",
    "\n",
    "제1사분위수 (Q1, 1번 보기): 데이터를 크기순으로 나열했을 때 하위 25% 지점의 값.\n",
    "\n",
    "중앙값 (Median, 4번 보기): 데이터를 크기순으로 나열했을 때 정중앙(50%)에 위치한 값. (상자 내부의 선)\n",
    "\n",
    "제3사분위수 (Q3): 데이터를 크기순으로 나열했을 때 상위 **25%(75% 지점)**의 값.\n",
    "\n",
    "최댓값 (Maximum): 제3사분위수에서 $1.5 \\times IQR$을 더한 값보다 작은 데이터 중 가장 큰 값."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90789f8",
   "metadata": {},
   "source": [
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13557f58",
   "metadata": {},
   "source": [
    "# 22. 레퍼 기법에 대한 설명으로 가장 부절절한 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a5ed9",
   "metadata": {},
   "source": [
    "1. 전진선택법은 변수를 알 수 없는 상테로 시작하여 반복할 때마다 가장 중요한 변수를 추가함으로써 더 이상 성능 향상이 없을 때까지 변수를 추가하는 방법이다.\n",
    "2. 전진선택법은 변수의 작은변동에도 그 결과가 유지되어 안정성이 높다.\n",
    "3. 후진선택법은 모든 변수를 가지고 시작하고 가장 덜 중요한 변수를 하나씩 제거하면서 모델의 성능을향상시키는 기법이다.\n",
    "4. 단계별 선택법은 전진과 후진제거법을 결합하여 사용하는 기법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9598c4",
   "metadata": {},
   "source": [
    "정답은 **2번 \"전진선택법은 변수의 작은 변동에도 그 결과가 유지되어 안정성이 높다\"**입니다.\n",
    "\n",
    "이 문제는 변수 선택(Feature Selection) 기법 중 래퍼(Wrapper) 기법의 세부 방법론에 대해 묻고 있습니다.\n",
    "래퍼 기법은 모델의 성능을 직접 측정하기 때문에 정확도가 높지만, 계산량이 많고 과적합(Overfitting)의 위험이 있습니다.\n",
    "\n",
    "1번: 전진선택법(Forward Selection)의 정의 [적절]\n",
    "\n",
    "변수가 없는 상태에서 시작하여, 모델의 성능(AIC, BIC 등)을 가장 많이 향상시키는 변수를 하나씩 추가해 나가는 방식입니다.\n",
    "\n",
    "2번: 전진선택법의 안정성 [부적절]\n",
    "\n",
    "이유: 전진선택법을 포함한 래퍼 기법들은 **변수의 작은 변동에도 모델의 결과가 크게 바뀔 수 있다는 단점(불안정성)**을 가지고 있습니다. 한 번 선택된 변수는 전진선택 과정에서 계속 남게 되므로, 초기 변수 선택이 전체 모델에 큰 영향을 미치기 때문입니다.\n",
    "\n",
    "3번: 후진제거법(Backward Elimination)의 정의 [적절]\n",
    "\n",
    "모든 독립 변수를 포함한 상태에서 시작하여, 기여도가 가장 낮은(중요도가 떨어지는) 변수를 하나씩 제거하며 성능을 최적화하는 방식입니다.\n",
    "\n",
    "4번: 단계별 선택법(Stepwise Selection)의 정의 [적절]\n",
    "\n",
    "전진선택과 후진제거를 혼합한 방식입니다. 변수를 추가한 후, 기존에 추가되었던 변수들이 새로운 변수 추가 시점에서도 여전히 유의미한지 검토하여 유의미하지 않으면 제거하는 과정을 반복합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f316d02",
   "metadata": {},
   "source": [
    "# 23. 대표적인 차원축소 기법에 대한 다음의 설명 중 옳지 않는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec58c2a",
   "metadata": {},
   "source": [
    "1. 주성분은 여러 변수들의 선형 결합으로 이루어진 주성분이라는 새로운 변수를 만들어 기존의 변수들을 요약하여 축소하며, 각 주성분 간에 우선순위가 없이 대등하다.\n",
    "2. 다차원척도법은 데이터 속게 잠재해 있는 패턴, 구조를 찾아내어 소수 자원의 공간에 객체 간 근접성을 시각화하는 통계 기법으로 차원축소에 사용될 수 있다.\n",
    "3. 차원축소 기법 중 하나인 요인분석은 여러 개의 변수들로 이루어진 데이터에서 변수들 간의 상관관계를 고려하여 서로 유사한 변수들을 묶어 새로운 잠재요인을 추출해내는 분석기법이다.\n",
    "4. t-SNE는 데이터에서 지역 인접성을 보존하려고 시도하는 차원축소 알고리즘으로, 비선형적이며 비결정적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a38883",
   "metadata": {},
   "source": [
    "정답은 1번입니다.\n",
    "\n",
    "이 문제는 데이터 분석의 주요 차원 축소 기법인 PCA(주성분 분석), MDS(다차원 척도법), 요인 분석, t-SNE의 개념과 특징을 구분하는 문제입니다.\n",
    "\n",
    "1번: 주성분 분석(PCA)의 특징 [부적절]이유: \n",
    "주성분 분석에서 생성된 주성분들 사이에는 명확한 우선순위가 존재합니다. \n",
    "제1주성분($PC_1$)은 전체 데이터의 분산(정보량)을 가장 많이 설명하며, 그다음 제2주성분($PC_2$),\n",
    "제3주성분 순으로 설명력이 낮아집니다. 또한, 각 주성분은 서로 **직교(Orthogonal)**하여 상관관계가 없도록 설계됩니다. \"대등하다\"는 설명은 PCA의 핵심 원리인 '정보 응축과 순위 결정'에 어긋납니다.\n",
    "\n",
    "2번: 다차원 척도법(MDS) [적절]\n",
    "MDS는 객체 간의 거리(유사성/부전형성)를 보존하면서 저차원 공간(보통 2차원이나 3차원)에 점을 배치하는 기법입니다. 데이터의 시각적 패턴을 파악하는 데 유용합니다.\n",
    "\n",
    "3번: 요인 분석(Factor Analysis) [적절]\n",
    "요인 분석은 겉으로 드러난 여러 변수 뒤에 숨어있는 **잠재 요인(Latent Factor)**을 찾아내는 기법입니다. 예를 들어 국어, 영어, 수학 점수라는 변수를 통해 '학습 능력'이라는 잠재 요인을 도출하는 식입니다.\n",
    "\n",
    "4번: t-SNE [적절]\n",
    "t-SNE는 고차원 데이터의 **지역적 인접성(Local Structure)**을 저차원에서도 최대한 유지하도록 하는 시각화 특화 알고리즘입니다. 비선형적인 구조를 잘 잡아내며, 실행할 때마다 결과가 조금씩 달라질 수 있는 비결정적(Non-deterministic) 특성을 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b15ea8",
   "metadata": {},
   "source": [
    "# 24. 아래의 설명 중 파생변수의 추가방법에 해당되는 것을 모두 고르시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0ce8ca",
   "metadata": {},
   "source": [
    "ㄱ. 한 값으로부터 특징들을 추출한다.\n",
    "\n",
    "ㄴ. 한 레코드의 값들을 결합한다.\n",
    "\n",
    "ㄷ. 다른 데이블의 부가적인 정보를 참조한다.\n",
    "\n",
    "ㄹ. 거래 레코드를 분리한다.\n",
    "\n",
    "ㅁ. 복잡한 표현방식으로 변환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249cce15",
   "metadata": {},
   "source": [
    "1. ㄱ, ㄷ\n",
    "2. ㄴ, ㄹ\n",
    "3. ㄱ, ㄴ, ㄷ\n",
    "4. ㄱ, ㄴ, ㅁ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9306e",
   "metadata": {},
   "source": [
    "질문하신 내용이 데이터 변환 및 가공(Data Transformation) 또는 **피처 엔지니어링(Feature Engineering)**의 구체적인 기법에 대한 문제로 보입니다.\n",
    "\n",
    "제시된 보기들(ㄱ~ㅁ) 중 일반적인 데이터 가공 및 변환 기법에 해당하는 것들을 고르는 문제라면 정답은 3번(ㄱ, ㄴ, ㄷ) 또는 맥락에 따라 **4번(ㄱ, ㄴ, ㅁ)**이 될 수 있으나, 보통 데이터 전처리 과정의 정석적인 단계를 묻는다면 **3번(ㄱ, ㄴ, ㄷ)**이 가장 적절합니다.\n",
    "\n",
    "ㄱ. 한 값으로부터 특징들을 추출한다. (Feature Extraction)\n",
    "\n",
    "예: 날짜 데이터('2025-12-20')에서 '연도', '월', '요일' 등을 따로 추출하여 분석에 활용하는 방식입니다.\n",
    "\n",
    "ㄴ. 한 레코드의 값들을 결합한다. (Feature Aggregation/Combination)\n",
    "\n",
    "예: '수량'과 '단가'라는 두 칼럼을 곱해 '총매출액'이라는 하나의 새로운 지표를 만드는 방식입니다.\n",
    "\n",
    "ㄷ. 다른 테이블의 부가적인 정보를 참조한다. (Data Joining/Referencing)\n",
    "\n",
    "예: 고객 ID만 있는 거래 테이블에 고객 상세 정보 테이블을 Join하여 연령대, 거주지 정보를 결합하는 방식입니다.\n",
    "\n",
    "ㄹ. 거래 레코드를 분리한다.\n",
    "\n",
    "일반적인 데이터 변환보다는 데이터 분할(Splitting)에 해당하며, 분석을 위한 '가공' 기법의 범주에는 우선순위가 밀립니다.\n",
    "\n",
    "ㅁ. 복잡한 표현방식으로 변환한다.\n",
    "\n",
    "데이터를 더 단순하게 요약하거나 정규화하는 것이 일반적이지, 고의로 복잡하게 만드는 것은 분석의 목적에 부합하지 않는 경우가 많습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622711b",
   "metadata": {},
   "source": [
    "# 25. 불균형 데이터의 처리방법으로 적합하지 않은 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243f42a",
   "metadata": {},
   "source": [
    "1. 언더 샘플링\n",
    "2. 오버 샘플링\n",
    "3. SMOTE\n",
    "4. 임베디드 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b0b9d9",
   "metadata": {},
   "source": [
    "정답은 **4번 \"임베디드 기법\"**입니다.\n",
    "불균형 데이터(Imbalanced Data) 처리는 다수 클래스(Majority Class)에 비해 소수 클래스(Minority Class)의 비중이 너무 적을 때, 모델이 다수 클래스에 편향되는 것을 막기 위한 기법들을 의미합니다.\n",
    "\n",
    "1번: 언더 샘플링 (Under-sampling) [적합]\n",
    "\n",
    "다수 클래스의 표본을 소수 클래스 수준으로 줄이는 방법입니다. 데이터 손실이 발생할 수 있다는 단점이 있습니다.\n",
    "\n",
    "2번: 오버 샘플링 (Over-sampling) [적합]\n",
    "\n",
    "소수 클래스의 표본을 복제하거나 생성하여 데이터를 늘리는 방법입니다. 정보 손실은 없지만 과적합(Overfitting)의 위험이 있습니다.\n",
    "\n",
    "3번: SMOTE (Synthetic Minority Over-sampling Technique) [적합]\n",
    "\n",
    "가장 대표적인 오버 샘플링 기법 중 하나입니다. 단순히 복제하는 것이 아니라, 소수 클래스 데이터들 사이의 인접한 샘플을 찾아 그 사이의 가상의 데이터를 생성하여 과적합을 방지합니다.\n",
    "\n",
    "4번: 임베디드 기법 (Embedded Method) [부적합]\n",
    "\n",
    "이유: 임베디드 기법은 불균형 데이터 처리 방법이 아니라, 변수 선택(Feature Selection) 기법 중 하나입니다. 모델 학습 과정에서 가중치(규제)를 주어 중요한 변수를 선택하는 기법(예: Lasso, Ridge)을 말합니다.\n",
    "\n",
    "불균형 데이터 처리 기법 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b19d1",
   "metadata": {},
   "source": [
    "# 26. 다음 중 오버샘플링에 대한 설명으로 옳지 않는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f9b3c",
   "metadata": {},
   "source": [
    "1. 정보가 손실되지 않는다는 장점이 있다.\n",
    "2. 데이터의 수를 증가시키므로 복제되는 데이터에 분류기가 과적합될 수 있는 단점이 있다.\n",
    "3. 오버샘플링 기법에는 Resampling, SMOTH, Berderline SMOTH가 있다.\n",
    "4. 데이터의 크기가 클때 효과적이며 계산 시간이 감소한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d19ef",
   "metadata": {},
   "source": [
    "정답은 **4번 \"데이터의 크기가 클 때 효과적이며 계산 시간이 감소한다.\"**입니다.\n",
    "이 설명은 오버샘플링(Over-sampling)이 아닌 **언더샘플링(Under-sampling)**에 대한 특징입니다. 각 보기의 상세 설명은 다음과 같습니다.\n",
    "\n",
    "1번: 정보 손실 유무 [적절]\n",
    "\n",
    "오버샘플링은 소수 클래스의 데이터를 삭제하지 않고 복제하거나 생성하기 때문에, 원본 데이터가 가진 정보가 소실되지 않는다는 장점이 있습니다.\n",
    "\n",
    "2번: 과적합(Overfitting) 위험 [적절]\n",
    "\n",
    "소수 데이터를 인위적으로 늘리는 과정에서 특정 데이터를 단순 복제하게 되면, 모델이 해당 데이터에만 너무 특화되어 학습되는 과적합 현상이 발생할 수 있습니다.\n",
    "\n",
    "3번: 주요 기법들 [적절]\n",
    "\n",
    "단순 복제(Resampling), 데이터 사이의 공간에 가상 데이터를 생성하는 SMOTE, 경계선(Borderline)에 위치한 데이터에 집중하여 생성하는 Borderline-SMOTE 등이 대표적인 오버샘플링 기법입니다. (문제의 오타: SMOTH → SMOTE, Berderline → Borderline)\n",
    "\n",
    "4번: 데이터 크기와 계산 시간 [부적절]\n",
    "\n",
    "이유: 오버샘플링은 기존 데이터에 새로운 데이터를 '추가'하는 방식이므로, 전체 데이터의 크기가 늘어납니다. 따라서 계산량은 많아지고 학습 시간은 길어집니다. 반면, 데이터 크기가 클 때 데이터를 줄여서 속도를 높이는 방식은 언더샘플링의 장점입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41951150",
   "metadata": {},
   "source": [
    "# 27. 상관관계 분석에 대한 설명으로 가장 부절적한 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38efde",
   "metadata": {},
   "source": [
    "1. 두 개의 변수 간의 직선관계 선형성과 산점도르 확이할 수 있다.\n",
    "2. 양(+)의 상관관계는 두 변수가 동반 증가되는 것이며, 음(-)으 상관관계는 두 변수의 값이 반대로 증감하는 것이다.\n",
    "3. -1~+1 이의 값으로 -1과 +1은 완전한 비선형관계를 의미한다.\n",
    "3. 두 변수의 선형관계 측정을 위한 수치로 공분산과 상관계수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f2feb",
   "metadata": {},
   "source": [
    "정답은 **3번 \"-1~+1 사이의 값으로 -1과 +1은 완전한 비선형관계를 의미한다\"**입니다.\n",
    "\n",
    "1번: 선형성과 산점도 [적절]\n",
    "\n",
    "상관분석은 기본적으로 두 변수가 얼마나 '직선(선형)'에 가깝게 분포하는지를 측정합니다. 이를 시각적으로 가장 먼저 확인하는 방법이 바로 **산점도(Scatter Plot)**입니다.\n",
    "\n",
    "2번: 상관관계의 방향 [적절]\n",
    "\n",
    "양(+)의 상관관계: 한 변수가 커질 때 다른 변수도 같이 커지는 관계 (우상향 직선 형태)\n",
    "\n",
    "음(-)의 상관관계: 한 변수가 커질 때 다른 변수는 작아지는 관계 (우하향 직선 형태)\n",
    "\n",
    "3번: 상관계수의 범위와 의미 [부적절]\n",
    "\n",
    "이유: 상관계수(r)가 -1과 +1이라는 것은 **\"완전한 선형관계\"**를 의미합니다.\n",
    "\n",
    "+1: 모든 점이 기울기가 양수인 하나의 직선 위에 있음 (완전 양의 선형관계)\n",
    "\n",
    "-1: 모든 점이 기울기가 음수인 하나의 직선 위에 있음 (완전 음의 선형관계)\n",
    "\n",
    "따라서 \"비선형관계\"라는 설명은 틀린 것입니다. 참고로 상관계수가 0인 경우 선형관계가 없다고 하며, 이때 비선형 관계가 존재할 가능성이 있습니다.\n",
    "\n",
    "4번: 공분산과 상관계수 [적절]\n",
    "\n",
    "공분산(Covariance): 두 변수의 관계를 나타내지만 단위(Scale)의 영향을 받습니다.\n",
    "\n",
    "상관계수(Correlation Coefficient): 공분산을 각 변수의 표준편차로 나누어 정규화한 값으로, 단위에 영향을 받지 않고 -1에서 1 사이의 값을 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66496893",
   "metadata": {},
   "source": [
    "# 28. 프로야구 선수 A의 연도별 홈런 개수를 조사하였다.주어진 자료가 {1,2,5,4,7,10}일때, 자료의 중앙값을 계산하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3cb89",
   "metadata": {},
   "source": [
    "1. 중앙값 = 4.2\n",
    "2. 중앙값 = 4.3\n",
    "3. 중앙값 = 4,4\n",
    "4. 중앙값 = 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b6b0b",
   "metadata": {},
   "source": [
    "정답은 **4번 \"중앙값 = 4.5\"**입니다.\n",
    "\n",
    "이번 문제에서는 자료의 개수가 **6개(짝수)**로 주어졌습니다. 데이터가 짝수일 때 중앙값을 구하는 절차는 다음과 같습니다.\n",
    "\n",
    "가운데 위치한 두 개의 값을 찾습니다.자료의 개수($n$)가 6개이므로, 3번째 값인 4와 4번째 값인 5가 중앙에 위치합니다.\n",
    "\n",
    "자료의 개수($n$)가 6개이므로, 3번째 값인 4와 4번째 값인 5가 중앙에 위치합니다.\n",
    "\n",
    "자료 개수에 따른 중앙값 결정 방식중앙값은 자료의 분포 특성에 따라 계산법이 달라집니다.\n",
    "자료의 개수결정 방법예시 (1,2,4,5,7)\n",
    "\n",
    "홀수 ($n=2k-1$) 정중앙에 위치한 값3번째 값인 4짝수 ($n=2k$)\n",
    "가운데 두 값의 산술평균(3번째+4번째)/2\n",
    "\n",
    "Tip: 시험 문제에서 중앙값을 구할 때 가장 흔히 하는 실수는 데이터를 크기순으로 정렬하지 않고 바로 가운데 값을 찾는 것입니다. 반드시 정렬(Sorting)을 먼저 해야 한다는 점을 잊지 마세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed10763",
   "metadata": {},
   "source": [
    "# 29. 다음 시계열의 그래프 추이를 보고, 올바른 해석을 선택하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5f6c2",
   "metadata": {},
   "source": [
    "1. A회사에 매출은 2년 동안 완만하게 증가했음을 알 수 있다.\n",
    "2. B회사애 매출도 2년 동안 전반적으로 증가했음을 보여주지만, A회사의 경우보다느 변동이 크다.\n",
    "3. B회사는 A회사보다 매출이 낮게 시작했으나, 4월에 B회사가 A회사의 매출을 추월했다.\n",
    "4. A회사가 B회사보다 변동성이 높은 사업구조를 지니고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971cd6f",
   "metadata": {},
   "source": [
    "# 30.다음에서 설명하는 다변량 데이터 탐색방법으로 가장 알맞는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa256f4",
   "metadata": {},
   "source": [
    "()는 하나의 변수를 하나의 세로축으로 평행하게 배치하여, 한 축의 윗부분은 각 변수의 (매핑되는 하나의 축)최대값, 아래의 부분은 각 변수의 최소값을 나타낸다.\n",
    "변수를 표시한 세로축 여러 개를 나란히 늘어놓음으로써 한 번에 많은 데이터를 비교하기에 유용한 그래프다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e503e5b",
   "metadata": {},
   "source": [
    "1. 산점도 행렬\n",
    "2. 평행좌표그래프\n",
    "3. 체르노프페이스\n",
    "4. 스몰멀티플즈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ce3c1",
   "metadata": {},
   "source": [
    "정답은 **2번 \"평행 좌표 그래프(Parallel Coordinates Plot)\"**입니다.\n",
    "\n",
    "2번: 평행 좌표 그래프 (Parallel Coordinates Plot) [정답]\n",
    "\n",
    "설명: 고차원 데이터를 시각화하기 위해 각 변수(Feature)를 수직선(세로축)으로 평행하게 배치합니다.\n",
    "\n",
    "특징: 하나의 데이터 레코드는 각 세로축의 해당 값을 지나는 하나의 **선(Line)**으로 표현됩니다. 여러 변수 간의 관계와 패턴을 한눈에 파악하기에 매우 유용합니다.\n",
    "\n",
    "1번: 산점도 행렬 (Scatter Plot Matrix)\n",
    "\n",
    "설명: 다수의 변수 쌍에 대한 산점도를 격자 형태로 배열한 것입니다.\n",
    "\n",
    "특징: 두 변수 간의 상관관계를 모든 조합에 대해 한꺼번에 확인할 수 있습니다.\n",
    "\n",
    "3번: 체르노프 페이스 (Chernoff Faces)\n",
    "\n",
    "설명: 데이터의 수치를 사람의 얼굴 모양(눈의 크기, 입의 모양, 얼굴 너비 등)에 매핑하여 표현하는 방식입니다.\n",
    "\n",
    "특징: 인간이 얼굴의 변화에 민감하다는 점을 이용한 독특한 다변량 탐색 방법입니다.\n",
    "\n",
    "4번: 스몰 멀티플즈 (Small Multiples)\n",
    "\n",
    "설명: 동일한 격자 구조에 데이터의 부분 집합만 바꾸어 가며 여러 개의 작은 그래프를 나열하는 방식입니다.\n",
    "\n",
    "특징: 에드워드 터프티(Edward Tufte)가 강조한 개념으로, 변수 간의 비교를 용이하게 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a01d30",
   "metadata": {},
   "source": [
    "# 31. 다음에서 설명하는 표본추출 방법은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887fa3cc",
   "metadata": {},
   "source": [
    "모집단의 전체에 번호를 붙어 놓고 첫 번째 선택 후 23개 간경으로 샘플을 추출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1b8d7",
   "metadata": {},
   "source": [
    "↓      ↓      ↓\n",
    "1 2 3 / 4 5 6 / 7 8 9\n",
    "        1 4 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740a79d",
   "metadata": {},
   "source": [
    "1. 단순확률표본추출\n",
    "2. 츨화확률표본추출\n",
    "3. 계통표본추출\n",
    "4. 집락추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb76965",
   "metadata": {},
   "source": [
    "질문에 제시된 그림과 숫자 배열(1, 4, 7)이 설명하는 정답은 3번. 계통표본추출입니다\n",
    "\n",
    "1. 단순확률표본추출 (Simple Random Sampling)\n",
    "방식: 아무런 규칙 없이 제비뽑기나 난수표를 이용해 무작위로 뽑는 방식입니다.\n",
    "\n",
    "특징: 모든 대상이 뽑힐 확률이 동일합니다.\n",
    "\n",
    "2. 층화확률표본추출 (Stratified Random Sampling)\n",
    "방식: 모집단을 성별, 연령대 등 특정 기준(층)으로 나눈 뒤, 각 층 안에서 무작위로 뽑는 방식입니다.\n",
    "\n",
    "특징: 집단 내부는 동질적이고, 집단 간에는 이질적일 때 유리합니다.\n",
    "\n",
    "3. 계통표본추출 (Systematic Sampling) ★ 정답\n",
    "방식: 첫 번째 표본을 무작위로 정한 뒤, **일정한 간격(k)**마다 표본을 뽑는 방식입니다.\n",
    "\n",
    "예시 분석: * 1, 2, 3 / 4, 5, 6 / 7, 8, 9와 같이 구간을 나눕니다.\n",
    "\n",
    "첫 번째 구간에서 '1'을 뽑았다면, 간격 3을 적용해 1, 4, 7을 차례로 추출합니다.\n",
    "\n",
    "4. 집락추출 (Cluster Sampling)\n",
    "방식: 모집단을 여러 개의 소집단(군집)으로 나누고, 특정 소집단을 통째로 선택하여 조사하는 방식입니다.\n",
    "\n",
    "특징: 집단 내부는 이질적이고, 집단 간에는 동질적일 때 사용합니다 (예: 전국 조사를 위해 특정 구/동을 선정)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a20a9e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb26ac",
   "metadata": {},
   "source": [
    "# 32. 핸드폰 생산공장에서 핸드폰을 생산하는데 A,B,C 라인을 사용한다. 생산품의 10%를 A라인, 30%를 B라인, 60%를 C라인에 생산하고, A라인의 불량률은 1%, B라인의 불량률은 2%, C라인의 불량률은 3%라고 했을때, A라인에서 불량품이 발생할 확률은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c99e8d",
   "metadata": {},
   "source": [
    "1. 0.04\n",
    "2. 0,017\n",
    "3. 0.4\n",
    "4. 0.017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0130a",
   "metadata": {},
   "source": [
    "이 문제는 조건부 확률과 전체 확률의 법칙을 활용하여 풀 수 있는 문제입니다.\n",
    "\n",
    "질문의 의도는 \"전체 불량품 중에서 A라인에서 나왔을 확률\"을 묻는 것(베이즈 정리)이 아니라, 단순히 **\"A라인에서 생산되었으면서 동시에 불량품일 확률\"**을 묻는 것으로 보입니다. (만약 \"불량품이 발생했을 때, 그것이 A라인일 확률\"을 묻는 것이라면 계산 방식이 달라집니다.)\n",
    "\n",
    "1. 각 라인별 불량 발생 확률 계산먼저 각 라인에서 불량이 발생할 확률($P$)을 각각 구합니다.A라인 불량 확률: 전체 생산량의 10%($0.1$) × A라인 불량률 1%($0.01$) = 0.001B라인 불량 확률: 전체 생산량의 30%($0.3$) × B라인 불량률 2%($0.02$) = 0.006C라인 불량 확률: 전체 생산량의 60%($0.6$) × C라인 불량률 3%($0.03$) = 0.0182. 전체 불량률 구하기공장 전체에서 불량품이 발생할 총 확률은 위 값을 모두 더한 것입니다.$$0.001 + 0.006 + 0.018 = 0.025$$3. 문제의 의도 파악 및 정답 확인제시된 선택지(0.04, 0.017, 0.4 등)와 위 계산값이 일치하지 않는 것으로 보아, 문제의 수치나 질문 내용에 오타가 있을 가능성이 높습니다.보통 이런 유형의 문제는 **\"불량품이 발견되었을 때, 이 불량품이 A라인에서 생산되었을 확률(베이즈 정리)\"**을 묻는 경우가 많습니다. 이 경우 계산식은 다음과 같습니다.$$P(A|불량) = \\frac{A라인 불량 확률}{전체 불량 확률} = \\frac{0.001}{0.025} = 0.04$$따라서 이 문제의 정답은 **1번(0.04)**이 됩니다.💡 요약 정리전체 불량 중 A의 비중: $0.001 / 0.025 = 0.04$ (4%)따라서 정답은 1번입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6f9eb",
   "metadata": {},
   "source": [
    "# 33. 확률분포에 대한 설명으로 가장 알맞지 않는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df86e4e",
   "metadata": {},
   "source": [
    "1. 이산확률분포는 이항분포, 초기하 분포, 포아송분포가 해당된다.\n",
    "2. 연속확률분포는 x2 분포, F분포, t분포, 정규분포가 있다.\n",
    "3. 확률변수 X가 n개의 이산값{x1, x1, x1,...xn}을 가지며 각 값들이 최할 확률이 동일한 경우, 이 확률변수를 이산규등분포를 따른다고 한다.\n",
    "4. 베르누이분포는 '성공'에 해당하는 사건이 발생할 확률이 P인 똑같은 시행을 n번 반복해서 시행하여 일어난 두 가지 경과에 의해 0과 1로 결정되는 확률 분포이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7bdf80",
   "metadata": {},
   "source": [
    "🔍 오답 분석: 왜 4번이 틀렸나요?\n",
    "4번 설명은 베르누이분포가 아니라 **이항분포(Binomial Distribution)**에 대한 설명입니다.\n",
    "\n",
    "베르누이분포 (Bernoulli Distribution): '성공' 혹은 '실패'라는 두 가지 결과만 나오는 시행을 딱 1번 했을 때의 확률분포입니다. (예: 동전 한 번 던지기)\n",
    "\n",
    "이항분포 (Binomial Distribution): 베르누이 시행을 $n$번 반복했을 때, 성공한 횟수의 합에 대한 확률분포입니다.\n",
    "\n",
    "따라서 4번 문장에서 \"똑같은 시행을 $n$번 반복해서\"라는 부분은 이항분포의 핵심 정의에 해당합니다.\n",
    "\n",
    "1번: 이산확률분포는 값이 뚝뚝 끊어져 있는 분포로, 이항분포, 초기하 분포, 포아송분포가 대표적입니다. (정답)\n",
    "\n",
    "2번: 연속확률분포는 값이 연속적인 범위 내에 있는 분포로, 정규분포를 포함하여 통계적 가설 검정에 쓰이는 $\\chi^2$(카이제곱), $F$, $t$ 분포가 모두 포함됩니다. (정답)\n",
    "\n",
    "3번: 모든 이산값의 확률이 동일한 경우(예: 공정한 주사위 눈이 나올 확률이 모두 1/6인 경우)를 **이산균등분포(Discrete Uniform Distribution)**라고 합니다. (정답)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a45e42",
   "metadata": {},
   "source": [
    "# 34. 이산 확률분포가 아닌 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb9223",
   "metadata": {},
   "source": [
    "1. 이항분포\n",
    "3. 균등분포\n",
    "3. 정규분포\n",
    "4. 포하송분포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b648eec",
   "metadata": {},
   "source": [
    "정답은 3번. 정규분포입니다.\n",
    "\n",
    "이 문제는 확률변수가 취하는 값의 형태에 따라 **이산(Discrete)**과 **연속(Continuous)**을 구분할 수 있는지를 묻는 문제입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17174b32",
   "metadata": {},
   "source": [
    "# 35. 아래의 사례의 경우 사용할 수 있는 연속확률분포는?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27aa29",
   "metadata": {},
   "source": [
    "모분산이 알려져 있고 포본이 충분하지 못한 경우(일반적으로 n < 30이면) 평균의 포본분포를 산출하기 위한 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5aa25",
   "metadata": {},
   "source": [
    "1. z분포\n",
    "2. 카이제곱분포\n",
    "3. t분포\n",
    "4. F분포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87294c9d",
   "metadata": {},
   "source": [
    "정답은 **1번. z분포(표준정규분포)**입니다.\n",
    "\n",
    "이 문제는 표본의 크기($n$)보다 **'모분산($\\sigma^2$)을 알고 있는가'**가 핵심인 문제입니다.\n",
    "\n",
    "모분산($\\sigma^2$)을 알고 있는 경우:\n",
    "표본의 크기($n$)와 상관없이(크든 작든) **z분포(표준정규분포)**를 사용합니다.\n",
    "따라서 제시된 사례처럼 모분산이 알려져 있다면, \n",
    "$n < 30$이더라도 z분포를 사용할 수 있습니다.\n",
    "\n",
    "모분산($\\sigma^2$)을 모르고 모표준편차 대신 표본표준편차($s$)를 사용하는 경우:\n",
    "표본의 크기가 충분히 크면($n \\ge 30$) 중심극한정리에 의해 z분포를 사용할 수 있습니다.\n",
    "표본의 크기가 작으면($n < 30$) t분포를 사용해야 합니다.\n",
    "\n",
    "1. z분포 (정답): 모분산을 알 때 사용합니다.\n",
    "2. 카이제곱분포: 주로 단일 집단의 '분산'을 추정하거나 독립성 검정, 적합도 검정 등에 사용합니다.\n",
    "3. t분포: 모분산을 모를 때, 소표본($n < 30$)의 평균을 추정하기 위해 사용합니다.\n",
    "4. F분포: 두 집단의 '분산'을 비교할 때 사용합니다.\n",
    "따라서 문제에서 **\"모분산이 알려져 있고\"**라는 전제 조건이 붙었으므로, 표본 크기에 상관없이 z분포가 정답이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b98e9",
   "metadata": {},
   "source": [
    "# 36. 다음의 사례에 대한 모평균의 점추정량을 구할 때 빈칸에 적절한 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a72be",
   "metadata": {},
   "source": [
    "대학생들의 한 달 평균 용돈을 알기 위하여 500명의 대학생을 단순무작위로 추출하여 조사한 결과 표본평균 x=50만원 이었다. 따라서 모집단의 모수 모평균은 50만원일 것이라고 추정하는데, 이때 표본통계량(      )는 추정량이 되고, 계산된 구체적인 수치(      )은 추정값이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b518f",
   "metadata": {},
   "source": [
    "제시된 사례에서 빈칸에 들어갈 적절한 용어와 수치는 다음과 같습니다.\n",
    "\n",
    "첫 번째 빈칸 (추정량): 표본평균 ($\\bar{x}$)\n",
    "\n",
    "두 번째 빈칸 (추정값): 50만원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b76ec",
   "metadata": {},
   "source": [
    "# 37. 다음 중 모집단의 점추정 방법으로 적합하지 않은 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35267d41",
   "metadata": {},
   "source": [
    "1. 평균제곱오차(MSE)\n",
    "2. 적률법\n",
    "3. 유의수준\n",
    "4. 최대가능도추정법(MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998cea33",
   "metadata": {},
   "source": [
    "정답은 3번. 유의수준입니다.\n",
    "\n",
    "나머지 보기들은 모집단의 모수를 하나의 수치로 추정하기 위한 점추정(Point Estimation) 방법론이나 그 기준이 되지만, 유의수준은 추정의 영역이 아닌 가설검정의 영역에 속합니다.\n",
    "\n",
    "점추정 관련: 적률법, 최대가능도추정법(MLE), 최소자승법(LSE) 등\n",
    "\n",
    "추정량 평가 기준: 편향(Bias), 효율성, 일치성, 평균제곱오차(MSE)\n",
    "\n",
    "가설검정 관련: 유의수준($\\alpha$), 유의확률($p$-value), 기각역"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdea00c",
   "metadata": {},
   "source": [
    "평균제곱오차 (Mean Squared Error, MSE): 추정량이 실제 모수와 얼마나 차이가 나는지를 측정하는 척도입니다. 점추정량의 정확성을 평가하는 기준으로 사용되며, MSE가 작을수록 더 좋은 추정량으로 평가합니다.\n",
    "\n",
    "적률법 (Method of Moments): 모집단의 적률(평균, 분산 등)을 표본의 적률과 같다고 놓고 모수를 추정하는 방법입니다. 계산이 비교적 간편하여 초기 점추정치로 자주 활용됩니다.\n",
    "\n",
    "유의수준 (Significance Level, $\\alpha$):\n",
    "가설검정에서 귀무가설이 참임에도 불구하고 이를 기각할 확률(제1종 오류)의 최대 허용치를 의미합니다. 이는 **'추정'**보다는 세워진 가설이 맞는지 틀린지를 **'판단'**할 때 사용하는 기준입니다.\n",
    "\n",
    "최대가능도추정법 (Maximum Likelihood Estimation, MLE): 어떤 모숫값이 주어졌을 때, 관측된 데이터가 나타날 확률(우도)을 최대화하는 값을 모수의 추정치로 사용하는 방법입니다. 가장 널리 쓰이는 강력한 점추정 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d132f",
   "metadata": {},
   "source": [
    "# 37. 정규분포를 따르는 모집단에서 표본을 16개 추출하여 어떤 물건의 무게를 측정한 결과 표본평균은 x = 19, 표준편차 S = 12를 얻었다. 이 물건의 모평균 무게에 대한 95% 통계량이 2.131일 경우 신뢰구간을 구하시오?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc729494",
   "metadata": {},
   "source": [
    "1.12.61 <= π <= 25.39\n",
    "\n",
    "2.12.35 <= π <= 24.592\n",
    "\n",
    "3.13.49 <= π <= 25.12\n",
    "\n",
    "3.13.38 <= π <= 24.672"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c740c",
   "metadata": {},
   "source": [
    "문제에서 제시된 조건들을 바탕으로 계산하면 정답은 1번입니다.\n",
    "\n",
    "신뢰구간 결정 : 표본평균($\\bar{x}$)에서 오차한계를 더하고 뺍니다.\n",
    "\n",
    "하한값: $19 - 6.393 = 12.607$\n",
    "\n",
    "상한값: $19 + 6.393 = 25.393$\n",
    "\n",
    "$t \\times \\frac{S}{\\sqrt{n}}$을 사용하여 구합니다.\n",
    "$$\\text{오차한계} = 2.131 \\times \\frac{12}{\\sqrt{16}}$$$$\\text{오차한계} = 2.131 \n",
    "\\times \\frac{12}{4}$$$$\\text{오차한계} = 2.131 \n",
    "\\times 3$$$$\\text{오차한계} = 6.393$$\n",
    "\n",
    "정답: 오차한계는 6.393입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67e332",
   "metadata": {},
   "source": [
    "# 39. 다음의 표에서 제1종 오류에 대한 것으로 알맞는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2799b",
   "metadata": {},
   "source": [
    "가설/가설검증결과       귀무가설이 사실이라고 판정    대립가설이 사실이라고 판정\n",
    "\n",
    "귀무가설이 사실                   (a)                               (b)\n",
    "\n",
    "귀무가설이 사실이 아님            (c)                               (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575ed3b",
   "metadata": {},
   "source": [
    "1.  (a),(b)\n",
    "2.  (a),(c)\n",
    "3.  (b),(c)\n",
    "4.  (b),(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0842052",
   "metadata": {},
   "source": [
    "문제의 형식상 제1종 오류와 제2종 오류를 묻는 의도라면 **3번 (b), (c)**가 가장 적절한 답이 됩니다.\n",
    "\n",
    "문제에서 묻는 제1종 오류는 표의 **(b)**에 해당합니다.\n",
    "\n",
    "질문하신 문제에서 제1종 오류에 해당하는 위치는 **(b)**입니다. \n",
    "\n",
    "만약 시험 문제에서 오류 종류 전체를 묻는 맥락이었다면 \n",
    "\n",
    "**(b)**는 제1종 오류, **(c)**는 제2종 오류이므로 이 둘을 묶은 3번이 정답이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fd0302",
   "metadata": {},
   "source": [
    "# 40. 가설 검정의 제 1, 2종 오류에 대한 설명으로 알맞지 않는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49117556",
   "metadata": {},
   "source": [
    "1. 유의수준은 제1종 오류를 범할 확률로 기무가설이옳음에도 불구하고 이를 기각하는 확률의 크기를 말한다.\n",
    "2. 신뢰도는 검정하려는 귀무가설이 참인 경우, 이를 옳다고 판단할 확률이다.\n",
    "3. 검정력은 대립가설이 참일 때 귀무가설을 기각한는 확률이다.\n",
    "4. 위험도는 주어진 유의 수준에서 귀무가설을 체택하거나 기각하는 기준이 되는 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a91c29",
   "metadata": {},
   "source": [
    "정답은 4번입니다.\n",
    "\n",
    "4번 설명은 위험도가 아니라 **임계값(Critical Value)**에 대한 설명입니다\n",
    "\n",
    "유의수준 ($\\alpha$): (옳음)\n",
    "제1종 오류를 범할 확률의 최대 허용 한계입니다. 즉, \"실제로는 맞는데 틀렸다고 할 확률\"을 $5%(0.05)$나 $1\\%(0.01)$ 등으로 미리 정해두는 것입니다.\n",
    "\n",
    "신뢰도 ($1-\\alpha$): (옳음)\n",
    "귀무가설이 참일 때, 이를 기각하지 않고 옳게 채택할 확률입니다. (예: 유의수준이 $5%$이면 신뢰도는 $95%$입니다.)\n",
    "\n",
    "검정력 ($1-\\beta$): (옳음)대립가설이 참일 때(즉, 귀무가설이 거짓일 때), 이를 올바르게 기각하고 대립가설을 선택할 확률입니다. 연구자가 통계적 차이를 찾아낼 수 있는 실제 능력을 의미합니다.\n",
    "\n",
    "임계값 (Critical Value): (틀림 - 보기의 설명에 해당) 귀무가설을 채택할지 기각할지를 결정하는 경계점을 의미합니다. '위험도'라는 용어는 통계학에서 보통 제1종 오류의 확률(유의수준) 자체를 지칭할 때 사용되기도 하지만, '채택과 기각의 기준이 되는 값'은 임계값이 정확한 표현입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a79f3",
   "metadata": {},
   "source": [
    "# 41. 다음 중 진단분석에 대한 설명으로 알맞는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512dde5d",
   "metadata": {},
   "source": [
    "1. 조직이 원하는 결과를 달성하기 위해 수행해야 할 방향성을 제시하는 분석\n",
    "2. 과거 데이터로 왜 일어났는지 찾기 위한 분석으로 데이터를 자세히 조사하고 주어진 상황의 군본 원인을 파악하기 위한 분석\n",
    "3. 현재 생성되는 데이터를 통해 무엇이 일어날 것인지 예측한느 분석\n",
    "4. 과거에서 현재 데이터를 통해 무엇이 일어났고, 일어나고 있는지를 파악하기 위한 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a1e7a",
   "metadata": {},
   "source": [
    "정답은 2번입니다.\n",
    "\n",
    "데이터 분석은 분석의 목적과 시점에 따라 크게 4가지 단계(묘사, 진단, 예측, 처방)로 나뉩니다. 그중 **진단분석(Diagnostic Analytics)**은 문제의 **'원인'**을 파악하는 데 집중합니다.\n",
    "\n",
    "1\t처방 분석 (Prescriptive)\t어떻게 해야 하는가?\t원하는 결과를 위해 최적의 행동 방향이나 대안을 제시합니다. (최적화, 시뮬레이션)\n",
    "\n",
    "2\t진단 분석 (Diagnostic)\t왜 일어났는가?\t데이터 간의 상관관계와 인과관계를 파악하여 근본 원인을 찾아냅니다. (드릴다운, 데이터 마이닝)\n",
    "\n",
    "3\t예측 분석 (Predictive)\t무엇이 일어날 것인가?\t통계 모델과 머신러닝을 사용하여 미래의 결과를 추정합니다.\n",
    "\n",
    "4\t묘사 분석 (Descriptive)\t무엇이 일어났는가?\t과거 데이터를 요약하고 시각화하여 현재 상태를 파악합니다. (대시보드, 보고서)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cff765",
   "metadata": {},
   "source": [
    "# 42. 다음 중 비지도학습에 해당하는 것을 모두 고르시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715a6af",
   "metadata": {},
   "source": [
    "ㄱ. 사기 감지를 위해 시스템에서 신용카드의 높은 가치 거래를 감지\n",
    "\n",
    "ㄴ. 기저귀를 사면 맥주도 함께 구매함.\n",
    "\n",
    "ㄷ. 집 크기에 따른 매매비용 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f922133a",
   "metadata": {},
   "source": [
    "1. ㄱ\n",
    "2. ㄱ, ㄴ\n",
    "3. ㄴ, ㄷ\n",
    "4. ㄱ, ㄴ, ㄷ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c98cb",
   "metadata": {},
   "source": [
    "정답은 **2번(ㄱ, ㄴ)**입니다.\n",
    "\n",
    "ㄱ. 신용카드 사기 감지 (이상치 탐지): \n",
    "    \n",
    "    비지도학습 사기 거래는 데이터가 매우 적고 미리 정의하기 어렵기 때문에, 보통 '정상적인 거래' 패턴을 학습한 뒤 여기서 크게 벗어나는 **이상치(Outlier)**를 찾아내는 방식을 사용합니다. 이는 데이터의 분포를 파악하는 비지도학습의 영역입니다.\n",
    "\n",
    "ㄴ. 기저귀와 맥주의 연관성 (연관 규칙 학습): \n",
    "    \n",
    "    비지도학습 데이터 간의 상관관계를 분석하여 \"A를 사면 B도 산다\"는 규칙을 찾아내는 **연관 규칙 학습(Association Rule Learning)**은 대표적인 비지도학습입니다. 별도의 정답지 없이 데이터 간의 유사성과 빈도를 분석합니다.\n",
    "\n",
    "ㄷ. 집 크기에 따른 매매비용 예측 (회귀): \n",
    "    \n",
    "    지도학습 '집 크기'라는 입력값과 '매매비용'이라는 명확한 **정답(Label)**이 존재하는 데이터를 학습하여 미래의 값을 예측하는 방식입니다. 이는 지도학습 중 회귀(Regression) 알고리즘에 해당합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ae284",
   "metadata": {},
   "source": [
    "# 43. 다음 중 보기를 분석하기 위한 알고리즘 사례로 알맞는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d10f72b",
   "metadata": {},
   "source": [
    "0000000\n",
    "1111111\n",
    "2222222\n",
    "3333333\n",
    "4444444\n",
    "5555555\n",
    "6666666\n",
    "7777777\n",
    "8888888\n",
    "9999999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0163a",
   "metadata": {},
   "source": [
    "1. 회귀\n",
    "2. 분류\n",
    "3. 다지분류\n",
    "4. 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9975ade1",
   "metadata": {},
   "source": [
    "질문하신 문제의 정답은 **2번. 분류(Classification)**입니다.\n",
    "데이터의 특징: 보기로 주어진 데이터는 0부터 9까지의 숫자들이 나열되어 있습니다. 이는 전형적인 손글씨 숫자 데이터(MNIST 등) 또는 숫자를 이미지로 인식하여 어떤 숫자인지 판별해야 하는 상황을 의미합니다.\n",
    "\n",
    "알고리즘의 목적: 나열된 데이터를 보고 \"이 데이터는 0이다\", \"이 데이터는 1이다\"라고 미리 정해진 범주(Category) 중 하나로 결정하는 것은 분류(Classification) 알고리즘의 핵심 역할입니다.\n",
    "\n",
    "만약 범주가 2개(예: 0인지 아닌지)라면 이진 분류라고 하고,\n",
    "\n",
    "지금처럼 0~9까지 여러 범주 중 하나를 고르는 것이라면 **다중 분류(Multi-class Classification)**라고 합니다.\n",
    "1. 회귀 (Regression): 집값 예측이나 온도 예측처럼 연속적인 수치를 맞히는 알고리즘입니다. \"이 숫자는 7.5이다\"라고 예측하는 것이 아니므로 적절하지 않습니다.\n",
    "\n",
    "3. 다지분류 (Multi-label Classification): 한 데이터가 여러 개의 정답을 가질 때 사용합니다. (예: 사진 한 장에 '고양이'와 '강아지'가 동시에 있는 경우). 숫자 하나는 하나의 정답만 가지므로 일반적인 분류가 더 적합합니다.\n",
    "\n",
    "4. 예측 (Prediction): 예측은 머신러닝의 포괄적인 목적을 뜻하는 용어에 가깝습니다. 문제에서 요구하는 구체적인 '알고리즘 사례' 명칭으로는 '분류'가 가장 정확합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154cd600",
   "metadata": {},
   "source": [
    "# 44. 다음 보기를 분석하기 위한 알고리즘으로 알맞는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f719bf30",
   "metadata": {},
   "source": [
    "교복에 대한 치수와 너비를 잰 학생 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06197da1",
   "metadata": {},
   "source": [
    "1. 분류트리\n",
    "2. 군집화\n",
    "3. 회귀분석\n",
    "4. 서포트벡터머신"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e5da5",
   "metadata": {},
   "source": [
    "질문하신 문제의 정답은 **2번. 군집화(Clustering)**입니다.\n",
    "\n",
    "데이터의 성격: '교복 치수와 너비를 잰 데이터'는 각 학생의 신체 사이즈 정보를 담고 있습니다. 하지만 이 데이터에는 \"이 학생은 \n",
    "\n",
    "Small 사이즈다\" 또는 \"이 학생은 100 사이즈다\"라는 식의 정답(Label)이 처음부터 매겨져 있지 않습니다.\n",
    "\n",
    "알고리즘의 목적: 교복 제조사는 수많은 학생의 체형 데이터를 바탕으로, 체형이 유사한 학생들을 몇 개의 그룹(예: S, M, L, XL)으로 묶어서 효율적으로 생산해야 합니다. 이렇게 정답이 없는 상태에서 데이터의 유사성을 측정하여 비슷한 특징을 가진 그룹으로 나누는 것이 바로 비지도학습의 대표격인 군집화(Clustering) 알고리즘입니다.\n",
    "\n",
    "분류트리 (Classification Tree): \"이 학생은 S사이즈다\"라는 정답이 이미 있는 데이터를 학습시켜, 새로운 학생의 사이즈를 결정할 때 사용합니다. (지도학습)\n",
    "\n",
    "회귀분석 (Regression): 키를 바탕으로 몸무게라는 '연속적인 수치'를 예측할 때 사용합니다. 그룹을 나누는 목적과는 거리가 있습니다. (지도학습)\n",
    "\n",
    "서포트벡터머신 (SVM): 주로 데이터를 두 개의 카테고리로 명확하게 분류(예: 사과냐 배냐)하기 위해 경계선을 찾는 알고리즘으로 사용됩니다. (지도학습)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f94777d",
   "metadata": {},
   "source": [
    "# 45. 분석 모델 구축절차 중 모델링에 해당하는 상세 단계로 알맞는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d4195",
   "metadata": {},
   "source": [
    "ㄱ. 모델링 마트 설계 및 구축\n",
    "\n",
    "ㄴ. 모델링 모델 확정\n",
    "\n",
    "ㄷ. 모델링 성능 평가\n",
    "\n",
    "ㄹ. 탐색 전 분석 및 유의변수 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569fa959",
   "metadata": {},
   "source": [
    "1. ㄱ ㄴ ㄷ ㄹ\n",
    "2. ㄹ ㄷ ㄱ ㄴ\n",
    "3. ㄱ ㄹ ㄴ ㄷ\n",
    "4. ㄹ ㄱ ㄷ ㄴ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d9f60f",
   "metadata": {},
   "source": [
    "정답은 **3번 (ㄱ → ㄹ → ㄴ → ㄷ)**입니다\n",
    "\n",
    "ㄱ. 모델링 마트 설계 및 구축: 모델링에 사용할 데이터를 추출하고, 분석에 적합한 형태(요약 변수, 파생 변수 등)로 구성된 데이터 세트(마트)를 가장 먼저 구축해야 합니다.\n",
    "\n",
    "ㄹ. 탐색적 분석 및 유의변수 도출: 구축된 데이터 마트를 바탕으로 데이터의 특성을 파악(EDA)하고, 모델 성능에 영향을 줄 중요한 변수(Feature)를 선택합니다.\n",
    "\n",
    "ㄴ. 모델링 모델 확정: 선택된 변수와 알고리즘(회귀, 분류 등)을 적용하여 모델을 생성하고, 하이퍼파라미터 튜닝 등을 통해 최적의 모델을 결정합니다.\n",
    "\n",
    "ㄷ. 모델링 성능 평가: 확정된 모델이 얼마나 정확한지 검증 데이터(Test Data)를 통해 최종 성능을 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d01a7b",
   "metadata": {},
   "source": [
    "# 46. 데이터 분할에 대한 설명으로 거리가 먼 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a23fb5",
   "metadata": {},
   "source": [
    "1. 학습 데이터 : 모델을 적합하게 만들기 위해 사용되는 데이터\n",
    "2. 일반적으로 학습 데이터는 분석 데이터의 50%, 검증 데이터는 30%, 테스트 데이터는 20%를 사용한다.\n",
    "3. 검증 데이터 : 모델이 얼마나 적합하게 선택되었는지 평가, 일부 모델을 조정하여 구축된 모델 중 좋은 모델 선택\n",
    "3. 테스트 데이터를 반복해서 재사용하면 이는 훈련셋의 일부가 되고 모델 과소적합의 원인이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef47146d",
   "metadata": {},
   "source": [
    "질문의 정답은 **4번(지문의 두 번째 3번)**입니다.\n",
    "데이터 분할(Data Splitting)의 원칙과 각 데이터셋의 역할을 이해하면 쉽게 풀 수 있는 문제입니다. 각 보기를 하나씩 짚어보겠습니다.\n",
    "1번. 학습 데이터 (Training Data): 모델이 데이터의 패턴을 배우고 가중치를 최적화(적합)하는 데 사용하는 기초 데이터입니다. (옳음)\n",
    "\n",
    "2번. 데이터 분할 비율: 일반적으로 7:3 또는 6:2:2 (학습:검증:테스트) 비율을 많이 사용합니다. 지문에 제시된 5:3:2 비율도 일반적인 범주 내에 포함됩니다. (옳음)\n",
    "\n",
    "3번. 검증 데이터 (Validation Data): 학습된 여러 모델 중 어떤 모델이 가장 성능이 좋은지 비교하거나, 하이퍼파라미터를 튜닝하여 최적의 모델을 선택하는 데 사용합니다. (옳음)\n",
    "\n",
    "4번. 테스트 데이터 (Test Data) 재사용의 위험성:\n",
    "\n",
    "테스트 데이터를 반복해서 재사용하여 모델을 수정하면, 모델이 테스트 데이터의 정답에 맞춰지게 됩니다.\n",
    "\n",
    "이 경우 모델이 실제 데이터에 대해 유연성을 잃고 특정 데이터에만 과하게 맞춰지는 **과대적합(Overfitting)**이 발생합니다. 지문의 '과소적합(Underfitting)' 설명은 잘못된 부분입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e629251",
   "metadata": {},
   "source": [
    "# 47. K-폴드 교차검증(K-fold cross- Validation)에 대한 설명으로 틀린 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0ce1c",
   "metadata": {},
   "source": [
    "1. 홀드 아웃에 비해 훈련 데이터 분할에 덜 민감한 성능 추정을 얻을 수 있다.\n",
    "2. 데이터가 충분하지 않은 경우 사용한다.\n",
    "3. 훈련 데이터가 작다면 Fold 갯수를 줄이는 것이 좋다.\n",
    "4. 대체로 학습 데이터와 검증 데이터만 사용하는 것 보다 높은 정확도를 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae7a97",
   "metadata": {},
   "source": [
    "정답은 3번입니다.\n",
    "\n",
    "K-폴드 교차검증의 원리와 데이터 크기에 따른 효율성을 이해하는 것이 중요합니다. 각 보기를 상세히 분석해 드릴게요.\n",
    "\n",
    "1번. 홀드 아웃 대비 안정성: 홀드 아웃(데이터를 한 번만 나누는 방식)은 데이터를 어떻게 나누느냐에 따라 성능 결과가 크게 달라질 수 있습니다. 반면 K-폴드는 데이터를 골고루 번갈아 가며 학습/검증에 사용하므로 분할 방식에 따른 성능 편차가 적고 안정적입니다. (옳음)\n",
    "\n",
    "2번. 데이터 부족 시 유용성: 데이터가 적을 때 홀드 아웃을 쓰면 학습 데이터 자체가 너무 부족해집니다. K-폴드는 모든 데이터를 한 번씩은 검증에 사용할 수 있어 데이터 활용도가 매우 높습니다. (옳음)\n",
    "\n",
    "3번. 훈련 데이터가 작을 때 Fold(K) 값 설정:\n",
    "\n",
    "훈련 데이터가 적을수록 Fold의 개수(K)를 늘리는 것이 좋습니다. * K값을 늘리면 각 반복 회차에서 학습에 사용하는 데이터의 양이 많아지기 때문입니다. (예: K=10이면 전체의 90%를 학습에 사용, K=3이면 66%만 사용)\n",
    "\n",
    "극단적으로 데이터가 적을 때 사용하는 방식이 데이터 개수만큼 Fold를 나누는 **LOOCV(Leave-One-Out Cross-Validation)**입니다. 따라서 3번 설명은 틀렸습니다. (틀림)\n",
    "\n",
    "4번. 높은 정확도(신뢰도): 모든 데이터를 학습과 검증에 참여시키기 때문에, 단일 분할 방식보다 모델의 일반화 성능을 더 정확하게 측정할 수 있으며 보통 더 신뢰할 수 있는 결과를 줍니다. (옳음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29f61e",
   "metadata": {},
   "source": [
    "# 48. 다음 중 회귀분석 가정에 대한 문제로 아래 그래프가 의미하는 성질은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c12db6",
   "metadata": {},
   "source": [
    "|-------------------\n",
    "|o o o o o o\n",
    "| o o o o o\n",
    "|------------------->\n",
    "| o o o o o\n",
    "|o o o o o o\n",
    "|-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dabe81d",
   "metadata": {},
   "source": [
    "1. 독립성\n",
    "2. 등분산성\n",
    "3. 정산성\n",
    "4. 비산관성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb864c7",
   "metadata": {},
   "source": [
    "질문에 제시된 텍스트 그래프가 나타내는 정답은 2. 등분산성입니다.\n",
    "\n",
    "보내주신 텍스트 그림은 잔차(Residuals)의 분포를 시각화한 것인데, 이를 해석하면 다음과 같습니다.\n",
    "\n",
    "1. 왜 '등분산성'인가요?\n",
    "제시된 그래프를 보면 독립변수(X축)의 값이 커지더라도, 오차(잔차)들이 흩어진 정도(폭)가 일정하게 유지되고 있습니다.\n",
    "\n",
    "등분산성(Homoscedasticity): 모든 독립변수 값에 대하여 오차항의 분산이 일정한 성질입니다.\n",
    "\n",
    "이분산성(Heteroscedasticity): 아래 그림처럼 데이터가 뒤로 갈수록 넓게 퍼지거나(깔때기 모양) 좁아지는 경우를 말하며, 이때는 등분산성 가정을 위반한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd285811",
   "metadata": {},
   "source": [
    "# 49. 모든 변수 적용 후 하나씩 변수를 제거하는 방법은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae1db6",
   "metadata": {},
   "source": [
    "1. 층화추출법\n",
    "2. 단계적방법\n",
    "3. 전진선택법\n",
    "4. 후진선택법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79038e71",
   "metadata": {},
   "source": [
    "정답은 **4번. 후진제거법(후진선택법, Backward Elimination)**입니다.\n",
    "4. 후진제거법 (Backward Elimination): * 방법: 모델에 모든 후보 변수를 넣고 시작합니다.\n",
    "\n",
    "과정: 영향력이 가장 적은(통계적으로 유의하지 않은) 변수를 하나씩 제거해 나가며, 더 이상 제거할 변수가 없을 때 멈춥니다.\n",
    "\n",
    "특징: 변수가 너무 많을 때는 계산량이 많아질 수 있지만, 변수 간의 상호작용을 초기에 고려할 수 있다는 장점이 있습니다.\n",
    "\n",
    "층화추출법 (Stratified Sampling): 변수 선택 기법이 아니라, 데이터를 나눌 때 특정 그룹의 비율을 유지하며 뽑는 표본 추출 방법입니다.\n",
    "\n",
    "단계적 방법 (Stepwise Selection): 전진선택과 후진제거를 결합한 방식입니다. 변수를 추가하면서, 기존에 추가된 변수가 새로운 변수의 등장으로 인해 중요도가 낮아지면 다시 제거하는 과정을 반복합니다.\n",
    "\n",
    "전진선택법 (Forward Selection): 변수가 하나도 없는 상태에서 시작하여, 가장 유의한 변수를 하나씩 추가해 나가는 방식입니다. 한 번 추가된 변수는 나중에 중요도가 낮아져도 제거되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599a13c",
   "metadata": {},
   "source": [
    "# 50. 라쏘(Lasso) 희귀 모델에 대한 설명 중 옳지 않는 것은? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b70e97",
   "metadata": {},
   "source": [
    "1. 가중치 벡터의 L1 norm을 사용한다.\n",
    "2. 공식은 $$J(W) = \\text{MSE}(W) + \\lambda \\sum_{j=1}^{p} |w_j|$$로 도출할 수 있다.\n",
    "3. 규제가 적용된 선형희귀이다.\n",
    "4. L2 norm 제곱을 나눠 규제항으로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a98ec",
   "metadata": {},
   "source": [
    "정답은 4번입니다\n",
    "\n",
    "1. 가중치 벡터의 L1 norm을 사용한다. (옳음)\n",
    "\n",
    "**L1 노름(norm)**은 벡터 성분들의 절대값 합을 의미합니다. 라쏘는 이 L1 노름을 패널티로 사용하여 모델의 복잡도를 제어합니다.\n",
    "\n",
    "2. 공식 (옳음)\n",
    "\n",
    "3. 규제가 적용된 선형회귀이다. (옳음)\n",
    "\n",
    "일반적인 선형회귀(OLS)는 데이터에 너무 과하게 맞춰져 과대적합(Overfitting)이 발생하기 쉽습니다. 라쏘는 이를 방지하기 위해 가중치 크기에 제한을 두는 '규제(Regularization)' 기법을 도입한 선형회귀의 한 종류입니다.\n",
    "\n",
    "4. L2 norm 제곱을 나눠 규제항으로 사용한다. (틀림)\n",
    "\n",
    "L2 노름의 제곱($\\sum w_j^2$)을 규제항으로 사용하는 것은 라쏘가 아니라 릿지(Ridge) 회귀입니다. \n",
    "\n",
    "라쏘는 오직 L1 노름(절대값 합)만을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1acac",
   "metadata": {},
   "source": [
    "# 51. 반응변수가 범주형일 때 사용하는 희귀분석으로 알맞는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8227aa",
   "metadata": {},
   "source": [
    "1. 로지스틱회귀분석\n",
    "2. 선형희귀분석\n",
    "3. 규제가 있는 선형희귀분석\n",
    "4. 비선형희귀분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b6309e",
   "metadata": {},
   "source": [
    "반응변수(종속변수)가 범주형(예: 성공/실패, 예/아니오, 합격/불합격 등)일 때 사용하는 회귀분석으로 가장 적절한 것은 **1번 로지스틱 회귀분석(Logistic Regression)**입니다.\n",
    "\n",
    "로지스틱 회귀분석 (Logistic Regression):\n",
    "\n",
    "종속변수가 이진형(0 또는 1)이거나 범주형일 때 사용합니다.\n",
    "\n",
    "선형 회귀와 달리 출력값이 0과 1 사이의 확률값으로 제한되며, 특정 임계값을 기준으로 분류를 수행합니다.\n",
    "\n",
    "선형 회귀분석 (Linear Regression):\n",
    "\n",
    "종속변수가 연속형 숫자(예: 키, 몸무게, 매출액 등)일 때 사용합니다. 반응변수가 범주형일 때는 적합하지 않습니다.\n",
    "\n",
    "규제가 있는 선형 회귀분석 (Regularized Linear Regression):\n",
    "\n",
    "릿지(Ridge)나 라쏘(Lasso) 회귀 등을 의미하며, 선형 회귀에서 과적합을 방지하기 위해 사용됩니다. 역시 연속형 변수를 다룹니다.\n",
    "\n",
    "비선형 회귀분석 (Non-linear Regression):\n",
    "\n",
    "독립변수와 종속변수의 관계가 직선이 아닌 곡선 형태의 함수로 표현될 때 사용하며, 주로 연속형 데이터를 분석합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27acab",
   "metadata": {},
   "source": [
    "# 52. 인공신경망에서 학습을 통해 정해지는 값으로 가장 알맞는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697819c7",
   "metadata": {},
   "source": [
    "1. 커널값\n",
    "2. 뉴런값\n",
    "3. 가중치\n",
    "4. 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2aec9",
   "metadata": {},
   "source": [
    "인공신경망에서 학습(Training) 과정을 통해 최적의 값을 찾아가는 가장 핵심적인 요소는 **3번 가중치(Weight)**입니다.\n",
    "인공신경망의 학습 목적은 입력 데이터에 대해 정확한 정답을 출력할 수 있도록 모델 내부의 매개변수(Parameter)를 조정하는 것입니다.\n",
    "\n",
    "가중치 (Weight): 각 입력 신호가 결과에 주는 영향력을 조절하는 요소입니다. 학습 과정에서 오차를 줄이기 위해 끊임없이 업데이트되는 가장 중요한 '학습되는 값'입니다. (보통 **편향(Bias)**과 함께 학습됩니다.)\n",
    "\n",
    "커널값 (Kernel): 합성곱 신경망(CNN) 등에서 가중치 묶음으로 사용되기도 하지만, 넓은 의미에서는 가중치의 한 형태이며 일반적인 정답으로는 '가중치'가 더 포괄적이고 적절합니다.\n",
    "\n",
    "뉴런값 (Node/Neuron Value): 각 층의 뉴런이 가지는 값은 입력값과 가중치가 계산되어 나온 결과물(활성화 값)이지, 학습을 통해 직접적으로 정해지는 매개변수가 아닙니다.\n",
    "\n",
    "오차 (Error/Loss): 모델의 예측값과 실제 정답의 차이를 나타내는 지표입니다. 오차를 최소화하기 위해 가중치를 학습시키는 것이지, 오차 자체가 학습으로 정해지는 결과 목적물은 아닙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee5b7f",
   "metadata": {},
   "source": [
    "# 53. 서포트벡터머신의 설명으로 틀린것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d695bb86",
   "metadata": {},
   "source": [
    "1. 분류와 회귀분석에 사용된는 지도학습 알고리즘이다.\n",
    "2. 데이처가 사상된 공간에서 경계선과 가장 근접한 데이터를 서포트 벡터라고 한다.\n",
    "3. 데스트가 매우 쉽다.\n",
    "4. 고차원에서의 특징 추출이 오려운 경우 차워의 저주를 회피한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680c4d6",
   "metadata": {},
   "source": [
    "서포트 벡터 머신(SVM)의 설명 중 틀린 것은 **3번 \"테스트가 매우 쉽다(또는 모델 생성이 매우 쉽다)\"**입니다.\n",
    "\n",
    "분류와 회귀분석에 사용되는 지도학습 알고리즘이다. (O)\n",
    "\n",
    "SVM은 주로 분류(Classification) 문제에 쓰이지만, SVR(Support Vector Regression)을 통해 회귀 분석에도 널리 사용됩니다.\n",
    "\n",
    "데이터가 사상된 공간에서 경계선과 가장 근접한 데이터를 서포트 벡터라고 한다. (O)\n",
    "\n",
    "결정 경계(Hyperplane)와 가장 가까이 있는 데이터 포인트들을 '서포트 벡터'라고 부르며, 이들이 결정 경계를 정의하는 결정적인 역할을 합니다.\n",
    "\n",
    "테스트가 매우 쉽다. (X)\n",
    "\n",
    "SVM은 모델 학습 시 데이터 양이 많아질수록 계산 복잡도가 급격히 증가하며, \n",
    "\n",
    "최적의 하이퍼파라미터($C, \\gamma$)와 커널(Kernel)을 찾는 과정이 까다롭고 시간이 많이 소요됩니다. \n",
    "\n",
    "또한 결과에 대한 해석(Explainability)이 선형 회귀 등에 비해 어렵습니다.\n",
    "\n",
    "고차원에서의 특징 추출이 어려운 경우 차원의 저주를 회피한다. (O)\n",
    "\n",
    "SVM은 커널 트릭(Kernel Trick)을 사용하여 저차원 데이터를 고차원 공간으로 보낸 뒤 분리합니다. \n",
    "\n",
    "이 과정에서 모든 차원을 직접 계산하지 않고도 효율적으로 경계를 찾기 때문에 차원의 저주 문제를 비교적 잘 극복하는 알고리즘으로 평가받습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a0b36d",
   "metadata": {},
   "source": [
    "# 54. K-평균군집(K-Means Clusterting)의 K를 구하는 방법으로 알맞는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555290ba",
   "metadata": {},
   "source": [
    "1. 엘보우메소드(Elbow Method)\n",
    "2. 정밀도(Precision)\n",
    "3. 재현율(Recall)\n",
    "4. 던지수(Dunn index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e8f99",
   "metadata": {},
   "source": [
    "K-평균 군집분석(K-Means Clustering)에서 최적의 군집 수인 K를 결정하는 가장 대표적인 방법은 **1번 엘보우 메소드(Elbow Method)**입니다.\n",
    "\n",
    "엘보우 메소드 (Elbow Method):\n",
    "\n",
    "군집 내 오차 제곱합(WSS, Within-Cluster Sum of Squares)을 구하여 K값에 따른 변화를 그래프로 그립니다.\n",
    "\n",
    "K가 증가할수록 오차는 줄어드는데, 어느 지점부터는 오차 감소 폭이 급격히 줄어들며 그래프가 마치 **'팔꿈치(Elbow)'**처럼 꺾이는 지점이 발생합니다. 이 지점의 K를 최적의 군집 수로 판단합니다.\n",
    "\n",
    "정밀도 (Precision) & 3. 재현율 (Recall):\n",
    "\n",
    "이 두 가지는 주로 분류(Classification) 모델의 성능을 평가할 때 사용하는 지표입니다. 군집 분석(비지도 학습)보다는 지도 학습에서 중요하게 다뤄집니다.\n",
    "\n",
    "던 지수 (Dunn Index):\n",
    "\n",
    "군집 분석의 품질을 평가하는 지표 중 하나입니다. 군집 간 거리의 최솟값을 군집 내 거리의 최댓값으로 나눈 비율로, 이 값이 클수록 군집화가 잘 되었다고 판단합니다. K를 결정하는 직접적인 '방법론'의 명칭으로는 엘보우 메소드가 더 보편적입니다. (실루엣 계수와 함께 군집 타당성 지표로 사용됩니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a9774",
   "metadata": {},
   "source": [
    "# 55. 제시한 데이터를 사용할 수 있는 분석기법은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b9819",
   "metadata": {},
   "source": [
    "Matrix\t서울\t인천\t부산\t대구\t대전\t청주\t춘천\t광주\t전주\n",
    "서울\t0\t\t\t\t\t\t\t\t\n",
    "인천\t28km\t0\t\t\t\t\t\t\t\n",
    "부산\t310km\t320km\t0\t\t\t\t\t\t\n",
    "대구\t220km\t225km\t97km\t0\t\t\t\t\t\n",
    "대전\t130km\t135km\t188km\t105km\t0\t\t\t\t\n",
    "청주\t95km\t110km\t202km\t114km\t34km\t0\t\t\t\n",
    "춘천\t88km\t120km\t300km\t250km\t125km\t288km\t0\t\t\n",
    "광주\t245km\t228km\t177km\t159km\t130km\t159km\t288km\t0\t\n",
    "전주\t175km\t170km\t180km\t120km\t55km\t88km\t200km\t72km\t0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7da670",
   "metadata": {},
   "source": [
    "1. 군집분석\n",
    "2. 시계열분석\n",
    "3. 다차원분석법\n",
    "4. 비정형데이터분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2397d8",
   "metadata": {},
   "source": [
    "제시된 데이터(도시 간 거리를 나타내는 거리 행렬)를 분석하는 데 가장 적절한 기법은 **3번 다차원 분석법(다차원 척도법, Multidimensional Scaling, MDS)**입니다\n",
    "다차원 분석법 (MDS):\n",
    "\n",
    "객체 간의 거리가 주어졌을 때, 이 거리 관계를 유지하면서 객체들을 2차원 또는 3차원 공간상에 점으로 배치하는 분석 기법입니다.\n",
    "\n",
    "제시된 데이터처럼 도시 간 실제 거리(비유사성) 데이터를 바탕으로 각 도시의 상대적인 위치를 시각화하여 지도를 그리듯 표현할 때 주로 사용됩니다.\n",
    "\n",
    "군집분석 (Cluster Analysis):\n",
    "\n",
    "데이터 간의 유사성을 바탕으로 비슷한 것끼리 그룹을 묶는 기법입니다. 거리 데이터를 사용할 수는 있지만, 위와 같은 행렬 데이터의 전체적인 구조를 시각화하는 핵심 목적에는 MDS가 더 적합합니다.\n",
    "\n",
    "시계열분석 (Time Series Analysis):\n",
    "\n",
    "시간의 흐름에 따라 변하는 데이터를 분석하는 기법입니다. 위 데이터는 시간에 따른 변화가 없는 정적인 거리 데이터이므로 해당하지 않습니다.\n",
    "\n",
    "비정형 데이터 분석 (Unstructured Data Analysis):\n",
    "\n",
    "텍스트, 이미지, 음성 등 정해진 규격이 없는 데이터를 분석하는 기법입니다. 위 데이터는 숫자와 표로 이루어진 전형적인 정형 데이터(행렬)입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb51b03",
   "metadata": {},
   "source": [
    "# 56. 다음 중 시계열 분석기법에서 사용모델로 가장 거리가 먼 것은? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b37ad8",
   "metadata": {},
   "source": [
    "1. 분해법\n",
    "2. 이항분포\n",
    "3. 자기회귀\n",
    "4. 이동평균"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d986719",
   "metadata": {},
   "source": [
    "시계열 분석 기법과 가장 거리가 먼 것은 **2번 이항분포(Binomial Distribution)**입니다.\n",
    "\n",
    "분해법 (Decomposition Method):\n",
    "\n",
    "시계열 데이터를 추세(Trend), 계절성(Seasonal), 순환(Cyclical), 불규칙(Irregular) 요인으로 분리하여 분석하는 시계열의 고전적인 방법입니다.\n",
    "\n",
    "이항분포 (Binomial Distribution):\n",
    "\n",
    "결과가 두 가지(성공/실패)뿐인 독립적인 시행을 $n$번 반복했을 때 성공 횟수의 확률 분포를 나타내는 일반 통계학의 확률 분포입니다. 시계열 분석의 핵심인 '시간에 따른 연속적 종속성'과는 직접적인 관련이 없는 이산 확률 모델입니다.\n",
    "\n",
    "자기회귀 (AutoRegression, AR):\n",
    "\n",
    "현재의 시계열 값이 과거의 자신의 값들에 의해 결정된다고 가정하는 모델입니다. 시계열 분석의 가장 기본적이고 핵심적인 확률 모델 중 하나입니다.\n",
    "\n",
    "이동평균 (Moving Average, MA):\n",
    "\n",
    "과거의 예측 오차(White Noise)들을 이용하여 현재의 값을 설명하는 모델입니다. 데이터의 변동을 매끄럽게(Smoothing) 하여 추세를 파악하는 데에도 널리 쓰입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ef1d4",
   "metadata": {},
   "source": [
    "# 57. 베이즈 정러에 대해 아래 빈 칸에 알맞은 내용을 넣으시오. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7bfff",
   "metadata": {},
   "source": [
    "          (빈칸)P(B|A)\n",
    "P(A|B) = ---------------------\n",
    "              P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800fffce",
   "metadata": {},
   "source": [
    "1. P(A)\n",
    "2. P(B)\n",
    "3. P(B교집합A)\n",
    "4. P(A합집합B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f11e3",
   "metadata": {},
   "source": [
    "빈칸에 들어갈 알맞은 내용은 **1번 $P(A)$**입니다.\n",
    "베이즈 정리 (Bayes' Theorem) 공식베이즈 정리는 새로운 정보나 증거($B$)가 주어졌을 때, 어떤 사건의 확률($A$)이 어떻게 변하는지를 나타내는 정리입니다. 완성된 공식은 다음과 같습니다.$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "$P(A|B)$ (사후 확률, Posterior): 사건 $B$가 발생했다는 조건 하에 사건 $A$가 발생할 확률입니다\n",
    "\n",
    "$P(B|A)$ (가능도/우도, Likelihood): 사건 $A$가 발생했을 때 사건 $B$가 나타날 확률입니다\n",
    "\n",
    "$P(A)$ (사전 확률, Prior): 사건 $B$가 조사되기 전, 이미 알고 있는 사건 $A$의 초기 확률입니다. (빈칸의 정답)\n",
    "\n",
    "$P(B)$ (증거, Evidence): 사건 $B$가 발생할 전체 확률입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ae506",
   "metadata": {},
   "source": [
    "# 58. 합성곱신경망(CNN) 알고리즘에서 6x6 Input Data에 대해 3x3의 Filter를 거치는 특징맴(피처맵)의 크기를 계산하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0fb00f",
   "metadata": {},
   "source": [
    "1. 2 x 2\n",
    "2. 3 x 3\n",
    "3. 4 x 4\n",
    "4. 5 x 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57f4d2",
   "metadata": {},
   "source": [
    "특징맵(Feature Map)의 크기를 계산하는 공식을 적용하면 정답은 3번 4 x 4입니다.\n",
    "\n",
    "성곱(Convolution) 연산 후 출력되는 데이터의 크기는 아래 공식으로 구할 수 있습니다. (단, 별도의 스트라이드($S$)나 패딩($P$)에 대한 언급이 없으므로 기본값인 $S=1, P=0$을 적용합니다.)$$Output = \\frac{Input - Filter + 2 \\times Padding}{Stride} + 1$$\n",
    "\n",
    "Input Size ($I$): 6Filter Size ($F$): 3Padding ($P$): 0Stride ($S$): 1$$Output = \\frac{6 - 3 + 0}{1} + 1 = 3 + 1 = 4$$따라서 결과값은 4 x 4가 됩니다\n",
    "\n",
    "패딩(Padding): 만약 입력 데이터와 출력 데이터의 크기를 같게 유지하고 싶다면(6x6 유지), 가장자리에 0을 채워 넣는 패딩 과정을 거치게 됩니다.\n",
    "\n",
    "스트라이드(Stride): 필터가 한 번에 이동하는 칸 수입니다. 스트라이드가 커질수록 출력되는 특징맵의 크기는 작아집니다.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28faef3b",
   "metadata": {},
   "source": [
    "# 59. 다음 중 앙상블 기법이 아닌 것은? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21fe769",
   "metadata": {},
   "source": [
    "1. 시그모이드\n",
    "2. 배깅\n",
    "3. 랜덤 포레스트\n",
    "4. 부스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfbb2d",
   "metadata": {},
   "source": [
    "앙상블(Ensemble) 기법이 아닌 것은 **1번 시그모이드(Sigmoid)**입니다.\n",
    "\n",
    "시그모이드 (Sigmoid): (정답)\n",
    "\n",
    "시그모이드는 앙상블 기법이 아니라, 주로 로지스틱 회귀나 인공신경망에서 사용되는 **활성화 함수(Activation Function)**입니다. 입력값을 0과 1 사이의 값으로 변환하여 확률로 표현하거나 비선형성을 추가하는 역할을 합니다.\n",
    "\n",
    "배깅 (Bagging):\n",
    "\n",
    "'Bootstrap Aggregating'의 약자로, 훈련 데이터를 무작위로 복원 추출하여 여러 개의 모델을 만들고 그 결과를 집계(투표 또는 평균)하는 앙상블 기법입니다.\n",
    "\n",
    "랜덤 포레스트 (Random Forest):\n",
    "\n",
    "배깅 기법의 대표적인 알고리즘으로, 여러 개의 의사결정나무(Decision Tree)를 만들어서 다수결로 결과를 결정하는 강력한 앙상블 모델입니다.\n",
    "\n",
    "부스팅 (Boosting):\n",
    "\n",
    "여러 개의 약한 학습기(Weak Learner)를 순차적으로 학습시키되, 이전 모델이 틀린 데이터에 더 큰 가중치를 두어 성능을 보완해 나가는 앙상블 기법입니다. (예: AdaBoost, Gradient Boost, XGBoost 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda57bf",
   "metadata": {},
   "source": [
    "# 60. 여러 개의 약한 학습기(Week Leaner)를 순차적으로 학습/예측하면서 잘못 예측한 데이터에 가중치를 부여해 오류를 개선해 나가는 학습으로 모델 개수가 많을수록 정확도가 높은 방식은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70e6c6",
   "metadata": {},
   "source": [
    "1. 베깅 - 아나부스트\n",
    "2. 랜덤포레스트 - 베깅\n",
    "3. 부스팅 - 랜덤포레스트\n",
    "4. 부스팅 - GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362893ab",
   "metadata": {},
   "source": [
    "질문하신 내용에 가장 부합하는 정답은 4번 부스팅 - GBM입니다.\n",
    "\n",
    "부스팅 (Boosting): 약한 학습기를 순차적으로 학습시키며, 이전 단계에서 틀린 데이터에 가중치를 부여하여 다음 학습기가 이를 더 잘 맞추도록 개선해 나가는 방식입니다.\n",
    "\n",
    "GBM (Gradient Boosting Machine): 부스팅 기법의 대표적인 알고리즘으로, 가중치 업데이트를 '경사하강법(Gradient Descent)'을 통해 수행합니다. 모델(Tree)의 개수가 많아질수록 오차를 계속 줄여나가기 때문에 일반적으로 정확도가 매우 높습니다.\n",
    "\n",
    "베깅 - 아나부스트: '아나부스트'는 **에이다부스트(AdaBoost)**를 잘못 표기한 것으로 보이며, 이는 부스팅 기법입니다. 하지만 앞의 '베깅'과 묶여 있어 틀린 조합입니다.\n",
    "\n",
    "랜덤포레스트 - 베깅: 랜덤포레스트는 베깅(Bagging) 방식을 사용하지만, 이는 여러 모델을 병렬적으로 독립 학습시키는 방식입니다. 문제에서 언급한 '순차적 학습' 및 '오류 가중치 부여'와는 거리가 멉니다.\n",
    "\n",
    "부스팅 - 랜덤포레스트: 부스팅은 맞지만, 랜덤포레스트는 부스팅이 아닌 베깅 계열의 알고리즘입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6ceb5",
   "metadata": {},
   "source": [
    "# 61. ROC에서 완벽한 모델인 경우 x,y값은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b27319",
   "metadata": {},
   "source": [
    "1. 0, 0\n",
    "2. 0, 1\n",
    "3. 1, 0\n",
    "4. 1, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4301bf13",
   "metadata": {},
   "source": [
    "ROC 커브에서 완벽한 성능을 가진 모델의 좌표는 **2번 (0, 1)**입니다.\n",
    "\n",
    "ROC 커브(Receiver Operating Characteristic Curve)는 모델의 분류 성능을 평가하기 위해 가로축(X)과 세로축(Y)에 다음의 지표를 배치합니다.\n",
    "\n",
    "X축 (1 - 특이도, FPR): 실제 음성인 것 중 양성으로 잘못 예측한 비율 (오답률)\n",
    "\n",
    "Y축 (재현율, TPR): 실제 양성인 것 중 양성으로 올바르게 예측한 비율 (정답률)\n",
    "\n",
    "왜 (0, 1)이 완벽한 모델인가요?\n",
    "\n",
    "X = 0: 틀린 것이 하나도 없어야 하므로 FPR(False Positive Rate)이 0이어야 합니다.\n",
    "\n",
    "Y = 1: 맞는 것은 모두 다 맞혀야 하므로 TPR(True Positive Rate)이 1이어야 합니다.\n",
    "\n",
    "따라서 그래프의 **왼쪽 꼭대기(좌상단)**인 (0, 1) 지점에 점이 찍힐 때 가장 완벽한 분류 성능을 가졌다고 판단합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3499b",
   "metadata": {},
   "source": [
    "# 62. 실제 False 100개 중 40개를 False로 예측했을때, 특이도에 해당하는 값은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f75927e",
   "metadata": {},
   "source": [
    "1. 4/10\n",
    "2. 6/10\n",
    "3. 0\n",
    "4. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da86f1",
   "metadata": {},
   "source": [
    "정답은 **1번 4/10 (0.4)**입니다.\n",
    "\n",
    "특이도(Specificity)의 정의를 정확히 이해하는 것이 중요합니다.\n",
    "\n",
    "특이도(Specificity): 실제 **False(음성)**인 전체 데이터 중에서 모델이 False(음성)라고 올바르게 맞춘 비율을 의미합니다.\n",
    "\n",
    "실제 False의 총수: 100개True Negative (TN): 실제 False를 False로 맞게 예측한 수 = 40개특이도 공식: $$\\text{Specificity} = \\frac{TN}{\\text{Actual False}} = \\frac{40}{100} = \\frac{4}{10}$$\n",
    "\n",
    "따라서 계산된 값은 0.4이므로 정답은 1번입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a994e9",
   "metadata": {},
   "source": [
    "# 63. 다음 중 ROC Curve에 대한 설명으로 맞는 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3900a3",
   "metadata": {},
   "source": [
    "1. 진양성율(True Positive Rate, TPR) = 민감도 = 1 - 음성율\n",
    "2. 양성율이란 True인 케이스에 대해 True로 맞게 예측한 비율이다.\n",
    "3. 위양성율(False Positive Rate, FPR) = 1 - 정밀도\n",
    "4. 위양성율이란 False인 케이스에 대해 False로 예측한 비율이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf6b73b",
   "metadata": {},
   "source": [
    "ROC 커브에 대한 설명 중 옳은 것은 **2번 \"양성율(True Positive Rate, TPR)이란 True인 케이스에 대해 True로 맞게 예측한 비율이다.\"**입니다.\n",
    "\n",
    "진양성율(TPR) = 민감도 = 1 - 음성율 (X)\n",
    "\n",
    "진양성율(TPR)은 **민감도(Sensitivity)**와 같은 말입니다. 하지만 이는 **1 - 위음성율(FNR)**로 표현해야 맞습니다. (음성율이라는 표현은 모호하며, 보통 '특이도'를 의미하는 경우가 많습니다.)\n",
    "\n",
    "양성율(TPR)이란 True인 케이스에 대해 True로 맞게 예측한 비율이다. (O)\n",
    "\n",
    "이것이 바로 **민감도(Sensitivity)**의 정의입니다. 실제 양성(Positive)인 전체 데이터 중에서 모델이 양성이라고 정확히 판정한 비율을 의미합니다.\n",
    "\n",
    "위양성율(FPR) = 1 - 정밀도 (X)\n",
    "\n",
    "**위양성율(FPR)**은 **1 - 특이도(Specificity)**입니다. 정밀도(Precision)와는 관계가 없습니다.\n",
    "\n",
    "위양성율(FPR)이란 False인 케이스에 대해 False로 예측한 비율이다. (X)\n",
    "\n",
    "위양성율은 실제 **False(음성)**인 케이스에 대해 모델이 True(양성)라고 잘못 예측한 비율입니다. 실제 False를 False로 잘 예측한 비율은 **특이도(Specificity)**입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f2bda6",
   "metadata": {},
   "source": [
    "# 64. 다음 중 혼동행렬에서 도출 가능한 평가지표가 아닌것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a3934",
   "metadata": {},
   "source": [
    "1. 정밀도\n",
    "2. 재현율\n",
    "3. 신뢰도\n",
    "4. 특이도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea7f34",
   "metadata": {},
   "source": [
    "혼동행렬(Confusion Matrix)에서 도출 가능한 평가지표가 아닌 것은 **3번 신뢰도(Reliability/Confidence)**입니다.\n",
    "\n",
    "정밀도 (Precision): 모델이 **Positive(양성)**라고 예측한 것 중 실제로 양성인 비율입니다.공식: $TP / (TP + FP)$\n",
    "\n",
    "재현율 (Recall): 실제 **Positive(양성)**인 것 중 모델이 양성이라고 맞게 예측한 비율입니다. (민감도와 같습니다.)공식: $TP / (TP + FN)$\n",
    "\n",
    "신뢰도 (Confidence/Reliability): * 일반적으로 혼동행렬의 기본 평가지표(정밀도, 재현율, 정확도, F1-스코어 등)에는 포함되지 않습니다.\n",
    "\n",
    "통계적 가설 검정이나 연관규칙 분석 등에서 사용하는 용어이며, 분류 모델의 혼동행렬 분석에서는 쓰이지 않는 용어입니다.\n",
    "\n",
    "특이도 (Specificity): 실제 **Negative(음성)**인 것 중 모델이 음성이라고 맞게 예측한 비율입니다.공식: $TN / (TN + FP)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928de91",
   "metadata": {},
   "source": [
    "# 65. 다음 중 혼동행렬에서 도출가능한 평가지표가 아닌것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908f492",
   "metadata": {},
   "source": [
    "\t\t예측값\t\n",
    "\t\tPositive\tNagative\n",
    "실제값\tPositive\t30\t10\n",
    "\tNagative\t20\t40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc8b9a",
   "metadata": {},
   "source": [
    "1. 1/3\n",
    "2. 2/3\n",
    "3. 1\n",
    "4. 3/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aafe8f",
   "metadata": {},
   "source": [
    "도출된 값은 1/3, 2/3, 3/4이며, **3번 \"1\"**은 해당 행렬에서 어떤 지표로도 도출되지 않습니다. 따라서 정답은 3번입니다.\n",
    "\n",
    "TP (True Positive): 실제 Positive를 Positive로 맞춤 = 30\n",
    "\n",
    "FN (False Negative): 실제 Positive를 Negative로 틀림 = 10\n",
    "\n",
    "FP (False Positive): 실제 Negative를 Positive로 틀림 = 20\n",
    "\n",
    "TN (True Negative): 실제 Negative를 Negative로 맞춤 = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b02d9",
   "metadata": {},
   "source": [
    "# 66. 다음 중 데이터 관측값 영항력을 진단하는 방법이 아닌것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844451e6",
   "metadata": {},
   "source": [
    "1. 쿡의거리(Cook's Distance)\n",
    "2. DFBETAS\n",
    "3. DFFITS\n",
    "4. 샤피로-월크(Shapiro-Wilk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddcd916",
   "metadata": {},
   "source": [
    "정답은 4번 샤피로-윌크(Shapiro-Wilk) 검정입니다.\n",
    "\n",
    "쿡의 거리 (Cook's Distance): 특정 관측값을 제외했을 때 모델의 회귀계수가 얼마나 변하는지 종합적으로 측정하는 수치입니다. 값이 클수록 모델에 큰 영향을 미치는 '영향점'으로 판단합니다.\n",
    "\n",
    "DFBETAS: 특정 관측값이 개별 회귀계수($\\beta$) 각각에 미치는 영향력을 나타냅니다.\n",
    "\n",
    "DFFITS: 특정 관측값이 해당 데이터의 예측값(Fit)에 미치는 영향력을 나타냅니다.\n",
    "\n",
    "샤피로-윌크 (Shapiro-Wilk): (정답)\n",
    "\n",
    "데이터가 얼마나 정규 분포를 따르는지 확인하는 정규성 검정(Normality Test) 방법입니다.\n",
    "\n",
    "이는 모델의 가정을 확인하는 절차이지, 개별 관측값의 영향력을 측정하는 지표가 아닙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750fd15",
   "metadata": {},
   "source": [
    "# 67. 다음 중 군집분석 모델의 진단방법이 아닌 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a9b33",
   "metadata": {},
   "source": [
    "1. 샤크로-월크 검정\n",
    "2. 단순계산법\n",
    "3. 던지수\n",
    "4. 옐로우메소드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b32541e",
   "metadata": {},
   "source": [
    "정답은 1번. 샤크로-월크 검정입니다. (정확한 명칭은 샤피로-윌크 검정)\n",
    "\n",
    "샤크로-월크 검정\n",
    "(Shapiro-Wilk)\n",
    "\n",
    "[오답] 데이터가 정규분포를 따르는지 확인하는 '정규성 검정' 도구입니다. 군집의 품질을 직접 진단하는 모델 진단 방법은 아닙니다\n",
    "\n",
    "단순계산법\t주로 군집 내의 오차 제곱합(SSE) 등을 직접 계산하여 군집의 응집도를 확인하는 가장 기초적인 방법을 의미합니다.\n",
    "\n",
    "던지수\n",
    "(Dunn Index)\n",
    "\n",
    "[진단 기법] 군집 간 거리의 최솟값을 군집 내 거리의 최댓값으로 나눈 지표입니다. 값이 클수록 군집화가 잘 되었다고 판단합니다.\n",
    "\n",
    "옐로우메소드(Elbow Method)[진단 기법] (정확히는 엘보우 기법) 군집 수($K$)를 늘려가며 SSE의 감소율을 그래프로 그려, 꺾이는 지점(팔꿈치)을 찾는 가장 대표적인 적정 군집 수 결정법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5fff2",
   "metadata": {},
   "source": [
    "# 68. 그래프를 그려서 정규성 가정이 만족되는지 시각적으로 확인한는 방법은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aa9619",
   "metadata": {},
   "source": [
    "1. 샤크로-월크 검정\n",
    "2. 콜모고로프-스미르노프 검정\n",
    "3. 카이제곱 검정\n",
    "4. 분위수 대조도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98afd7",
   "metadata": {},
   "source": [
    "정답은 **4번. 분위수 대조도 (Q-Q Plot)**입니다.\n",
    "\n",
    "문제에서 묻는 포인트는 '그래프를 그려서', **'시각적으로 확인'**하는 방법이라는 점입니다. 나머지 보기들은 모두 수치적인 통계량을 통해 판단하는 '검정(Test)' 방법입니다.\n",
    "\n",
    "샤피로-윌크 검정\t수치적 (검정)\t\n",
    "표본 수가 적을 때 주로 사용하며, p-value를 통해 정규성을 판단합니다.\n",
    "\n",
    "콜모고로프-스미르노프\t수치적 (검정)\t\n",
    "표본의 분포와 이론적 분포의 최대 거리를 계산하여 정규성을 확인합니다.\n",
    "\n",
    "카이제곱 검정\t수치적 (검정)\t\n",
    "범주형 데이터의 적합도나 독립성을 확인할 때 사용하며, 정규성 시각화와 거리가 멉니다.\n",
    "\n",
    "분위수 대조도 (Q-Q Plot)\t시각적 (그래프)\t\n",
    "수집한 데이터의 분위수와 정규분포의 분위수를 평면에 타점하여 직선에 가까운지 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c54ef2",
   "metadata": {},
   "source": [
    "# 69. 다음은 모수의 유의성 검정절차다. 순서대로 나열하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c5b933",
   "metadata": {},
   "source": [
    "ㄱ. 가설(귀무가설, 대립가설)을 세운다.\n",
    "\n",
    "ㄴ. 관측된 자료에 대한 P값을 계산한다.\n",
    "\n",
    "ㄷ. 유의수준 a를 정한다.\n",
    "\n",
    "ㄹ. 감정통계량을 결정한다.\n",
    "\n",
    "ㅁ. P값 < a -> 귀무가설을 기각하거나 P값 < a 귀무가설을 체택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f1dcf",
   "metadata": {},
   "source": [
    "1. ㄱ ㄹ ㄴ ㄷ ㅁ\n",
    "2. ㄱ ㄷ ㄹ ㄴ ㅁ\n",
    "3. ㄱ ㄴ ㄹ ㄷ ㅁ\n",
    "4. ㄱ ㄷ ㄴ ㄹ ㅁ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e071bcc",
   "metadata": {},
   "source": [
    "제시된 단계들은 통계적 **가설검정(Hypothesis Testing)**의 표준 절차를 묻는 문제입니다.\n",
    "\n",
    "정답은 2번 (ㄱ $\\rightarrow$ ㄷ $\\rightarrow$ ㄹ $\\rightarrow$ ㄴ $\\rightarrow$ ㅁ) 입니다.\n",
    "\n",
    "1단계 (ㄱ)가설 설정무엇을 증명할지 귀무가설($H_0$)과 대립가설($H_1$)을 먼저 세웁니다.\n",
    "\n",
    "2단계 (ㄷ)유의수준($\\alpha$) 결정어느 정도 오차까지 허용할지(보통 0.05) 데이터 분석 전에 미리 정합니다.\n",
    "\n",
    "3단계 (ㄹ)검정통계량 결정데이터의 특성에 따라 어떤 공식($Z$, $t$, $F$, $\\chi^2$ 등)을 쓸지 선택합니다.\n",
    "\n",
    "4단계 (ㄴ)P-value 계산수집된 실제 데이터를 공식에 넣어 확률값(P-value)을 산출합니다.\n",
    "\n",
    "5단계 (ㅁ)결과 판정P-value와 **$\\alpha$**를 비교하여 귀무가설의 기각 여부를 결정합니다.\n",
    "\n",
    "🔍 핵심 포인트: 왜 ㄷ(유의수준)이 ㄴ(P값)보다 먼저인가요?가장 많이 헷갈리는 부분입니다. \n",
    "**유의수준($\\alpha$)**은 판단의 '기준선'입니다. 경기가 시작되기 전에 규칙(기준)을 먼저 정해야 하듯이, 데이터를 보고 나서 나에게 유리하게 기준을 바꾸지 못하도록 P값을 계산하기 전에 미리 정하는 것이 통계적 원칙입니다.\n",
    "\n",
    "💡 판정 기준 (ㅁ 단계)$ P \\text{-value} < \\alpha$: 발생할 확률이 너무 희박하므로, 귀무가설을 기각합니다.\n",
    "(내가 주장하는 대립가설이 맞음)$P \\text{-value} \\geq \\alpha$: 흔히 일어날 수 있는 일이므로, 귀무가설을 채택합니다. (내 주장이 근거가 부족함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f7ac0",
   "metadata": {},
   "source": [
    "# 70. 다음 그래프가 의미하는 적합도 검정 기법은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc7a61",
   "metadata": {},
   "source": [
    "누적확률\n",
    " |   -|-------- 관찰된 데이터\n",
    " |  / |--------- 비교대상\n",
    " | / /| <---- 최대거리\n",
    " |/-/\n",
    " -----------> x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87808867",
   "metadata": {},
   "source": [
    "1. 샤피로-윌크 검정\n",
    "2. 콜모고로프-스리프노프 검정\n",
    "3. 카이제곰 검정\n",
    "4. Q-Q plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c8f8f",
   "metadata": {},
   "source": [
    "제시된 그래프의 특징과 설명을 종합했을 때, 정답은 **2번. 콜모고로프-스미르노프 검정(Kolmogorov-Smirnov Test, K-S 검정)**입니다.\n",
    "콜모고로프-스미르노프(K-S) 검정은 데이터의 분포가 특정 이론적 분포(정규분포 등)와 얼마나 일치하는지 확인하는 적합도 검정 기법입니다.\n",
    "\n",
    "1. 샤피로-윌크 검정 (Shapiro-Wilk Test): 데이터가 정규분포를 따르는지 검정하며, 표본 수가 적을 때(보통 2,000개 미만) 주로 사용합니다. 그래프보다는 통계량($W$) 위주로 판단합니다.\n",
    "\n",
    "3. 카이제곱 검정 (Chi-square Test): 관찰 빈도와 기대 빈도의 차이를 이용합니다. 연속형 변수보다는 범주형 데이터의 적합도나 독립성을 검정할 때 사용합니다.\n",
    "\n",
    "4. Q-Q plot: 수집한 데이터의 분위수와 이론적 분포의 분위수를 산점도 형태로 그린 것입니다. 데이터가 직선상에 놓여있는지를 보고 시각적으로 정규성을 판단하지만, \"누적확률 곡선 간의 최대거리\"를 측정하는 방식은 아닙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a8e31d",
   "metadata": {},
   "source": [
    "# 71. 다음 중 과적합의 원인이 아닌 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c4f9b",
   "metadata": {},
   "source": [
    "1. 학습 데이터가 대표성을 가지지 못하는 경우\n",
    "2. 고려해야 할 독립변수가 너무 많아 차원의 저주 발생\n",
    "3. 복잡한 모델이 생성되어 독립변수와 종속변수간의 관계 설명 실패\n",
    "3. 특징이 단순하여 학습 정확도 저하"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aacec2",
   "metadata": {},
   "source": [
    "제시된 보기 중 과적합(Overfitting)의 원인이 아닌 것은 4번(지문상 두 번째 3번). 특징이 단순하여 학습 정확도 저하입니다.\n",
    "\n",
    "1번. 학습 데이터가 대표성을 가지지 못하는 경우: 데이터 양이 너무 적거나 편향되어 있으면, 모델은 데이터의 일반적인 패턴이 아니라 특정 노이즈까지 학습하게 되어 과적합이 발생합니다.\n",
    "\n",
    "2번. 차원의 저주 발생: 독립변수(Feature)가 너무 많으면 모델이 불필요한 정보까지 학습하게 되어 복잡도가 급격히 올라가고 과적합의 원인이 됩니다.\n",
    "\n",
    "3번. 복잡한 모델 생성: 모델이 너무 복잡하면(예: 고차 다항 회귀, 깊은 트리) 훈련 데이터의 아주 미세한 변동까지 다 학습해버려 일반화 능력을 잃게 됩니다.\n",
    "\n",
    "4번. 특징이 단순하여 학습 정확도 저하 (오답): 이는 과적합이 아니라 **과소적합(Underfitting)**의 원인입니다. 모델이 너무 단순하거나 특징이 부족하면 데이터의 본질적인 패턴조차 학습하지 못해 훈련 데이터에서도 정확도가 낮게 나옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698e730",
   "metadata": {},
   "source": [
    "# 72. 파라미터 최적화의 설명으로 틀린것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc961e5",
   "metadata": {},
   "source": [
    "1. 파라미터는 주어진 데이터로부터 학습을 통해 모델 내부에서 결정되는 변수이다.\n",
    "2. 학습 모델과 실제 레이블과의 차이는 손실 함수로 표현한다.\n",
    "3. 학습의 목적은 오차나 손실 함수의 값을 최소화하도록 가중치의 편향을 찾는 것이다.\n",
    "4. 학습률이 작은 경우 학습이 빠르고 높은 경우는 학습이 느리다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a3bd2",
   "metadata": {},
   "source": [
    "제시된 보기 중 파라미터 최적화에 대한 설명으로 틀린 것은 4번입니다.\n",
    "\n",
    "학습률(Learning Rate)이 작을 경우: 가중치(Weight)를 아주 조금씩 업데이트하기 때문에 최적의 값에 도달하기까지 학습 속도가 매우 느려집니다. 또한, 너무 작으면 국소 최적해(Local Minimum)에 갇힐 위험이 있습니다.\n",
    "\n",
    "학습률이 클 경우: 가중치를 크게 업데이트하므로 초기 학습 속도는 빠를 수 있으나, 최적의 값을 지나쳐버리는 오버슈팅(Overshooting) 현상이 발생하여 발산하거나 최솟값 주위를 맴돌며 수렴하지 못할 수 있습니다.\n",
    "\n",
    "1번: **파라미터(Parameter)**는 가중치(Weight)나 편향(Bias)처럼 모델이 데이터로부터 스스로 배우는 값을 의미합니다. (사용자가 직접 정하는 것은 하이퍼파라미터입니다.)\n",
    "\n",
    "2번: 모델의 예측값과 실제 정답(Label)의 차이를 계산하는 함수를 손실 함수(Loss Function) 또는 비용 함수(Cost Function)라고 부릅니다.\n",
    "\n",
    "3번: 머신러닝 학습의 본질은 경사하강법(Gradient Descent) 등을 이용해 손실 함수의 값을 최소화하는 최적의 가중치와 편향 조합을 찾는 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb7d8c",
   "metadata": {},
   "source": [
    "# 73. 다음 중 파라미터에 대한 설명으로 거리가 먼 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f11e9",
   "metadata": {},
   "source": [
    "1. 파라미터는 모델 내부에서 결정되는 변수로 그 값은 데이터로부터 결정된다.\n",
    "2. 선형휘귀의 계수도 모델링에 의해 자동으로 결정되는 값으로 파라미터라 한다.\n",
    "3. 하이퍼파라미터는 모델링할 때 사용자가 직접 세팅해주는 값을 의미한다.\n",
    "4. 하이퍼파라미터는 정해진 최적의 값이 있어 류리스틱한 방법이나 경험법칙으로 결정할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d48e9",
   "metadata": {},
   "source": [
    "제시된 보기 중 파라미터에 대한 설명으로 가장 거리가 먼 것은 4번입니다.\n",
    "4. 하이퍼파라미터는 정해진 최적의 값이 있어 휴리스틱한 방법이나 경험법칙으로 결정할 수 없다. (X)\n",
    "\n",
    "실제로는: 하이퍼파라미터는 모든 데이터셋에 완벽하게 적용되는 '절대적인 정답(최적의 값)'이 미리 정해져 있지 않습니다.\n",
    "\n",
    "결정 방법: 데이터의 특성에 따라 분석가의 경험 법칙(Rule of Thumb), 시시각각 결과를 확인하며 조정하는 휴리스틱(Heuristic) 방법, 또는 Grid Search/Random Search와 같은 반복적인 실험을 통해 최적의 값을 찾아내야 합니다.\n",
    "\n",
    "1번: **파라미터(Parameter)**는 모델이 학습을 통해 데이터로부터 스스로 도출하는 값입니다. (예: 인공신경망의 가중치 $w$, 편향 $b$)\n",
    "\n",
    "2번: **선형 회귀의 계수(Coefficient)**나 절편(Intercept)은 최소제곱법 등을 통해 자동으로 계산되는 값이므로 모델 파라미터에 해당합니다.\n",
    "\n",
    "3번: **하이퍼파라미터(Hyper-parameter)**는 학습 시작 전에 사용자가 수동으로 설정하는 설정값입니다. (예: 학습률, 의사결정나무의 깊이, KNN의 K값 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e8f19",
   "metadata": {},
   "source": [
    "# 74. 최종 모델 평가기준 선정 시 고려하는 대표적인 평가지표로 틀린것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646f566",
   "metadata": {},
   "source": [
    "1. 희귀모델 : MAE, MSE\n",
    "2. 예측모델 : 혼동행렬, 향상도챠트\n",
    "3. 군집모델 : 일치행렬, 랜드지수\n",
    "4. 분류모델 : ROC Curve, 카파(Kappa), 상관계수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819cf647",
   "metadata": {},
   "source": [
    "최종 모델 평가 지표에 관한 문제에서 틀린 것은 4번. 분류모델 : ROC Curve, 카파(Kappa), 상관계수입니다.\n",
    "\n",
    "4. 분류모델 : ROC Curve, 카파(Kappa), 상관계수 (X)\n",
    "이유: 분류 모델의 평가지표로 ROC Curve와 카파(Kappa) 상관계수는 매우 적절한 지표입니다. 하지만 단순한 **'상관계수(Correlation Coefficient)'**는 주로 두 연속형 변수 사이의 선형적 관계를 측정할 때 사용하며, 회귀(Regression) 모델에서 독립변수와 종속변수의 관계를 파악하거나 변수 선택 시 활용됩니다.\n",
    "\n",
    "분류 모델의 대표 지표: 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), F1-Score, AUC(ROC Curve의 면적) 등이 있습니다.\n",
    "\n",
    "1. 회귀모델 (Regression): 실제값과 예측값의 차이(오차)를 계산하는 MAE(평균 절대 오차), MSE(평균 제곱 오차), RMSE, 결정계수($R^2$) 등을 사용합니다.\n",
    "\n",
    "2. 예측모델 (Classification/Prediction): 분류 예측의 경우 정답과 오답을 표로 나타낸 **혼동행렬(Confusion Matrix)**과 모델의 효율성을 나타내는 **향상도 차트(Lift Chart)**를 사용합니다.\n",
    "\n",
    "3. 군집모델 (Clustering): 군집 결과의 정답이 있는 경우(외부 평가) 두 군집화 결과가 얼마나 일치하는지 측정하는 **일치행렬(Matching Matrix)**이나 랜드지수(Rand Index), 정밀도/재현율을 결합한 F-measure 등을 활용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa30267",
   "metadata": {},
   "source": [
    "# 75. 다음중 데이터 시각화 기능에 대한 설명으로 잘못된 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ab45f",
   "metadata": {},
   "source": [
    "1. 설명 : 전달하려는 메시지화 분석결과 설명\n",
    "2. 가설 : 증명하기 어려운 원인에 대한 예측 이론\n",
    "3. 탐색 : 유의미하고 흥미로운 요소 명확하게 보여줌\n",
    "4. 표현 : 이야기 전달과 공감을 불러 일으키기 위한 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd5f36e",
   "metadata": {},
   "source": [
    "데이터 시각화의 기능적 목적에 대한 문제군요. 정답은 2번. 가설 : 증명하기 어려운 원인에 대한 예측 이론입니다.\n",
    "2. 가설 : 증명하기 어려운 원인에 대한 예측 이론 (X)\n",
    "\n",
    "이유: 데이터 시각화에서 가설(Hypothesis) 기능은 단순히 '증명하기 어려운 예측 이론'을 제시하는 것이 아닙니다. 시각화를 통해 데이터 간의 관계를 살펴봄으로써 특정 가설이 **맞는지 틀린지를 검증(Validation)**하거나, 새로운 가설을 도출하는 과정을 의미합니다. 단순히 모호한 이론을 만드는 것이 아니라, 데이터를 눈으로 확인하며 논리적 근거를 찾는 실무적인 과정입니다.\n",
    "\n",
    "설명 (Explanation): 분석을 통해 얻은 핵심 인사이트와 메시지를 독자에게 쉽고 명확하게 전달하는 목적입니다. (보고서, 프레젠테이션 등)\n",
    "\n",
    "탐색 (Exploration): 원시 데이터에서 미처 발견하지 못한 패턴, 이상치(Outlier), 변수 간의 관계를 시각적으로 찾아내는 과정입니다. 흔히 **EDA(탐색적 데이터 분석)**라고 부르는 단계의 핵심 기능입니다.\n",
    "\n",
    "표현 (Expression): 단순히 정보를 전달하는 것을 넘어, 디자인 요소나 스토리텔링을 결합하여 사용자의 공감을 이끌어내고 데이터 속에 담긴 이야기를 효과적으로 드러내는 기능입니다. (데이터 저널리즘, 인포그래픽 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d0aeb",
   "metadata": {},
   "source": [
    "# 76. 아래의 그래프에 대한 해석으로 틀린것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54124098",
   "metadata": {},
   "source": [
    "300 |               []\n",
    "    |   []          [] \n",
    "300 |   []          []    []\n",
    "    |   []    []    []    []\n",
    "300 |   []    []    []    []\n",
    "    |   []    []    []    []\n",
    "300 |   []    []    []    []\n",
    "    |   []    []    []    []\n",
    "    |   []    []    []    []\n",
    "200 |   []    []    []    []   \n",
    "    |   []    []    []    []\n",
    "  0 |-----------------------------\n",
    "       2018  2019  2020  2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91097b",
   "metadata": {},
   "source": [
    "1. 2020년의 매출이 가장 크다.\n",
    "2. 매출은 꾸준히 증가하고 있다.\n",
    "3. 2019년에 매출 및 영업이익의 합이 가장작다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6af57",
   "metadata": {},
   "source": [
    "2. 매출은 꾸준히 증가하고 있다. (X)\n",
    "\n",
    "해설: 그래프의 흐름을 보면 2018년에서 2019년으로 넘어갈 때 수치가 감소했습니다. 또한 2020년에서 2021년 사이에도 다시 감소하는 패턴을 보입니다. 따라서 '꾸준히 증가'한다는 설명은 틀린 해석입니다.\n",
    "1. 2020년의 매출이 가장 크다. (O)\n",
    "\n",
    "그래프상에서 2020년 막대의 높이가 가장 높게 표시되어 있으므로 적절한 해석입니다.\n",
    "\n",
    "3. 2019년에 매출 및 영업이익의 합이 가장 작다. (O)\n",
    "\n",
    "그래프의 막대 높이가 2019년에 가장 낮게 그려져 있습니다. (제시된 그래프가 매출 혹은 합계를 의미한다고 가정할 때, 2019년이 최저점입니다.)\n",
    "\n",
    "추세(Trend): 전체적으로 우상향하는가, 우하향하는가, 혹은 변동이 있는가?\n",
    "\n",
    "비교(Comparison): 어느 항목이 가장 크고(Max) 어느 항목이 가장 작은가(Min)?\n",
    "\n",
    "왜곡(Distortion): Y축의 시작점이 0인가? (이 그래프는 0부터 시작하지만, 만약 중간부터 시작한다면 변화량이 과다하게 보일 수 있습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc2ee3",
   "metadata": {},
   "source": [
    "# 77. 빅데이터 사각화의 기법 중 X축과 Y축사이에 존재하는 두 개 변수 간의 관곌르 나타내는 방법으로 알맞은 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b133b05",
   "metadata": {},
   "source": [
    "1. 산점도\n",
    "2. 히스토그램\n",
    "3. 버블차트\n",
    "4. 플로팅차트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1c0bc",
   "metadata": {},
   "source": [
    "빅데이터 시각화 기법 중 두 변수 간의 관계를 좌표평면(X축, Y축) 위에 점으로 표시하여 나타내는 정답은 **1번. 산점도 (Scatter Plot)**입니다.\n",
    "산점도 (Scatter Plot):\n",
    "\n",
    "특징: 두 개의 연속형 변수 간의 상관관계를 파악하기 위해 사용됩니다.\n",
    "\n",
    "해석: 점들이 우상향하면 양의 상관관계, 우하향하면 음의 상관관계가 있다고 판단합니다. 점들이 흩어져 있으면 관계가 없는 것으로 봅니다.\n",
    "\n",
    "2. 히스토그램 (Histogram):\n",
    "\n",
    "용도: 단일 변수의 **분포(빈도)**를 나타낼 때 사용합니다.\n",
    "\n",
    "차이점: 막대그래프와 비슷해 보이지만, X축이 범주가 아닌 연속적인 구간(계급)으로 이루어져 있습니다.\n",
    "\n",
    "3. 버블차트 (Bubble Chart):\n",
    "\n",
    "용도: 세 개의 변수 간의 관계를 나타냅니다.\n",
    "\n",
    "차이점: 산점도(X, Y축)에 원의 **크기(세 번째 변수)**를 추가하여 더 입체적인 정보를 전달합니다.\n",
    "\n",
    "4. 플로팅차트 (Floating Chart):\n",
    "\n",
    "보통 **폭포 차트(Waterfall Chart)**나 특정 구간의 시작과 끝을 보여주는 막대 형태를 의미하며, 두 변수의 상관관계를 점으로 찍어 표현하는 일반적인 방식은 아닙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faab91a",
   "metadata": {},
   "source": [
    "# 78. 다음 그래프를 보고 같은 유형의 빅데이터 시각화 기법이 아닌것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f64d29",
   "metadata": {},
   "source": [
    "       12월17일     2월15일     4월 6일     5월15일\n",
    "기획 | --------\n",
    "분석 |         --\n",
    "설계 |           ------\n",
    "개발 |                 --------------\n",
    "시험 |                               -------\n",
    "운영 |                                      --------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a233cf6",
   "metadata": {},
   "source": [
    "1. 체르노프페이스\n",
    "2. 히트맵\n",
    "3. 스타챠트\n",
    "4. 트리맵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeaec54",
   "metadata": {},
   "source": [
    "제시된 텍스트 그래프는 시간의 흐름에 따른 일정과 진행 상황을 나타내는 **간트 차트(Gantt Chart)**의 형태입니다. 이는 여러 변수를 한눈에 비교하거나 데이터의 구조를 파악하는 관계/비교 시각화 또는 다변량 시각화 범주로 해석될 수 있습니다.\n",
    "질문의 의도는 여러 개의 변수를 동시에 보여주는 다변량(Multivariate) 데이터 시각화 기법이 아닌 것을 찾는 것입니다. 정답은 4번. 트리맵입니다.\n",
    "\n",
    "4. 트리맵 (Treemap):\n",
    "\n",
    "특징: 전체 항목을 사각형으로 분할하여 **계층 구조(Hierarchical)**나 **부분과 전체의 비중(Composition)**을 나타내는 기법입니다.\n",
    "\n",
    "차이점: 나머지 보기들이 여러 독립적인 변수들의 특성을 비교하는 데 중점을 둔다면, 트리맵은 데이터의 '포함 관계'와 '크기'를 면적으로 비교하는 데 특화되어 있습니다.\n",
    "\n",
    "체르노프 페이스 (Chernoff Faces):\n",
    "\n",
    "사람의 얼굴 형태(눈 크기, 입 모양 등)에 데이터 변수를 매핑하여 다차원 데이터를 표현합니다. 인간이 얼굴 변화에 민감하다는 점을 이용한 기법입니다.\n",
    "\n",
    "히트맵 (Heat Map):\n",
    "\n",
    "칸마다 색상을 다르게 하여 여러 변수 간의 관계나 밀도를 나타냅니다. X축과 Y축에 다양한 항목을 배치하여 다변량 데이터를 한눈에 보여줍니다.\n",
    "\n",
    "스타 차트 (Star Chart / Radar Chart):\n",
    "\n",
    "중심점에서 뻗어 나가는 축들에 변수 값을 표시하여 별 모양으로 데이터를 나타냅니다. 여러 항목의 균형이나 성능을 비교할 때 주로 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ec724",
   "metadata": {},
   "source": [
    "# 79. 인포그래픽의 설명으로 틀린것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59ab1e",
   "metadata": {},
   "source": [
    "1. 중요 정보를 하나의 그래픽으로 표현해서 보는 사람들이 쉽게 정보를 이해할 수 있도록 만드는 시각화 방법이다.\n",
    "2. 정보를 일반적으로 전달하는 방법과 스토리델링이 가는하다는 점에서 기존 차트나 그래픽과 동일하게 사용한다.\n",
    "3. 정보의 인포에이션과 시각적 그래프의 합성어이다.\n",
    "4. 인포그래픽 기법으로는 지도형, 도표형, 타임라인형 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450fe5d6",
   "metadata": {},
   "source": [
    "인포그래픽에 대한 설명 중 틀린 것은 2번입니다.\n",
    "\n",
    "2. 정보를 일반적으로 전달하는 방법과 스토리텔링이 가능하다는 점에서 기존 차트나 그래픽과 동일하게 사용한다. (X)\n",
    "\n",
    "틀린 이유: 인포그래픽은 단순히 데이터의 수치를 보여주는 기존 차트나 그래픽과 동일하게 취급되지 않습니다. * 차별점: 기존 차트가 데이터의 '사실적 표현'에 집중한다면, 인포그래픽은 스토리텔링 요소를 강화하여 분석가의 의도나 특정 메시지를 훨씬 더 강렬하고 직관적으로 전달하도록 설계된 고도의 디자인 결과물입니다. 즉, 정보 전달의 효율성과 시각적 유희가 훨씬 높습니다.\n",
    "        \n",
    "1번: 방대한 정보를 요약하여 하나의 이미지로 압축하기 때문에 사용자가 정보를 습득하는 시간을 단축해 줍니다.\n",
    "\n",
    "3번: 인포메이션(Information)과 그래픽(Graphic)의 합성어가 맞습니다.\n",
    "\n",
    "4번: 인포그래픽의 종류는 매우 다양합니다.\n",
    "\n",
    "지도형: 지리적 위치에 데이터를 배치 (예: 지역별 인구수)\n",
    "\n",
    "도표형: 복잡한 통계 수치를 아이콘과 결합 (예: 설문조사 결과)\n",
    "\n",
    "타임라인형: 역사적 흐름이나 과정 설명 (예: 기업의 연혁)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173cd5c",
   "metadata": {},
   "source": [
    "# 80. 데이터 분석결과 활용에 대한 설명으로 옳지 않은 것은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee17dc",
   "metadata": {},
   "source": [
    "1. 데이터가 많을 때 훈련만 하고 검증하지 않아도 신뢰성이 높다.\n",
    "2. 위기상황에 대한 판단을 할 수 있다.\n",
    "3. 숨은 니즈와 가치 발견이 가능하다.\n",
    "4. 새로운 정의 왜곡 및 실제 여부를 판단할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89bd84c",
   "metadata": {},
   "source": [
    "데이터 분석 결과 활용에 대한 설명 중 옳지 않은 것은 1번입니다.\n",
    "\n",
    "1. 데이터가 많을 때 훈련만 하고 검증하지 않아도 신뢰성이 높다. (X)\n",
    "\n",
    "틀린 이유: 데이터의 양이 아무리 많더라도 **훈련(Training)**만 하고 검증(Validation/Test) 과정을 거치지 않으면 모델의 신뢰성을 보장할 수 없습니다.\n",
    "\n",
    "과적합(Overfitting) 위험: 데이터가 많을수록 모델이 훈련 데이터의 특수한 노이즈까지 학습하여 실제 상황(새로운 데이터)에서는 성능이 급격히 떨어지는 과적합 현상이 발생할 수 있습니다. 따라서 반드시 데이터를 분할하여 성능을 검증해야 합니다.\n",
    "\n",
    "2. 위기상황에 대한 판단을 할 수 있다. (O): 과거의 패턴을 분석하여 이상 징후를 감지하거나, 미래의 위험 요소를 예측함으로써 선제적인 위기 관리가 가능해집니다.\n",
    "\n",
    "3. 숨은 니즈와 가치 발견이 가능하다. (O): 고객의 행동 데이터를 분석함으로써 고객 스스로도 인지하지 못했던 잠재적 요구(Unmet Needs)를 찾아내고 새로운 비즈니스 가치를 창출할 수 있습니다.\n",
    "\n",
    "4. 새로운 정의 왜곡 및 실제 여부를 판단할 수 있다. (O): 기존의 통념이나 잘못된 가설이 실제 데이터와 일치하는지 객관적으로 검증함으로써, 왜곡된 정보를 바로잡고 실체적인 진실을 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943de8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
