{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c806157e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:99% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
       "div.text_cell_render.rendered_html{font-size:20pt;}\n",
       "div.text_cell_render ul li, div.text_cell_render ol li p, code{font-size:22pt; line-height:30px;}\n",
       "div.output {font-size:24pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:24pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
       "table.dataframe{font-size:24px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:99% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
    "div.text_cell_render.rendered_html{font-size:20pt;}\n",
    "div.text_cell_render ul li, div.text_cell_render ol li p, code{font-size:22pt; line-height:30px;}\n",
    "div.output {font-size:24pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:24pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
    "table.dataframe{font-size:24px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46ec9eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1649882957.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    - Inference API 이용 : 모델의 결과를 sur\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# <span style=\"color:red\">ch1_허깅페이스</span>\n",
    "- Inference API 이용 : 모델의 결과를 sur\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ea06910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "# 경고 제거\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# transformers 로깅 레벨 조정\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Hugging Face symlink 경고 제거\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# from transformers import pipeline, logging as hf_logging\n",
    "# hf_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a582b0",
   "metadata": {},
   "source": [
    "# ch1. 허깅페이스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632eb56",
   "metadata": {},
   "source": [
    "* Inference API 이용 : 모델의 결과를 surver에서\n",
    "* pipeline() 이용 : 모델을 다운로드받아 모델의 결과를 local에서\n",
    "    * raw text -> tokenizer -> model -> [0.11, 0.55, 0.xx, ~] logits값으로 prediction 결과 출력\n",
    "```\n",
    "허깅페이스 transformers에서 지원하는 task\n",
    "\"sentiment-analysis\" : \"text-classification\"의 별칭(감정분석 전용으로 사용)\n",
    "\"text-classification\" : 감정분석, 뉴스분류, 리뷰 분류 등 일반적인 문장 분류\n",
    "\"zero-shot-classification\" : 레이블을 학습 없이 주어진 후보군 중에서 분류\n",
    "\"token-classification\" : 개체명 인식(NER ; Named Entity REcognition) 등 단위 라벨링\n",
    "\"ner\" : \"token-classification\"의 별칭\n",
    "\"fill-mask\" : 빈칸 채우기\n",
    "\"text-generation\" : 텍스트 생성 (GPT류 모델에 사용)\n",
    "\"text2text-generation\" : 번역, 요약 등 입력 -> 출력 변환 \n",
    "\"translation\" : 번역\n",
    "\"summarization\" : 텍스트요약\n",
    "\"question-answering\" : 주어진 context를 보고 질문에 답하기.\n",
    "\"image-to-text\" : 그림을 설명\n",
    "\"image-classification\" : 이미지분류\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc4273",
   "metadata": {},
   "source": [
    "# 1. 텍스트 기반 감정분석(긍정/부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9572e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(task=\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e832240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(task=\"text-classification\",\n",
    "                     model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# 감정분석시 내용이 많으면 list로\n",
    "classifier([\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cfa0406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
    " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b6f4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
    " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9649cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8577604293823242}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"이 물건 정말 사고 싶어요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4248edf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998695850372314},\n",
       " {'label': 'POSITIVE', 'score': 0.999488353729248},\n",
       " {'label': 'NEGATIVE', 'score': 0.599323034286499},\n",
       " {'label': 'POSITIVE', 'score': 0.8669533729553223}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"I like you\", \"I hat you\", \"나 너가 싫어\", \"힘들어요\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a3692c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(task=\"sentiment-analysis\",\n",
    "                     model=\"matthewburke/korean_sentiment\")\n",
    "texts = ['나는 너가 좋아', \"당신이 싫어요\", \"힘들어요\", \"오늘 기분이 최고야\"]\n",
    "result = classifier(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ac551b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mtexts\u001b[49m, classifier(texts)):\n\u001b[0;32m      2\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m긍정\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLABEL_1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m부정\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m => \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "for text, result in zip(texts, classifier(texts)):\n",
    "    label = \"긍정\" if result['label']=='LABEL_1' else \"부정\"\n",
    "    print(f\"{text} => {label} : {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9c665",
   "metadata": {},
   "source": [
    "# 2. 제로샷분류(Zero-shot분류)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa4d60",
   "metadata": {},
   "source": [
    "* 기계학습 및 자연어처리에서 각 개별 작업에 대한 특정 교육없이 작업을 수행할 수 있는 모형(비지도학습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1229d3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf2b2d99c3c4957be84fbfe372b1e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562bf848a77d4bc2beadbcac91bd85fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4830af9a0f04dcdb946e37857842b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a352896abfc84d87b5be29298ce512ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abefba6f2a744899c9436e264fdbabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b1649f4a0e4924983fe18316148109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!',\n",
       " 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n",
       " 'scores': [0.5227580070495605,\n",
       "  0.45814019441604614,\n",
       "  0.0142647260800004,\n",
       "  0.0026850001886487007,\n",
       "  0.002152054337784648]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                     # model=\"facebook/bart-large-mnli\"\n",
    "                     )\n",
    "classifier(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e7a060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'One day I well see the world',\n",
       " 'labels': ['travel', 'cooking', 'dancing'],\n",
       " 'scores': [0.9938077926635742, 0.003099897177889943, 0.003092351369559765]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"One day I well see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6259a752",
   "metadata": {},
   "source": [
    "# 3. text 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d70e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'in this course. We will teach you how to use the tools of the trade to gain valuable insight into the trade process. This course is designed to teach you how to build a solid and professional resume. We will also teach you how to build a strong business and to help you develop a solid and professional resume. You will learn to work with people who are knowledgeable in the trade, and you will learn how to build your resume. We will also teach you how to build a strong team that will succeed in the future and will be a force in your future. This course is designed to teach you how to build a solid and professional resume. We will also teach you how to build a strong team that will succeed in the future and will be a force in your future.\\n\\nCourse Overview\\n\\nWe will show you how to build a strong resume with a solid, professional resume. We will also show you how to build a strong resume with a solid, professional resume. We will also show you how to build a strong resume with a solid, professional resume. We will also show you how to build a strong resume with a solid, professional resume. We will also show you how to build a strong resume with a solid, professional resume. We will also show you how to build a strong resume with a solid, professional'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "# set_seed(2)\n",
    "generation = pipeline(\"text-generation\", \"gpt2\") # 텍스트 생성 gpt3부터는 허깅페이스없음\n",
    "generation(\n",
    "    \"in this course. We will teach you how to\",\n",
    "    pad_token_id=generation.tokenizer.eos_token_id\n",
    ") # pad_token_id 경고를 없애려고 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc929be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this course. We will teach you how to create and use CSS3 widgets for your website, and how to build responsive pages and responsive CSS pages inside of your website.\n",
      "\n",
      "The Course\n",
      "\n",
      "The course is taught in our Introduction to CSS3. It covers using our CSS3 widgets system, and using a variety of technologies to create responsive websites in JavaScript and CSS.\n",
      "\n",
      "The course is also divided into 5 sections which are divided into 3 sections, which will cover our core concepts:\n",
      "\n",
      "Introduction to CSS3 widgets\n",
      "\n",
      "Introduction to HTML 5-based CSS widgets\n",
      "\n",
      "How to create and use responsive pages and responsive CSS pages inside of your website\n",
      "\n",
      "How to build responsive pages and responsive CSS pages inside of your website\n",
      "\n",
      "How to build responsive pages and responsive CSS pages inside of your website\n",
      "\n",
      "How to build responsive pages and responsive CSS pages inside of your website\n",
      "\n",
      "How to build responsive pages and responsive CSS pages inside of your website\n",
      "\n",
      "How to build responsive pages and responsive CSS pages inside of your website\n",
      "\n",
      "How to build responsive pages and responsive CSS pages inside of your website\n",
      "\n",
      "How to build responsive pages and responsive CSS pages inside of your website\n",
      "\n",
      "How to build responsive pages and responsive CSS pages inside of your website\n",
      "\n",
      "How to build responsive pages and responsive CSS pages inside\n"
     ]
    }
   ],
   "source": [
    "result = generation(\n",
    "    \"in this course. We will teach you how to\",\n",
    "    pad_token_id=generation.tokenizer.eos_token_id\n",
    ") \n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79a4b97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65726d8c6d304b36911e5e62ea074541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--skt--kogpt2-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534269d57c81465f92333aa86e9fe83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cb2bd1446b44b6bf7b9fc2d02dd268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c9447828514204bbd5735b846bf8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 과정은 다음과 같은 방법을 알려드려요. 죠셉이라는 자가 말입니다.\"\n",
      "\"알겠습니다. 당신처럼 당신을 사랑하는 사람은 당신이 만든 것이 아니라 당신이 만든 것입니다.\"\n",
      "\"당신 같은 사람은 당신이 만든 것이 아니라 당신이 만든 것입니다. 당신이 만든 것이 아니라 당신이 만든 것입니다.\"\n",
      "당신은 다른 방법을 생각해내지 못했다.\n",
      "당신의 생각에는 그런 것이 없었다.\n",
      "\"당신이 만든 것이라면 당신이 원하는 것은 무엇일까?\"\n",
      "\"좋습니다. 당신은 당신을 사랑합니다.\"\n",
      "그러자 한 사람이 대답했다.\n",
      "\"당신을 사랑합니다, 당신. 당신을 사랑합니다.\"\n",
      "당신은 고개를 흔들며 말했다.\n",
      "\"당신은 당신이 만든 것입니다. 당신은 당신을 사랑합니다. 당신은 당신을 사랑합니다.\"\n",
      "\"당신은 당신을 사랑합니다.\"\n",
      "그러자 한 사람이 말했다.\n",
      "\"저는 당신이 만든 것입니다. 당신은 당신을 사랑합니다.\"\n",
      "\"그럼 당신은 당신을 사랑합니까?\"\n",
      "\"당신은 당신을 사랑합니다.\"\n",
      "\"당신은 당신을 사랑합니다.\"\n",
      "그런 다음 두 사람을 불러서 이야기를 나누어 들었다.\n",
      "\"당\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generation = pipeline(\"text-generation\", \"skt/kogpt2-base-v2\")\n",
    "result = generation(\n",
    "    \"이 과정은 다음과 같은 방법을 알려드려요. \",\n",
    "    pad_token_id = generation.tokenizer.eos_token_id,\n",
    "    max_new_tokens = 200, # 생성할 최대 길이(생성할 토큰 수)\n",
    "#     num_return_sequences=1,    # 생성할 문장 갯수\n",
    "#     do_sample=True,            # 다양한 샘플 사용\n",
    "#     top_k=50,            # top-k 샘플링(확률 높은 상위 50개 토큰만 사용)         \n",
    "#     top_p=0.95,          # 확률이 높은 순서대로 95%가 될 때까지의 단어들로만 후보로 사용\n",
    "#     temperature=1.0,     # 창의성 조절(낮을 수록 보수적)\n",
    "#     no_repeat_ngram_size=2 # 반복 방지\n",
    ")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e824a14a",
   "metadata": {},
   "source": [
    "# 4. 마스크(빈칸) 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842fbf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6654233225044a5ab86de178e591585a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf6d29a81e34aadaab5f7c279380b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbed599ba8a145eca3323e077bf18e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7feb303e60146f2a66a401023702e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01703adb57c14d1ea27cffffadb766ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39fdd4358c34576a5c318d711934924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19275707006454468,\n",
       "  'token': 3299,\n",
       "  'token_str': ' doctor',\n",
       "  'sequence': \"I'm going to hospital and meet a doctor\"},\n",
       " {'score': 0.06794589757919312,\n",
       "  'token': 27321,\n",
       "  'token_str': ' psychiatrist',\n",
       "  'sequence': \"I'm going to hospital and meet a psychiatrist\"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(task='fill-mask',\n",
    "                   model='distilbert/distilroberta-base') # 마스크 채우기\n",
    "unmasker(\"I'm going to hospital and meet a <mask>\", top_k=2) # top_k 기본값 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0af9bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unmasker(\"병원에 가서 <mask>를 만날 거예요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85515d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0629730075597763,\n",
       "  'token': 265,\n",
       "  'token_str': ' business',\n",
       "  'sequence': \"Hello, I'm a business model.\"},\n",
       " {'score': 0.038101598620414734,\n",
       "  'token': 18150,\n",
       "  'token_str': ' freelance',\n",
       "  'sequence': \"Hello, I'm a freelance model.\"},\n",
       " {'score': 0.03764132782816887,\n",
       "  'token': 774,\n",
       "  'token_str': ' role',\n",
       "  'sequence': \"Hello, I'm a role model.\"},\n",
       " {'score': 0.037326786667108536,\n",
       "  'token': 2734,\n",
       "  'token_str': ' fashion',\n",
       "  'sequence': \"Hello, I'm a fashion model.\"},\n",
       " {'score': 0.026023676618933678,\n",
       "  'token': 24526,\n",
       "  'token_str': ' Playboy',\n",
       "  'sequence': \"Hello, I'm a Playboy model.\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Hello, I'm a <mask> model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8784b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.14130638539791107,\n",
       "  'token': 35,\n",
       "  'token_str': ':',\n",
       "  'sequence': '안녕하세요? 나는: 모델이예요.'},\n",
       " {'score': 0.1223798543214798,\n",
       "  'token': 116,\n",
       "  'token_str': '?',\n",
       "  'sequence': '안녕하세요? 나는? 모델이예요.'},\n",
       " {'score': 0.08188082277774811,\n",
       "  'token': 328,\n",
       "  'token_str': '!',\n",
       "  'sequence': '안녕하세요? 나는! 모델이예요.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"안녕하세요? 나는 <mask> 모델이예요.\", top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb379014",
   "metadata": {},
   "source": [
    "###  InferenceAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f37e2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce7f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-nlp(ipykernel)",
   "language": "python",
   "name": "ml-dl-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
