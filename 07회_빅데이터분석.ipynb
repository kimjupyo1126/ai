{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae9bbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:99% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
       "div.text_cell_render.rendered_html{font-size:20pt;}\n",
       "div.text_cell_render li, div.text_cell_render p, code{font-size:22pt; line-height:30px;}\n",
       "div.output {font-size:24pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:24pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
       "table.dataframe{font-size:24px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:99% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
    "div.text_cell_render.rendered_html{font-size:20pt;}\n",
    "div.text_cell_render li, div.text_cell_render p, code{font-size:22pt; line-height:30px;}\n",
    "div.output {font-size:24pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:24pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
    "table.dataframe{font-size:24px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f490a48",
   "metadata": {},
   "source": [
    "# 01. 빅데이터의 특징 3가지로 옳은 것은?\n",
    "① 규모, 속도, 가치\n",
    "\n",
    "② 속도, 다양성, 가치\n",
    "\n",
    "③ 규모, 다양성, 가치\n",
    "\n",
    "④ 규모, 속도, 다양성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b62ee1",
   "metadata": {},
   "source": [
    "해설: 전통적으로 빅데이터의 핵심 속성은 3V라고 불리는 다음 세 가지 요소를 의미합니다.\n",
    "\n",
    "규모 (Volume): 디지털 환경에서 생성되는 데이터의 양이 막대함을 의미합니다.\n",
    "\n",
    "속도 (Velocity): 데이터가 생성되고 유통되는 속도가 매우 빠르며, 이를 실시간으로 처리할 수 있어야 함을 의미합니다.\n",
    "\n",
    "다양성 (Variety): 텍스트, 이미지, 영상 등 정형 데이터뿐만 아니라 비정형 데이터까지 포함하는 다양성을 의미합니다.\n",
    "\n",
    "참고: 최근에는 여기에 **가치(Value)**나 정확성(Veracity) 등을 더해 4V 또는 5V로 확장하여 정의하기도 하지만, 가장 기본이 되는 3가지 특징은 규모, 속도, 다양성입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59546b6b",
   "metadata": {},
   "source": [
    "# 02. 다음 중 데이터가 처리되는 과정에서 변경되거나 손상되지 않고, 유지됨을 보장하는 특성으로 알맞은 것은?\n",
    "① 데이터 완전성\n",
    "\n",
    "② 데이터 무결성\n",
    "\n",
    "③ 데이터 정확성\n",
    "\n",
    "④ 데이터 일관성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdf0b7d",
   "metadata": {},
   "source": [
    "정답: ② 데이터 무결성\n",
    "\n",
    "해설: 데이터 무결성(Data Integrity)은 데이터의 생명주기 전반에 걸쳐 데이터가 인가된 방법으로만 변경되고, 전송이나 처리 과정에서 의도하지 않은 수정이나 손상 없이 정확하고 완전하게 유지되는 성질을 의미합니다.\n",
    "\n",
    "데이터 무결성 (Integrity): 데이터의 정확성, 일관성, 유효성이 유지됨을 보장하는 특성입니다.\n",
    "\n",
    "데이터 완전성 (Completeness): 필수적인 데이터 요소들이 누락되지 않고 모두 존재함을 의미합니다.\n",
    "\n",
    "데이터 정확성 (Accuracy): 데이터가 실제 사실과 일치하는 정도를 의미합니다.\n",
    "\n",
    "데이터 일관성 (Consistency): 여러 시스템이나 위치에 있는 동일한 데이터가 서로 모순되지 않고 일치함을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602b5a2",
   "metadata": {},
   "source": [
    "# 03. 기업분석 수준진단의 항목에 속하지 않는 것은?\n",
    "① 조직구조\n",
    "\n",
    "② 분석 데이터\n",
    "\n",
    "③ 분석기법\n",
    "\n",
    "④ IT인프라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d528aa4",
   "metadata": {},
   "source": [
    "03. 기업분석 수준진단의 항목에 속하지 않는 것은?\n",
    "정답: ① 조직구조\n",
    "\n",
    "해설: 기업의 데이터 분석 수준을 진단할 때는 크게 **'분석 준비도(Readiness)'**와 **'분석 성숙도(Maturity)'**라는 두 가지 관점에서 평가를 진행합니다. 이 문항은 분석 준비도 모델의 구성 요소와 관련이 있습니다.\n",
    "\n",
    "분석 준비도(Readiness)의 주요 진단 항목:\n",
    "\n",
    "분석 데이터: 데이터의 충분성, 신뢰성, 적시성 등을 평가합니다.\n",
    "\n",
    "분석 기법: 기법의 적절성, 고급 분석 기법 사용 여부 등을 진단합니다.\n",
    "\n",
    "IT 인프라: 시스템 환경, 도구, 데이터 거버넌스 등 기술적 기반을 확인합니다.\n",
    "\n",
    "기타 항목으로는 분석 인력 및 조직, 분석 업무 파악, 분석 문화 등이 포함됩니다.\n",
    "\n",
    "오답 분석:\n",
    "\n",
    "조직구조: 분석을 수행하는 구체적인 '조직 구조' 그 자체보다는 **'분석 인력 및 조직의 역량'**이나 **'분석 문화'**가 준비도 진단의 핵심 항목으로 분류됩니다. 기업분석 수준진단 모델에서 '조직구조'는 독립된 최상위 진단 항목으로 정의되기보다는 인력 역량의 하위 요소나 문화적 맥락에서 다루어지는 경우가 많아, 보기 중 성격이 가장 거리가 멉니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e226d",
   "metadata": {},
   "source": [
    "# 04. 데이터 사이언티스트(데이터 과학자)가 가져야 할 소프트 스킬(Soft Skills)로 옳은 것은?\n",
    "① 통찰력 있는 분석\n",
    "\n",
    "② 전문지식\n",
    "\n",
    "③ 통계기법\n",
    "\n",
    "④ 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd25a2",
   "metadata": {},
   "source": [
    "정답: ① 통찰력 있는 분석\n",
    "\n",
    "해설: 데이터 사이언티스트에게 필요한 역량은 크게 **하드 스킬(Hard Skill)**과 **소프트 스킬(Soft Skill)**로 나뉩니다.\n",
    "\n",
    "소프트 스킬 (Soft Skill): 가시적인 기술보다는 정성적인 역량을 의미합니다.\n",
    "\n",
    "통찰력 있는 분석: 데이터를 통해 비즈니스 문제를 해결할 수 있는 직관과 통찰력을 발휘하는 능력입니다.\n",
    "\n",
    "설득력 있는 전달: 분석 결과를 이해관계자에게 효과적으로 전달하는 커뮤니케이션 능력(스토리텔링)입니다.\n",
    "\n",
    "다학제적 협력: 다양한 분야의 전문가들과 협력할 수 있는 능력입니다.\n",
    "\n",
    "하드 스킬 (Hard Skill): 객관적으로 증명 가능한 기술적 역량을 의미합니다.\n",
    "\n",
    "전문지식: 해당 비즈니스 도메인에 대한 깊은 이해입니다.\n",
    "\n",
    "통계기법: 데이터 모델링과 가설 검정을 위한 수학적·통계적 지식입니다.\n",
    "\n",
    "알고리즘: 머신러닝 및 데이터 처리를 위한 프로그래밍 및 알고리즘 구현 능력입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84583309",
   "metadata": {},
   "source": [
    "# 05. 데이터 사이언스의 업무와 관계가 없는 것은?\n",
    "① 적합한 모델을 선정한다.\n",
    "\n",
    "② 가정의 한계를 고려한다.\n",
    "\n",
    "③ 해석의 한계에서 벗어난다.\n",
    "\n",
    "④ 분석 모델에 대한 한계점은 배제하고 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7065143",
   "metadata": {},
   "source": [
    "05. 데이터 사이언스의 업무와 관계가 없는 것은?\n",
    "정답: ④ 분석 모델에 대한 한계점은 배제하고 진행한다.\n",
    "\n",
    "해설: 데이터 사이언스는 단순히 데이터를 계산하는 것이 아니라, 모델이 가진 불확실성과 한계를 명확히 인지하고 관리하는 과정입니다.\n",
    "\n",
    "한계점 인지의 중요성: 모든 데이터 분석 모델은 특정 가정(Assumption) 위에서 작동하므로, 그 모델이 설명하지 못하는 영역이나 한계점을 반드시 파악하고 결과에 반영해야 합니다. 이를 배제하고 진행할 경우 잘못된 의사결정으로 이어질 위험이 큽니다.\n",
    "\n",
    "적합한 모델 선정: 문제의 정의에 따라 가장 효율적인 알고리즘과 모델을 선택하는 것은 데이터 사이언스의 핵심 업무입니다.\n",
    "\n",
    "가정의 한계 고려: 통계적 모델링 시 세운 가정이 실제 데이터와 얼마나 부합하는지, 그 한계가 결과에 어떤 영향을 주는지 검토해야 합니다.\n",
    "\n",
    "해석의 한계에서 벗어난다: 이는 주관적인 편향이나 데이터가 보여주는 범위 이상의 자의적인 해석을 경계하고, 객관적인 분석을 지향한다는 의미로 이해할 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfbf982",
   "metadata": {},
   "source": [
    "# 06. 데이터 분석가의 특징 중 옳지 않은 것은?\n",
    "① 데이터 분석의 객관성을 위해 배경지식을 배제해야 한다.\n",
    "\n",
    "② 데이터를 다루는데 필요한 다양한 도구와 기술을 숙달해야 한다.\n",
    "\n",
    "③ 데이터를 수집하고 분석하여 통찰력을 얻어야 한다.\n",
    "\n",
    "④ 데이터 분석결과를 다른 부서나 팀원에게 효과적으로 전달하고 설명할 수 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251db2d4",
   "metadata": {},
   "source": [
    "06. 데이터 분석가의 특징 중 옳지 않은 것은?\n",
    "정답: ① 데이터 분석의 객관성을 위해 배경지식을 배제해야 한다.\n",
    "\n",
    "해설: 데이터 분석가에게 가장 중요한 역량 중 하나는 도메인 지식(Domain Knowledge), 즉 해당 분야의 배경지식입니다.\n",
    "\n",
    "배경지식의 중요성: 데이터 분석은 단순히 숫자를 계산하는 것이 아니라, 비즈니스 맥락 안에서 데이터의 의미를 찾아내는 과정입니다. 배경지식이 있어야 분석 결과가 현실적으로 타당한지 판단하고 실행 가능한 통찰(Actionable Insight)을 도출할 수 있습니다.\n",
    "\n",
    "도구와 기술 숙달: 분석을 위해 SQL, Python, R 등 다양한 분석 도구와 통계적 기술을 능숙하게 다루는 것은 필수적인 하드 스킬입니다.\n",
    "\n",
    "수집과 통찰: 원천 데이터를 수집하고, 정제한 뒤 분석하여 기업에 도움이 되는 통찰력을 얻는 것은 분석가의 핵심 역할입니다.\n",
    "\n",
    "효과적인 전달: 분석 결과가 아무리 뛰어나도 의사결정자나 다른 팀원에게 이해시키지 못하면 의미가 없습니다. 따라서 데이터 시각화나 스토리텔링을 통한 커뮤니케이션 능력이 매우 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9987ee",
   "metadata": {},
   "source": [
    "# 07. 하둡 분산파일시스템(Hadoop Distributed File System, HDFS)의 설명으로 옳은 것은?\n",
    "① 여러 데이터를 한 곳에 저장할 수 있다.\n",
    "\n",
    "② 블록당 10MB이하의 제한이 있다.\n",
    "\n",
    "③ 범용 하드웨어보다는 고성능 컴퓨터를 주로 사용한다.\n",
    "\n",
    "④ 네임노드가 망가지면 정상적인 작동을 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0128e668",
   "metadata": {},
   "source": [
    "정답: ④ 네임노드가 망가지면 정상적인 작동을 못한다.\n",
    "\n",
    "해설: 하둡 분산파일시스템(HDFS)은 대용량 데이터를 분산 저장하기 위해 설계된 시스템으로, 관리 중심인 '네임노드'의 역할이 절대적입니다.\n",
    "\n",
    "네임노드의 중요성: 네임노드(NameNode)는 HDFS의 메타데이터(파일 이름, 위치, 블록 정보 등)를 관리하는 마스터 노드입니다. 만약 네임노드에 장애가 발생하면 클라이언트는 데이터의 위치를 알 수 없게 되어 시스템 전체가 중단되는 **단일 장애점(Single Point of Failure)**이 됩니다.\n",
    "\n",
    "분산 저장: HDFS는 데이터를 한 곳에 저장하는 것이 아니라, 여러 대의 데이터노드(DataNode)에 나누어 복제하여 저장합니다.\n",
    "\n",
    "블록 크기: HDFS의 기본 블록 크기는 보통 64MB 또는 128MB 이상으로, 매우 큰 단위의 블록을 사용하여 대규모 데이터 처리 효율을 높입니다.\n",
    "\n",
    "범용 하드웨어: HDFS는 고가의 고성능 컴퓨터가 아닌, 저렴한 일반 사양의 서버(Commodity Hardware)들을 연결하여 구축하도록 설계되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa0b32",
   "metadata": {},
   "source": [
    "# 08. 다음 중 빅데이터 플랫폼 계층구조의 설명으로 옳지 않은 것은?\n",
    "① 소프트웨어 계층, 플랫폼 계층, 인프라스트럭처 계층이 있다.\n",
    "\n",
    "② 소프트웨어 계층은 앱과 관련이 있다.\n",
    "\n",
    "③ 플랫폼 계층은 데이터 제공 관리를 한다.\n",
    "\n",
    "④ 인프라스트럭처 계층은 데이터 수집, 저장, 정제를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b06d7",
   "metadata": {},
   "source": [
    "08. 다음 중 빅데이터 플랫폼 계층구조의 설명으로 옳지 않은 것은?\n",
    "정답: ④ 인프라스트럭처 계층은 데이터 수집, 저장, 정제를 한다.\n",
    "\n",
    "해설: 빅데이터 플랫폼은 일반적으로 사용자의 목적에 따라 여러 계층으로 나뉘며, 각 계층은 고유의 역할을 수행합니다.\n",
    "\n",
    "인프라스트럭처 계층 (Infrastructure Layer): 플랫폼의 가장 하단에 위치하며, 데이터를 실제로 저장하고 처리하기 위한 **하드웨어 자원(서버, 네트워크, 스토리지)**을 제공하는 역할을 합니다. 데이터의 수집, 저장, 정제와 같은 논리적인 프로세스는 주로 그 윗단계인 플랫폼 계층에서 담당합니다.\n",
    "\n",
    "플랫폼 계층 (Platform Layer): 데이터 분석에 필요한 환경을 제공합니다. 데이터의 수집, 저장, 처리(정제), 분석 및 관리 기능을 수행하며, 하둡(Hadoop)이나 스파크(Spark) 같은 프레임워크가 이 계층에 해당합니다.\n",
    "\n",
    "소프트웨어 계층 (Software Layer): 최상위 계층으로, 분석된 데이터를 시각화하거나 특정 비즈니스 목적에 맞게 활용하는 응용 프로그램(App) 및 인터페이스를 의미합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125909cc",
   "metadata": {},
   "source": [
    "# 09. 데이터 3법에 해당하지 않는 것은?\n",
    "① 개인정보 보호법\n",
    "\n",
    "② 정보통신망 이용촉진 및 정보보호 등에 관한 법률\n",
    "\n",
    "③ 공공데이터 제공 및 이용 활성화에 관한 법률\n",
    "\n",
    "④ 신용정보의 이용 및 보호에 관한 법률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfab7cd",
   "metadata": {},
   "source": [
    "정답: ③ 공공데이터 제공 및 이용 활성화에 관한 법률\n",
    "\n",
    "해설: 데이터 3법은 4차 산업혁명 시대에 맞춰 개인정보의 보호를 넘어 안전한 활용을 도모하기 위해 개정된 3개의 법률을 묶어서 부르는 명칭입니다.\n",
    "\n",
    "데이터 3법의 구성:\n",
    "\n",
    "개인정보 보호법: 개인정보 보호의 체계를 일원화하고 가명정보의 개념을 도입하여 데이터를 활용할 수 있는 법적 근거를 마련했습니다.\n",
    "\n",
    "정보통신망법 (정보통신망 이용촉진 및 정보보호 등에 관한 법률): 개인정보 관련 규정을 개인정보 보호법으로 이관하여 중복 규제를 해소했습니다.\n",
    "\n",
    "신용정보법 (신용정보의 이용 및 보호에 관한 법률): 금융 분야에서 빅데이터 활용을 활성화하고 마이데이터(MyData) 산업의 근거를 마련했습니다.\n",
    "\n",
    "오답 분석:\n",
    "\n",
    "공공데이터법 (공공데이터 제공 및 이용 활성화에 관한 법률): 공공기관이 보유·관리하는 데이터를 민간에 개방하는 것과 관련된 법률로, 데이터 활용 측면에서는 중요하지만 일반적으로 말하는 '데이터 3법'의 범주에는 포함되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8758efcc",
   "metadata": {},
   "source": [
    "# 10. 다음 중 데이터 일부 또는 전체를 식별할 수 없도록 노이즈나 공백 등으로 대체하는 것은?\n",
    "① 가명처리\n",
    "\n",
    "② 총계처리\n",
    "\n",
    "③ 데이터 범주화\n",
    "\n",
    "④ 데이터 마스킹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254db6e",
   "metadata": {},
   "source": [
    "정답: ④ 데이터 마스킹\n",
    "\n",
    "해설: 데이터 마스킹은 개인을 식별할 수 있는 데이터의 일부 또는 전체를 공백( ), 노이즈(*, #), 혹은 임의의 문자로 대체하여 노출을 차단하는 기술입니다.\n",
    "\n",
    "데이터 마스킹 (Data Masking): 이름의 성만 남기고 나머지는 *로 표시하거나(예: 홍*동), 주민등록번호 뒷자리를 숨기는 등 데이터의 형태는 유지하되 식별성을 제거하는 방식입니다.\n",
    "\n",
    "가명처리 (Pseudonymization): 개인정보의 일부를 삭제하거나 대체하여 추가 정보 없이는 특정 개인을 알아볼 수 없도록 하는 처리입니다. (예: 홍길동 → 임꺽정)\n",
    "\n",
    "총계처리 (Aggregation): 데이터의 개별 값을 보여주는 대신 전체의 합계나 평균값을 보여주어 개인의 값을 가리는 방식입니다. (예: 20대 남성 소득 평균 300만 원)\n",
    "\n",
    "데이터 범주화 (Categorization/Data Reduction): 구체적인 수치를 구간(범위)으로 변환하여 식별성을 낮추는 방식입니다. (예: 28세 → 20대)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a1ce8",
   "metadata": {},
   "source": [
    "# 11. 다음 중 분석기획에서 우선순위 고려요소가 아닌 것은?\n",
    "① 중요도\n",
    "\n",
    "② ROI\n",
    "\n",
    "③ 분석가능성\n",
    "\n",
    "④ 분석 데이터 적용 수준"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a587b",
   "metadata": {},
   "source": [
    "정답: ④ 분석 데이터 적용 수준\n",
    "\n",
    "해설: 분석 과제의 우선순위를 결정할 때는 비즈니스적 가치와 실행 가능성을 동시에 고려해야 합니다. 일반적으로 다음과 같은 요소들을 평가합니다:\n",
    "\n",
    "중요도 (Strategic Importance): 해당 과제가 비즈니스 목표 달성에 얼마나 필수적인지, 전략적 가치가 얼마나 높은지를 판단합니다.\n",
    "\n",
    "ROI (Return on Investment): 투자 대비 효과를 의미하며, 비즈니스 효과(Value)와 투입 비용 및 난이도(Volume)를 함께 고려하여 경제적 타당성을 분석합니다.\n",
    "\n",
    "분석 가능성 (Feasibility): 현재 가용 가능한 데이터가 있는지, 기술적으로 구현이 가능한지 등 실행 가능 여부를 판단합니다.\n",
    "\n",
    "오답 분석:\n",
    "\n",
    "분석 데이터 적용 수준: 이는 우선순위 결정 요소라기보다는 분석 마스터 플랜 수립 시 '데이터의 범위 및 방식' 등을 고려하는 세부 항목에 가깝습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3de96e",
   "metadata": {},
   "source": [
    "# 12. CRISP-DM 절차에 대해 옳은 것은?\n",
    "① 데이터 이해 - 업무 이해 - 전개 - 평가\n",
    "\n",
    "② 데이터 이해 - 업무 이해 - 평가 - 전개\n",
    "\n",
    "③ 업무 이해 - 데이터 이해 - 평가 - 전개\n",
    "\n",
    "④ 업무 이해 - 데이터 이해 - 전개 - 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bffdd1",
   "metadata": {},
   "source": [
    "정답: ③ 업무 이해 - 데이터 이해 - 평가 - 전개\n",
    "\n",
    "해설: CRISP-DM(Cross-Industry Standard Process for Data Mining)은 데이터 마이닝을 위한 계층적 프로세스 모델로, 총 6단계의 순환 과정을 거칩니다. 문제의 보기는 이 단계 중 주요 흐름을 묻고 있습니다.\n",
    "\n",
    "1단계: 업무 이해 (Business Understanding) 비즈니스 관점에서 프로젝트의 목적과 요구사항을 이해하고 설정하는 단계입니다.\n",
    "\n",
    "2단계: 데이터 이해 (Data Understanding) 데이터 수집 및 속성을 분석하여 데이터 품질을 확인하는 단계입니다.\n",
    "\n",
    "3단계: 데이터 준비 (Data Preparation) 분석을 위해 데이터를 정제하고 변환하는 단계입니다.\n",
    "\n",
    "4단계: 모델링 (Modeling) 다양한 분석 기법과 알고리즘을 선택하여 적용하는 단계입니다.\n",
    "\n",
    "5단계: 평가 (Evaluation) 모델이 비즈니스 목적에 부합하는지 성능을 평가하고 검증하는 단계입니다.\n",
    "\n",
    "6단계: 전개 (Deployment) 완성된 모델을 실무 환경에 적용하고 운영하는 최종 단계입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8886aa6e",
   "metadata": {},
   "source": [
    "# 13. 빅데이터 분석기획 단계에서 하는 일이 아닌 것은?\n",
    "① 데이터 준비\n",
    "\n",
    "② 위험요인 계획\n",
    "\n",
    "③ 설계 및 계획\n",
    "\n",
    "④ 비즈니스 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e857ff9",
   "metadata": {},
   "source": [
    "13. 빅데이터 분석기획 단계에서 하는 일이 아닌 것은?\n",
    "정답: ① 데이터 준비\n",
    "\n",
    "해설: 빅데이터 분석 방법론에서 '분석기획(Planning)' 단계는 프로젝트의 목표를 설정하고 실행 계획을 수립하는 최상위 단계입니다.\n",
    "\n",
    "분석기획 단계의 주요 과업:\n",
    "\n",
    "비즈니스 이해: 도메인의 이슈를 파악하고 분석의 목적을 정의합니다.\n",
    "\n",
    "프로젝트 범위 정의: 분석의 범위를 설정하고 데이터 확보 가능성을 검토합니다.\n",
    "\n",
    "위험요인 계획: 프로젝트 수행 중 발생할 수 있는 데이터 품질 저하, 일정 지연 등의 리스크에 대한 대응 방안을 수립합니다.\n",
    "\n",
    "설계 및 계획: 분석을 위한 구체적인 일정, 인력 투입, 시스템 구성 등 수행 계획을 설계합니다.\n",
    "\n",
    "오답 분석:\n",
    "\n",
    "데이터 준비: 이는 기획 다음 단계인 '데이터 준비(Preparing)' 단계에서 수행하는 일입니다. 이 단계에서는 실제 데이터를 수집, 저장하고 전처리하는 실무적인 작업이 이루어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d77599",
   "metadata": {},
   "source": [
    "# 14. 다음 중 비정형 데이터가 아닌 것은?\n",
    "① 동영상 파일\n",
    "\n",
    "② 오디오 파일\n",
    "\n",
    "③ 문서\n",
    "\n",
    "④ 판매가격 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea648529",
   "metadata": {},
   "source": [
    "정답: ④ 판매가격 데이터\n",
    "\n",
    "해설: 데이터는 그 형태와 구조화된 정도에 따라 정형, 반정형, 비정형 데이터로 분류됩니다.\n",
    "\n",
    "정형 데이터 (Structured Data): 고정된 필드에 저장되어 있으며, 형태(스키마)가 정해져 있는 데이터입니다. 보통 데이터베이스(RDB)의 테이블이나 엑셀 시트 형태로 관리됩니다. 판매가격 데이터는 수치 데이터로서 정해진 규격에 따라 저장되므로 전형적인 정형 데이터에 해당합니다.\n",
    "\n",
    "비정형 데이터 (Unstructured Data): 고정된 구조가 없고 형태가 일정하지 않은 데이터입니다.\n",
    "\n",
    "동영상 및 오디오 파일: 미디어 파일은 내부 구조를 수치나 텍스트로 바로 정형화하기 어렵습니다.\n",
    "\n",
    "문서: 자유로운 형식으로 작성된 보고서나 텍스트 파일 등은 비정형 데이터의 대표적인 예입니다.\n",
    "\n",
    "참고 - 반정형 데이터 (Semi-structured Data): 행과 열의 구조는 아니지만, 태그나 메타데이터를 통해 구조 정보를 포함하고 있는 데이터입니다. (예: HTML, XML, JSON 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8659eaf",
   "metadata": {},
   "source": [
    "# 15. 반정형 데이터는 테이블의 행과 열로 구조화되어 있지는 않으나 스키마 및 메타데이터 특성을 가지고 있다. 다음 중 반정형 데이터에 해당되지 않는 것은?\n",
    "① HTML\n",
    "\n",
    "② XML\n",
    "\n",
    "③ RDB\n",
    "\n",
    "④ RDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8913d",
   "metadata": {},
   "source": [
    "정답: ③ RDB\n",
    "\n",
    "해설: 반정형 데이터는 고정된 필드에 저장되지는 않지만, 데이터 내부에 구조에 대한 정보(메타데이터)가 포함된 데이터 형식을 말합니다.\n",
    "\n",
    "RDB (Relational Database, 관계형 데이터베이스): 테이블의 **행(Row)과 열(Column)**로 엄격하게 구조화된 데이터를 저장하고 관리하는 시스템으로, 대표적인 정형 데이터입니다.\n",
    "\n",
    "HTML (HyperText Markup Language): 태그를 사용하여 문서의 구조를 정의하는 대표적인 반정형 데이터입니다.\n",
    "\n",
    "XML (Extensible Markup Language): 사용자 정의 태그를 통해 데이터의 의미와 구조를 설명하는 메타데이터를 포함하는 반정형 데이터입니다.\n",
    "\n",
    "RDF (Resource Description Framework): 웹상의 자원 식별과 상호 운용을 위해 구조화된 정보를 표현하는 모델로, 반정형 데이터의 일종입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b485a022",
   "metadata": {},
   "source": [
    "# 16. 개인정보 비식별화 기술로 수치적 개인정보를 임의의 수 기준으로 올림 또는 내림하는 기법은?\n",
    "① 암호화\n",
    "\n",
    "② 랜덤라운딩\n",
    "\n",
    "③ 임의 잡음 추가\n",
    "\n",
    "④ 식별자 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3b4a0",
   "metadata": {},
   "source": [
    "정답: ② 랜덤라운딩\n",
    "\n",
    "해설: 수치 데이터를 처리할 때 정해진 수치를 그대로 노출하지 않고, 특정 기준에 따라 수치를 변경하여 개인을 식별하기 어렵게 만드는 기술입니다.\n",
    "\n",
    "랜덤라운딩 (Random Rounding): 수치형 데이터를 임의의 수(예: 5 또는 10)를 기준으로 올림, 내림 또는 반올림하여 처리하는 기법입니다. 예를 들어, 실제 소득이 3,450,200원일 때 이를 3,450,000원이나 3,500,000원 등으로 표현하는 방식입니다.\n",
    "\n",
    "암호화 (Encryption): 데이터를 알고리즘을 이용해 읽을 수 없는 형태의 암호문으로 변환하는 기술입니다.\n",
    "\n",
    "임의 잡음 추가 (Adding Random Noise): 데이터에 임의의 수치(잡음)를 더하거나 빼서 실제 값을 가리는 기법입니다. 라운딩과는 달리 수치 자체가 무작위로 변합니다.\n",
    "\n",
    "식별자 삭제 (Removing Identifiers): 데이터 집합에서 성명, 주민등록번호, 주소 등 특정 개인을 직접 식별할 수 있는 정보를 완전히 제거하는 기법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a50a9b",
   "metadata": {},
   "source": [
    "# 17. 다음 중 정형 데이터 품질검증방법으로 옳지 않은 것은?\n",
    "① 진단 대상 정의 : 품질 이슈에 대한 수요 및 현황을 조사해 품질 진단 대상 데이터베이스를 선정하고, 진단 방향성을 정의\n",
    "\n",
    "② 품질 진단 실시 : 대상에 대한 상세 수준의 품질 진단 계획 수립 후 품질 진단 영역별 진단을 실시\n",
    "\n",
    "③ 진단 결과 분석 : 오류 원인 분석, 업무 영향도 분석을 통해 개선 과제를 정의\n",
    "\n",
    "④ 업무규칙 : 진단 비즈니스 특성만 알 수 있고, 데이터 오류는 검증 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8e33d",
   "metadata": {},
   "source": [
    "정답: ④ 업무규칙 : 진단 비즈니스 특성만 알 수 있고, 데이터 오류는 검증 못함\n",
    "\n",
    "해설: 데이터 품질 진단에서 **업무규칙(Business Rule)**은 데이터가 비즈니스 목적에 맞게 올바르게 저장되어 있는지를 검증하는 핵심 수단입니다.\n",
    "\n",
    "업무규칙의 역할: 데이터의 정합성을 체크하기 위한 논리적 규칙(예: '나이는 0보다 커야 한다', '퇴사일은 입사일보다 빨라질 수 없다')을 정의하는 것입니다. 이를 통해 실제 데이터의 오류(유효성, 일관성 위배 등)를 직접적으로 검증할 수 있습니다.\n",
    "\n",
    "진단 대상 정의: 품질 관리의 범위를 정하고 전략을 수립하는 올바른 단계입니다.\n",
    "\n",
    "품질 진단 실시: 계획에 따라 데이터 프로파일링 등을 통해 실제 품질을 측정하는 과정입니다.\n",
    "\n",
    "진단 결과 분석: 발견된 품질 결함의 원인을 파악하고 이를 해결하기 위한 과제를 도출하는 단계입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beaf99b",
   "metadata": {},
   "source": [
    "# 18. 데이터 누락 시 다른 데이터 특징 값을 이용해 누락된 값을 처리하는가?\n",
    "① 일관성\n",
    "\n",
    "② 정확성\n",
    "\n",
    "③ 완전성\n",
    "\n",
    "④ 효율성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe98fb1",
   "metadata": {},
   "source": [
    "정답: ③ 완전성\n",
    "\n",
    "해설: 데이터 품질 지표 중 **완전성(Completeness)**은 데이터가 누락 없이 충분히 수집되었는지를 나타내는 척도입니다.\n",
    "\n",
    "완전성 (Completeness): 데이터 집합 내에 필요한 정보가 누락되지 않고 모두 채워져 있는 상태를 의미합니다. 결측치(데이터 누락)가 발생했을 때 이를 다른 데이터의 특징값(평균, 최빈값 등)을 이용해 대체하거나 처리하는 과정은 데이터의 완전성을 높이기 위한 작업입니다.\n",
    "\n",
    "일관성 (Consistency): 데이터가 서로 모순되지 않고 일관된 형태와 값을 유지하는 특성입니다.\n",
    "\n",
    "정확성 (Accuracy): 데이터가 실제 세계의 사실을 정확하게 반영하고 있는지를 의미합니다.\n",
    "\n",
    "효율성 (Efficiency): 데이터 처리나 분석 과정이 자원을 최소한으로 사용하여 목적을 달성하는 정도를 뜻합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddbbeb",
   "metadata": {},
   "source": [
    "# 19. 다음 중 오토샤딩(Auto-Sharding)을 사용하며, 처리속도가 빠른 Nosql DB의 종류는?\n",
    "① Casandra\n",
    "\n",
    "② Redis\n",
    "\n",
    "③ Mongodb\n",
    "\n",
    "④ Couchdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65bd959",
   "metadata": {},
   "source": [
    "정답: ③ MongoDB\n",
    "\n",
    "해설: **오토샤딩(Auto-Sharding)**은 데이터를 여러 서버에 자동으로 분산하여 저장하는 기술로, 대규모 데이터를 처리할 때 성능과 확장성을 보장하기 위한 핵심 기능입니다.\n",
    "\n",
    "MongoDB: 대표적인 문서 지향(Document-oriented) NoSQL 데이터베이스입니다. 오토샤딩 기능을 내장하고 있어 데이터 양이 늘어나면 자동으로 여러 노드에 데이터를 분산하며, 읽기 및 쓰기 성능이 매우 빠릅니다.\n",
    "\n",
    "Cassandra: 키-값(Key-Value)과 컬럼 패밀리(Column Family) 모델을 혼합한 형태로, 고가용성과 확장성에 강점이 있으나 오토샤딩이라는 용어보다는 주로 '파티셔닝' 개념으로 설명됩니다.\n",
    "\n",
    "Redis: 인메모리(In-memory) 기반의 키-값 저장소입니다. 속도가 매우 빠르지만 주로 캐싱이나 세션 관리에 사용되며, 자동 분산 처리 방식이 MongoDB의 샤딩 구조와는 차이가 있습니다.\n",
    "\n",
    "CouchDB: MongoDB와 같은 문서 지향 DB이지만, 동기화와 복제에 특화되어 있으며 샤딩보다는 마스터-마스터 복제 방식에 더 중점을 둡니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa939bd",
   "metadata": {},
   "source": [
    "# 20. 분석 마스터 플랜 수립에서의 범위 및 방식의 고려요소로 옳지 않은 것은?\n",
    "① 분석 데이터 적용 수준\n",
    "\n",
    "② 실행용이성\n",
    "\n",
    "③ 기술 적용 수준\n",
    "\n",
    "④ 업무 내재화 적용 수준"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8cab16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c14411c",
   "metadata": {},
   "source": [
    "# 21. 데이터 전처리 방법으로 잘못된 것은?\n",
    "① 레거시 시스템으로만 전처리를 진행해야 한다.\n",
    "\n",
    "② 비정형 데이터는 정제를 진행해야 한다.\n",
    "\n",
    "③ 전처리 시 삭제 및 수정 진행이 가능하다.\n",
    "\n",
    "④ 데이터를 통합하여 정제를 진행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d788cd66",
   "metadata": {},
   "source": [
    "정답: ② 실행용이성\n",
    "\n",
    "해설: 분석 마스터 플랜을 수립할 때, 과제의 우선순위를 결정하는 요소와 범위 및 방식을 결정하는 요소를 구분하는 것이 중요합니다.\n",
    "\n",
    "범위 및 방식의 고려요소: 분석 과제를 어떤 수준으로, 어느 범위까지 적용할 것인지를 결정하는 기준입니다.\n",
    "\n",
    "분석 데이터 적용 수준: 데이터의 양, 질, 범위 등을 고려합니다.\n",
    "\n",
    "기술 적용 수준: 사용될 분석 알고리즘이나 플랫폼의 기술적 난이도를 고려합니다.\n",
    "\n",
    "업무 내재화 적용 수준: 분석 결과가 실제 업무 프로세스에 얼마나 깊게 통합되고 내재화될지를 고려합니다.\n",
    "\n",
    "오답 분석:\n",
    "\n",
    "실행용이성: 이는 과제의 **우선순위(Prioritization)**를 평가할 때 주로 사용하는 기준입니다. 비즈니스 효과(ROI)와 함께 해당 과제를 얼마나 쉽게 실행할 수 있는지를 따져서 우선순위를 정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9cd601",
   "metadata": {},
   "source": [
    "# 22. 다음 중 데이터 정제 방법으로 옳은 것은?\n",
    "① 데이터 손실을 최소화하기 위해 누락된 데이터를 임의의 데이터로 채운다.\n",
    "\n",
    "② 구분자가 포함되어 있을 수 있으니 처리해야 한다.\n",
    "\n",
    "③ 이상치 데이터를 모두 제거한다.\n",
    "\n",
    "④ 단일 데이터 포인트를 기반으로 모든 데이터를 수정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db255e8",
   "metadata": {},
   "source": [
    "정답: ② 구분자가 포함되어 있을 수 있으니 처리해야 한다.\n",
    "\n",
    "해설: 데이터 정제(Data Cleaning)는 데이터의 오류를 수정하고 분석에 적합한 형태로 다듬는 과정입니다.\n",
    "\n",
    "구분자 처리 (Separator Handling): 텍스트 기반 데이터(CSV 등)를 불러올 때, 데이터 값 안에 콤마(,)나 탭(\\t) 같은 구분자가 포함되어 있으면 열이 어긋날 수 있으므로 이를 적절히 처리해야 합니다.\n",
    "\n",
    "누락 데이터 처리: 임의의 데이터로 무조건 채우는 것은 데이터의 왜곡을 초래합니다. 누락된 값(결측치)은 평균값, 중앙값 등 통계적 근거가 있는 값으로 대체하거나 분석 목적에 따라 삭제 여부를 결정해야 합니다.\n",
    "\n",
    "이상치 처리: 이상치를 무조건 제거하는 것은 옳지 않습니다. 이상치가 단순 입력 오류인지, 아니면 비즈니스적으로 중요한 의미를 갖는 희귀 사례(예: 부정 결제)인지 먼저 판별한 후 처리 방식을 결정해야 합니다.\n",
    "\n",
    "데이터 수정: 모든 데이터를 단일 포인트를 기준으로 수정하는 것은 데이터의 다양성을 훼손하며, 데이터 정제는 각 필드의 도메인 규칙과 비즈니스 로직에 맞게 수행되어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38de75c",
   "metadata": {},
   "source": [
    "# 23. 다음 표를 보고 알맞은 것을 고르시오.\n",
    "| year | age | health_ins | wage | |---|---|---|---|---| | Min. \n",
    "\n",
    "| 2003 | 18.00 | 1. Yes: 2083 | 20.09 | | 1st Qu.\n",
    "\n",
    "| 2004 | 33.75 | 2. No: 917 | 85.38 | | Median \n",
    "\n",
    "| 2006 | 42.00 | NA's: 10 | 104.92 | | Mean \n",
    "\n",
    "| 2006 | 42.41 | | 111.70 | | 3rd Qu. \n",
    "\n",
    "| 2008 | 51.00 | | 128.68 | | Max.\n",
    "\n",
    "| 2009 | 80.00 | | 318.34 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff9db55",
   "metadata": {},
   "source": [
    "① year는 Numerical한 변수이다.\n",
    "\n",
    "② 결측값은 age 변수와 health_ins 변수가 가지고 있다.\n",
    "\n",
    "③ wage 변수는 우측으로 기울어져 있다.\n",
    "\n",
    "④ wage 변수의 max값은 이상값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24d5eb",
   "metadata": {},
   "source": [
    "정답: ③ wage 변수는 우측으로 기울어져 있다.\n",
    "\n",
    "해설:제시된 표는 R 프로그램의 summary() 함수 결과로, 각 변수의 기술 통계량과 데이터 분포를 보여줍니다.\n",
    "\n",
    "③ wage 변수는 우측으로 기울어져 있다 (정답):wage 변수의 중앙값(Median)은 104.92이고 평균(Mean)은 111.70입니다.\n",
    "\n",
    "통계학적으로 평균이 중앙값보다 크면($Mean > Median$) 데이터 분포의 꼬리가 오른쪽으로 길게 늘어진 우측 왜도(Positive Skewness) 형태를 띱니다.\n",
    "① year는 Numerical한 변수이다 (오답):\n",
    "\n",
    "year 컬럼에 대해 기술 통계량(Min, Mean 등)이 출력되었으나, 연도는 수치적 크기보다는 특정 시점을 나타내는 범주형(Factor) 변수 혹은 이산형 변수로 취급하는 것이 일반적입니다.\n",
    "\n",
    "② 결측값은 age 변수와 health_ins 변수가 가지고 있다 (오답):\n",
    "\n",
    "표에서 결측값은 NA's 항목으로 표시됩니다.\n",
    "\n",
    "health_ins 변수에는 NA's: 10이라는 표시가 있어 결측치가 확인되지만, age 변수에는 해당 표시가 없으므로 결측치가 존재하지 않습니다.\n",
    "\n",
    "④ wage 변수의 max값은 이상값이다 (오답):\n",
    "\n",
    "wage의 최대값(318.34)이 평균에 비해 상당히 높은 편이지만, 단순히 요약 수치만 보고 이를 확정적인 **이상값(Outlier)**으로 단정할 수는 없습니다. 보통 박스플롯(Boxplot)이나 IQR 방식을 통해 검증해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce861b2",
   "metadata": {},
   "source": [
    "# 24. 다음 중 혈액형에 대해 결측치가 발생했을 때 대체할 값으로 적절한 것은?\n",
    "① 최빈값\n",
    "\n",
    "② 기하평균\n",
    "\n",
    "③ 중앙값\n",
    "\n",
    "④ 산술평균"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1875c862",
   "metadata": {},
   "source": [
    "정답: ① 최빈값\n",
    "\n",
    "해설: 데이터의 특성에 따라 결측치를 대체하는 적절한 통계량이 달라집니다.\n",
    "\n",
    "최빈값(Mode): 혈액형(A, B, O, AB)은 수치로 계산할 수 없는 범주형(명목형) 데이터입니다. 범주형 데이터에서 가장 합리적인 대체값은 데이터 집합에서 가장 빈번하게 등장하는 항목인 '최빈값'입니다.\n",
    "\n",
    "산술평균 / 중앙값 / 기하평균: 이들은 모두 수치형 데이터(예: 키, 몸무게, 소득 등)에서 사용하는 중심경향값입니다. 혈액형은 더하거나 나눌 수 없는 문자 형태의 데이터이므로 이러한 산술적 계산이 불가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605d165",
   "metadata": {},
   "source": [
    "# 25. 일변량일 때 이상값 검출방법으로 옳지 않은 것은?\n",
    "① 사분위수를 이용한다.\n",
    "\n",
    "② 산포도를 그려본다.\n",
    "\n",
    "③ 상자그림에서 상자를 벗어나는 값을 이상치로 판단한다.\n",
    "\n",
    "④ 3표준편차보다 큰 경우 이상치로 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80314c5e",
   "metadata": {},
   "source": [
    "정답: ③ 상자그림에서 상자를 벗어나는 값을 이상치로 판단한다.\n",
    "\n",
    "해설:이상값(Outlier)은 대다수의 데이터 범위에서 크게 벗어난 값을 의미하며, 일변량 데이터에서는 주로 분포의 끝부분을 확인하여 검출합니다.\n",
    "\n",
    "③ 상자그림에서 상자를 벗어나는 값을 이상치로 판단한다 \n",
    "(오답):상자그림(Boxplot)에서 '상자' 자체는 제1사분위수($Q1$)부터 제3사분위수($Q3$)까지의 범위를 의미합니다.이상치는 상자를 벗어난 값이 아니라, \n",
    "\n",
    "상자 양 끝에서 $1.5 \\times IQR$ (사분위수 범위)보다 더 멀리 떨어진 울타리(Fence) 밖의 값을 의미합니다. \n",
    "\n",
    "상자 바로 밖의 값들은 여전히 정상 범위에 포함될 수 있습니다.\n",
    "\n",
    "① 사분위수를 이용한다 (옳음):데이터를 4등분 하여 $Q1$, $Q3$를 구하고 그 차이인 $IQR$을 이용해 이상치를 정의하는 방식(보통 $1.5 \\times IQR$ 기준)은 매우 일반적인 방법입니다.\n",
    "\n",
    "② 산포도를 그려본다 (옳음):데이터를 시각화하여 점들이 모여 있는 군집에서 멀리 떨어져 있는 개별 점을 육안으로 확인하는 것은 직관적인 이상치 검출 방법입니다.\n",
    "\n",
    "④ 3표준편차보다 큰 경우 이상치로 본다 (옳음):데이터가 정규분포를 따른다고 가정할 때, \n",
    "\n",
    "**평균으로부터 $\\pm3$표준편차($3\\sigma$)**를 벗어날 확률은 약 0.3% 미만입니다. 따라서 이 범위를 벗어나는 값을 이상치로 간주하는 $3\\sigma$ 원칙을 자주 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea4df30",
   "metadata": {},
   "source": [
    "# 26. 아래 표를 보고 옳지 않은 것을 고르시오.\n",
    "\n",
    "번호 : 12345678910\n",
    "\n",
    "값 : 234567891050"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0acae",
   "metadata": {},
   "source": [
    "① 왜도를 사용하여 데이터의 쏠림 정도를 파악한다.\n",
    "\n",
    "② 분포의 기울어진 정도를 설명한 통계량을 분석한다.\n",
    "\n",
    "③ 좌측 방향으로 기울어져 있다.\n",
    "\n",
    "④ 이상치가 없음을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353af7fd",
   "metadata": {},
   "source": [
    "정답: ④ 이상치가 없음을 확인할 수 있다.\n",
    "\n",
    "해설: 제시된 데이터의 값들을 나열해 보면 2, 3, 4, 5, 6, 7, 8, 9, 10까지는 1씩 일정하게 증가하지만, 마지막 10번 데이터는 **50**으로 다른 값들에 비해 현저히 큽니다.\n",
    "\n",
    "④ 이상치가 없음을 확인할 수 있다 (오답): 데이터 중 50은 나머지 값들의 분포(2~10)에서 매우 멀리 떨어져 있습니다. 육안으로 보아도 전형적인 **이상치(Outlier)**에 해당하며, 통계적 기준(IQR이나 표준편차)을 적용하더라도 이상치로 판정될 가능성이 매우 높습니다.\n",
    "\n",
    "① 왜도를 사용하여 데이터의 쏠림 정도를 파악한다 (옳음): 왜도(Skewness)는 데이터 분포의 비대칭성을 측정하는 지표로, 특정 방향으로 데이터가 얼마나 쏠려 있는지 파악할 때 사용합니다.\n",
    "\n",
    "② 분포의 기울어진 정도를 설명한 통계량을 분석한다 (옳음): 왜도의 정의 자체가 분포가 어느 한쪽으로 치우치거나 기울어진 정도를 나타내는 통계량입니다.\n",
    "\n",
    "③ 좌측 방향으로 기울어져 있다 (옳음): 오른쪽에 매우 큰 값(50)이 존재하면 분포의 꼬리가 오른쪽으로 길게 늘어집니다. 이를 **우측 왜도(Positive Skewness)**라고 하며, 분포의 덩어리(최빈값/중앙값)는 좌측으로 치우친(기울어진) 형태가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f770c4",
   "metadata": {},
   "source": [
    "# 27. 다음 중 계량적 수치에 해당하지 않는 것은?\n",
    "① 직장인의 평균 업무시간\n",
    "\n",
    "② 같은 반 학생의 무게\n",
    "\n",
    "③ 기업의 매출액\n",
    "\n",
    "④ 개인의 견해/의견"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f46db",
   "metadata": {},
   "source": [
    "정답: ④ 개인의 견해/의견\n",
    "\n",
    "해설: 데이터는 성격에 따라 수치로 표현 가능한 **정량적 데이터(Quantitative Data)**와 수치로 표현하기 어려운 **정성적 데이터(Qualitative Data)**로 나뉩니다.\n",
    "\n",
    "④ 개인의 견해/의견 (정답): 사람의 생각, 느낌, 의견 등은 언어나 문자 형태로 표현되는 대표적인 정성적 데이터입니다. 이는 \"매우 좋다\", \"나쁘다\"와 같이 주관적이며, 그 자체로는 산술적인 계산이 불가능한 비계량적 정보입니다.\n",
    "\n",
    "① 직장인의 평균 업무시간 (오답): 시간은 셀 수 있고 계산이 가능한 수치형(연속형) 데이터입니다.\n",
    "\n",
    "② 같은 반 학생의 무게 (오답): 무게(체중) 또한 측정 가능하며 산술 연산이 가능한 수치형(연속형) 데이터입니다.\n",
    "\n",
    "③ 기업의 매출액 (오답): 화폐 단위로 표현되는 금액은 대표적인 수치형(비율 척도) 데이터로, 계량화가 매우 명확한 항목입니다.\n",
    "\n",
    "Tip: * 정량적 데이터(계량적): 양(Quantity)을 측정함. 통계 분석 및 산술 계산이 가능함. (예: 온도, 가격, 횟수 등)\n",
    "\n",
    "정성적 데이터(비계량적): 성질(Quality)을 나타냄. 주로 텍스트, 이미지, 오디오 형태임. (예: 설문 주관식 답변, 인터뷰 내용 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6f5a2",
   "metadata": {},
   "source": [
    "# 28. 변수선택에 대한 설명으로 옳지 않은 것은?\n",
    "① 분산 변수선택 - 분산이 기준치보다 높은 데이터 제거\n",
    "\n",
    "② 단일 변수선택 - 분류 성능 혹은 상관관계가 높은 특성만을 선택\n",
    "\n",
    "③ 모델기반 변수선택 - 결정트리 등으로 특성 중요도가 높은 특성을 선택\n",
    "\n",
    "④ 반복적 변수선택 - 성능에 대해서 가장 좋은 걸 선택하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708789c",
   "metadata": {},
   "source": [
    "정답: ① 분산 변수선택 - 분산이 기준치보다 높은 데이터 제거\n",
    "\n",
    "해설: 변수 선택(Feature Selection)은 모델의 복잡도를 줄이고 성능을 높이기 위해 가장 유의미한 변수들만 골라내는 과정입니다.\n",
    "\n",
    "① 분산 변수선택 (오답): 분산(Variance) 기반 변수 선택은 분산이 너무 낮은(기준치 미만인) 변수를 제거하는 방식입니다. 분산이 낮다는 것은 모든 데이터의 값이 거의 일정하다는 뜻이며, 이는 예측 모델에서 정보로서의 가치(변별력)가 낮다는 것을 의미합니다. 따라서 분산이 높은 데이터는 정보를 많이 담고 있으므로 유지해야 합니다.\n",
    "\n",
    "② 단일 변수선택 (옳음): 개별 변수와 타겟 변수 사이의 상관관계(Correlation)나 통계적 유의성을 평가하여 성능이 높은 특성을 선택하는 방식입니다. (예: 카이제곱 검정, F-검정 등)\n",
    "\n",
    "③ 모델기반 변수선택 (옳음): 머신러닝 모델(랜덤 포레스트, 결정 트리 등) 자체의 알고리즘을 활용하여 각 변수의 **특성 중요도(Feature Importance)**를 계산하고, 중요도가 높은 변수만 추출하는 방식입니다.\n",
    "\n",
    "④ 반복적 변수선택 (옳음): 변수를 하나씩 추가하거나 제거하면서 모델의 성능을 반복적으로 테스트하여 최적의 변수 조합을 찾는 방법입니다. (예: 후진 제거법, 전진 선택법, 단계적 선택법)\n",
    "\n",
    "Tip: 변수 선택의 목적은 '노이즈'를 줄이는 것입니다. 값이 거의 변하지 않는(분산이 낮은) 변수는 분석에 도움이 되지 않는 노이즈로 간주합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d909b978",
   "metadata": {},
   "source": [
    "# 29. 특이값 분해(Singular Value Decomposition, SVD)에 대한 설명 중 옳지 않은 것은?\n",
    "(이미지 하단 텍스트 일부 잘림) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74231914",
   "metadata": {},
   "source": [
    "① 행렬을 세 개의 행렬의 곱으로 분해하는 방법\n",
    "\n",
    "② 차원 축소나 데이터 압축 등에 사용됨\n",
    "\n",
    "③ 모든 행렬에 대하여 적용이 가능함\n",
    "\n",
    "④ 정사각행렬에 대해서만 일반화한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a78463",
   "metadata": {},
   "source": [
    "정답: ④ 정사각행렬에 대해서만 일반화한 것\n",
    "\n",
    "해설:\n",
    "④ 정사각행렬에 대해서만 일반화한 것 \n",
    "\n",
    "(정답): 이 설명은 **고유값 분해(Eigenvalue Decomposition)**에 해당합니다. \n",
    "\n",
    "고유값 분해는 행과 열의 수가 같은 정사각행렬에만 적용할 수 있는 반면, S\n",
    "\n",
    "VD는 행과 열의 크기가 다른 임의의 $m \\times n$ 행렬(직사각형 행렬)에 대해서도 적용이 가능하도록 일반화된 기법입니다.\n",
    "\n",
    "① 행렬을 세 개의 행렬의 곱으로 분해하는 방법 (옳음): 임의의 행렬 $A$를 세 개의 행렬 $U, \\Sigma, V^T$의 곱으로 분해합니다 ($A = U\\Sigma V^T$). 여기서 $U$와 $V$는 직교 행렬이고, $\\Sigma$는 특이값을 대각 성분으로 하는 대각 행렬입니다.\n",
    "\n",
    "② 차원 축소나 데이터 압축 등에 사용됨 (옳음): SVD를 통해 분해된 행렬에서 정보량이 적은 작은 특이값들을 제거하고 다시 결합하면, 데이터의 핵심 특징은 유지하면서 크기를 대폭 줄이는 차원 축소 및 압축이 가능합니다.\n",
    "\n",
    "③ 모든 행렬에 대하여 적용이 가능함 (옳음): SVD의 가장 큰 장점은 행렬의 모양이나 성질에 관계없이 모든 $m \\times n$ 행렬에 대해 정의될 수 있다는 범용성입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71d9e7",
   "metadata": {},
   "source": [
    "# 30. 다음 파생변수를 생성하는 방법으로 옳지 않은 것은?\n",
    "① 컬럼이름 변경\n",
    "\n",
    "② 컬럼별 데이터 나누기\n",
    "\n",
    "③ 컬럼별 데이터 더하기\n",
    "\n",
    "④ 컬럼 데이터 1대1 배치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1376d39",
   "metadata": {},
   "source": [
    "정답: ① 컬럼이름 변경\n",
    "\n",
    "해설:\n",
    "\n",
    "파생변수(Derived Variable)란 기존에 존재하던 변수를 조합하거나 특정 조건에 따라 가공하여 새롭게 만들어낸 변수를 의미합니다.\n",
    "\n",
    "① 컬럼이름 변경 (정답): 단순히 컬럼의 이름(변수명)을 바꾸는 것은 데이터의 구조나 값에 변화를 주는 것이 아니므로 데이터 관리나 전처리의 영역에는 해당할 수 있으나, 새로운 정보를 창출하는 파생변수 생성이라고 보지는 않습니다.\n",
    "\n",
    "② 컬럼별 데이터 나누기 (옳음): 기존 변수를 나누어 새로운 지표를 만드는 전형적인 방법입니다. 예를 들어, '총 매출액'을 '구매 횟수'로 나누어 **'객단가(1인당 평균 구매액)'**라는 새로운 파생변수를 생성할 수 있습니다.\n",
    "\n",
    "③ 컬럼별 데이터 더하기 (옳음): 여러 변수를 합산하여 통합 지표를 만드는 방법입니다. 예를 들어, '국어', '영어', '수학' 점수를 모두 더해 '총점' 변수를 만드는 것이 이에 해당합니다.\n",
    "\n",
    "④ 컬럼 데이터 1대1 배치 (옳음): 특정 기준이나 조건문(If-Then)을 사용하여 기존 데이터를 1대1로 매핑하거나 변환하는 방식입니다. 예를 들어, '점수' 데이터가 90점 이상이면 'A', 80점 이상이면 'B'와 같이 '등급' 변수로 변환하는 것이 대표적입니다.\n",
    "\n",
    "Tip: 파생변수는 분석가의 주관이나 비즈니스 지식이 가장 많이 반영되는 단계이며, 모델의 성능을 획기적으로 높일 수 있는 핵심 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee448961",
   "metadata": {},
   "source": [
    "# 31. 표준분표인 X1 ~ X3의 공분산 행렬을 보고 옳지 않은 것을 고르시오.\n",
    "           X1       X2       X3\n",
    "\n",
    "X1 24.586473 14.458009  4.015487\n",
    "\n",
    "X2 14.458009 33.797439 -2.001997\n",
    "\n",
    "X3  4.015487 -2.001997 25.309955"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b0a9b",
   "metadata": {},
   "source": [
    "① X1, X3 상관계수는 1이다.\n",
    "\n",
    "② X1의 분산은 24.58이다.\n",
    "\n",
    "③ X1, X2는 양의 상관관계이다.\n",
    "\n",
    "④ X2, X3의 상관은 -2.001997이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af79972e",
   "metadata": {},
   "source": [
    "정답: ① X1, X3 상관계수는 1이다.\n",
    "\n",
    "해설:공분산 행렬(Covariance Matrix)은 여러 변수 간의 분산과 공분산을 한눈에 보여주는 행렬입니다. \n",
    "\n",
    "행렬의 대각 성분은 각 변수의 분산을, 비대각 성분은 두 변수 간의 공분산을 의미합니다.\n",
    "\n",
    "① X1, X3 상관계수는 1이다 \n",
    "\n",
    "(정답): 행렬에서 $X_1$과 $X_3$의 공분산은 4.015487입니다. \n",
    "\n",
    "상관계수가 1이 되려면 두 변수가 완전히 동일하거나 선형적으로 완벽하게 일치해야 하는데, 공분산 값이 분산($X_1: 24.58$, $X_3: 25.30$)에 비해 작으므로 상관계수는 1보다 훨씬 작은 양의 값을 가질 것입니다. \n",
    "\n",
    "상관계수 공식은 다음과 같습니다.$$\\rho_{X,Y} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "② X1의 분산은 24.58이다 (옳음): 공분산 행렬의 대각선 첫 번째 값($X_1, X_1$)인 24.586473이 $X_1$의 분산입니다. 소수점 둘째 자리까지 표현하면 24.58이 맞습니다.\n",
    "\n",
    "③ X1, X2는 양의 상관관계이다 (옳음): $X_1$과 $X_2$의 공분산은 14.458009로 **양수(+)**입니다. 공분산이 양수이면 상관계수도 양수가 되므로, 두 변수는 한쪽이 증가할 때 다른 쪽도 증가하는 양의 상관관계를 가집니다.\n",
    "\n",
    "④ X2, X3의 상관은 -2.001997이다 (옳음): 여기서 '상관'은 두 변수 간의 관계 정도를 나타내는 공분산 값을 의미합니다. 행렬에서 $X_2$와 $X_3$가 만나는 지점의 값은 -2.001997이며, 이는 두 변수가 음의 상관관계에 있음을 보여줍니다.Tip: 공분산 행렬은 대칭 행렬(Symmetric Matrix)입니다. 즉, $X_1$과 $X_2$의 공분산은 $X_2$와 $X_1$의 공분산과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e1d4f",
   "metadata": {},
   "source": [
    "# 32. 다음의 설명에 대한 해석으로 가장 옳은 상관계수는?\n",
    "두 변수 간의 선형관계가 있지만 비교적 약한 양의 관계를 나타냄. 한 변수가 증가하면 다른 변수도 증가함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa41f9d",
   "metadata": {},
   "source": [
    "① 상관계수 1\n",
    "\n",
    "② 상관계수 0\n",
    "\n",
    "③ 상관계수 0.25\n",
    "\n",
    "④ 상관계수 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a6c78b",
   "metadata": {},
   "source": [
    "정답: ③ 상관계수 0.25\n",
    "\n",
    "해설:상관계수($r$)는 두 변수 사이의 선형적 관계의 방향과 강도를 $-1$에서 $1$ 사이의 숫자로 나타내는 지표입니다.\n",
    "\n",
    "③ 상관계수 0.25 (정답): * 방향: 양수($+$)이므로 한 변수가 증가할 때 다른 변수도 증가하는 양의 관계입니다.\n",
    "\n",
    "강도: 상관계수의 절대값이 $1$에 가까울수록 강한 관계, $0$에 가까울수록 약한 관계를 의미합니다. \n",
    "\n",
    "일반적으로 $0.25$ 정도의 수치는 두 변수 간에 어느 정도의 선형성은 존재하지만, \n",
    "\n",
    "데이터가 흩어져 있어 비교적 약한 관계임을 나타냅니다.\n",
    "\n",
    "① 상관계수 1 (오답): 두 변수가 직선상에 완벽하게 위치하는 완벽한 양의 선형 관계를 의미합니다. '약한 관계'라는 설명과 배치됩니다.\n",
    "\n",
    "② 상관계수 0 (오답): 두 변수 사이에 선형적인 관계가 전혀 없는 무상관 상태를 의미합니다.\n",
    "\n",
    "④ 상관계수 -1 (오답): 한 변수가 증가할 때 다른 변수는 일정한 비율로 감소하는 완벽한 음의 선형 관계를 의미합니다.\n",
    "\n",
    "Tip: 상관계수의 일반적인 해석 기준$0.7 \\sim 1.0$: 강한 양의 상관관계$0.3 \\sim 0.7$: 뚜렷한 양의 상관관계$0.1 \\sim 0.3$: 약한 양의 상관관계$-0.1 \\sim 0.1$: 상관관계가 거의 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5289ad7",
   "metadata": {},
   "source": [
    "# 34. 데이터가 한쪽으로 쏠려 있는지 알 수 있는 통계량은?\n",
    "① 상관계수\n",
    "\n",
    "② 첨도\n",
    "\n",
    "③ 분산\n",
    "\n",
    "④ 왜도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2918db9",
   "metadata": {},
   "source": [
    "정답: ④ 왜도\n",
    "\n",
    "해설:\n",
    "\n",
    "데이터의 분포가 대칭을 이루지 않고 어느 한쪽으로 치우친 정도를 나타내는 통계량입니다.\n",
    "\n",
    "④ 왜도 (Skewness) (정답): 데이터가 왼쪽이나 오른쪽으로 얼마나 치우쳐 있는지를 수치화한 것입니다.\n",
    "\n",
    "왜도 > 0 (Positive Skewness): 꼬리가 오른쪽으로 길게 늘어지며, 데이터가 왼쪽으로 쏠려 있습니다.\n",
    "\n",
    "왜도 < 0 (Negative Skewness): 꼬리가 왼쪽으로 길게 늘어지며, 데이터가 오른쪽으로 쏠려 있습니다.\n",
    "\n",
    "왜도 = 0: 좌우가 대칭인 분포(예: 정규분포)를 이룹니다.\n",
    "\n",
    "① 상관계수 (Correlation Coefficient): 두 변수 사이의 선형적 관계가 얼마나 강한지를 나타내는 지표입니다.\n",
    "\n",
    "② 첨도 (Kurtosis): 데이터 분포의 뾰족한 정도나 꼬리의 두꺼운 정도를 나타내는 지표입니다.\n",
    "\n",
    "③ 분산 (Variance): 데이터가 평균으로부터 얼마나 멀리 퍼져 있는지를 나타내는 산포의 척도입니다.\n",
    "\n",
    "Tip: '쏠림'은 왜도, '뾰족함'은 첨도라고 암기하시면 헷갈리지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9ad4a",
   "metadata": {},
   "source": [
    "# 35. 다음 중 최빈값에 대한 설명으로 바르지 않은 것은?\n",
    "① 주어진 데이터 집합에서 가장 자주 나타나는 값, 즉 최고 빈도로 나타나는 값을 나타낸다.\n",
    "\n",
    "② 데이터의 중심경향성을 파악하는데 사용한다.\n",
    "\n",
    "③ 연속형 자료의 대표값으로 가장 적절하다.\n",
    "\n",
    "④ 이상치(Outlier)가 데이터 집합에 포함되어 있는 경우, 최빈값은 그 영향을 받을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea9cdf8",
   "metadata": {},
   "source": [
    "정답: ③ 연속형 자료의 대표값으로 가장 적절하다.\n",
    "\n",
    "해설:\n",
    "\n",
    "최빈값(Mode)은 데이터에서 가장 빈번하게 발생하는 값을 의미하며, 데이터의 성격에 따라 대표값으로서의 가치가 달라집니다.\n",
    "\n",
    "③ 연속형 자료의 대표값으로 가장 적절하다 (정답): 연속형 자료(예: 몸무게, 키)는 값이 소수점 단위로 매우 세밀하게 나뉘기 때문에 동일한 값이 중복해서 나타날 확률이 매우 낮습니다. 따라서 연속형 자료의 대표값으로는 **평균(Mean)**이나 **중앙값(Median)**이 더 적절합니다. 최빈값은 주로 혈액형, 성별, 선호도와 같은 범주형(이산형) 자료에서 가장 유용한 대표값입니다.\n",
    "\n",
    "① 가장 자주 나타나는 값 (옳음): 최빈값의 정의 그 자체입니다. 빈도수(Frequency)가 가장 높은 값을 선택합니다.\n",
    "\n",
    "② 중심경향성을 파악하는데 사용 (옳음): 평균, 중앙값과 함께 데이터가 어디에 집중되어 있는지(중심)를 나타내는 주요 척도 중 하나입니다.\n",
    "\n",
    "④ 이상치의 영향을 받을 수 있다 (옳음): 일반적으로 최빈값은 평균보다 이상치에 강건(Robust)하다고 알려져 있습니다. 하지만 데이터의 양이 적은 상태에서 특이한 값이 반복적으로 나타나 이상치가 최빈값이 되어버리는 경우, 데이터 전체의 특성을 왜곡할 수 있으므로 영향을 전혀 받지 않는다고 할 수는 없습니다.\n",
    "\n",
    "Tip: 범주형 데이터(명목척도)에서는 오직 최빈값만이 대표값으로 사용될 수 있다는 점을 기억해 두세요! (예: 가장 인기 있는 색상을 구할 때 평균 색상을 구할 수는 없습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621e27e",
   "metadata": {},
   "source": [
    "# 36. 이산형확률변수의 확률분포로 알맞은 것은?\n",
    "① F 분포\n",
    "\n",
    "② z 분포\n",
    "\n",
    "③ 이항분포\n",
    "\n",
    "④ 지수분포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b88ec",
   "metadata": {},
   "source": [
    "정답: ③ 이항분포\n",
    "\n",
    "해설:확률분포는 확률변수가 취할 수 있는 값의 형태에 따라 이산확률분포와 연속확률분포로 나뉩니다.\n",
    "\n",
    "③ 이항분포 (Binomial Distribution) (정답): 결과가 '성공' 또는 '실패' 두 가지뿐인 시행(베르누이 시행)을 $n$번 반복했을 때의 성공 횟수를 나타내는 분포입니다. 성공 횟수는 $0, 1, 2, \\dots$와 같이 **셀 수 있는 정수(이산형)**로 나타나므로 대표적인 이산확률분포입니다.\n",
    "\n",
    "① F 분포 (F-distribution) (오답): 두 집단의 분산을 비교할 때 사용하며, 값이 소수점으로 이어지는 연속확률분포입니다.\n",
    "\n",
    "② z 분포 (Standard Normal Distribution) (오답): 표준정규분포를 의미하며, 평균이 0이고 표준편차가 1인 대칭 모양의 연속확률분포입니다.\n",
    "\n",
    "④ 지수분포 (Exponential Distribution) (오답): 어떤 사건이 발생할 때까지 걸리는 시간이나 거리 등을 나타내는 분포로, 시간은 연속적인 값이므로 연속확률분포에 해당합니다.\n",
    "\n",
    "Tip: 시험 대비 암기 리스트이산확률분포: 이항분포, 포아송분포, 베르누이분포, 기하분포연속확률분포: 정규분포($z$), $t$분포, 카이제곱($\\chi^2$)분포, $F$분포, 지수분포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4bd2fd",
   "metadata": {},
   "source": [
    "# 37. 표준편차가 10, 평균이 60인 모집단(모집단은 정규분포)이 있다. 70의 Z-score를 구하라.\n",
    "① 0\n",
    "\n",
    "② 1\n",
    "\n",
    "③ 1.25\n",
    "\n",
    "④ -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338dcd76",
   "metadata": {},
   "source": [
    "정답: ② 1\n",
    "\n",
    "해설:**Z-score(표준점수)**는 특정 데이터 값이 평균으로부터 표준편차의 몇 배만큼 떨어져 있는지를 나타내는 수치입니다.\n",
    "\n",
    "표준화 공식은 다음과 같습니다\n",
    "\n",
    ":$$Z = \\frac{X - \\mu}{\\sigma}$$$X$ \n",
    "\n",
    "(측정값): 70$\\mu$ (평균): 60$\\sigma$ \n",
    "\n",
    "(표준편차): 10위 공식에 대입하면:$$Z = \\frac{70 - 60}{10} = \\frac{10}{10} = 1$$\n",
    "\n",
    "따라서 $Z$-score는 1이 됩니다. 이는 측정값 70이 평균보다 표준편차의 1배만큼 높은 위치에 있다는 것을 의미합니다.\n",
    "\n",
    "Tip: $Z$-score가 0이면 평균과 같다는 뜻이고, 음수이면 평균보다 작다는 뜻입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187d751",
   "metadata": {},
   "source": [
    "# 38. 도시 내 여성 비중 40%, 180cm 이상의 남성은 15%, 여성은 2.5% 일 때, 180cm인 사람이 여성일 확률은?\n",
    "① 0.08\n",
    "\n",
    "② 0.1\n",
    "\n",
    "③ 0.15\n",
    "\n",
    "④ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869eb8b6",
   "metadata": {},
   "source": [
    "정답: ② 0.1\n",
    "\n",
    "해설:이 문제는 특정 조건이 주어졌을 때의 확률을 구하는 조건부 확률(Bayes' Theorem) 문제입니다.\n",
    "\n",
    "전체 인구 중 '180cm 이상인 사람'이라는 조건 하에 그 사람이 '여성'일 확률을 구해야 합니다.\n",
    "\n",
    "1단계: 각 집단의 비율 정의여성일 확률 $P(W) = 0.4$\n",
    "남성일 확률 $P(M) = 0.6$ (100% - 40%)\n",
    "\n",
    "2단계: 각 집단 내에서 180cm 이상($H$)일 확률여성이면서 180cm 이상일 확률 \n",
    "$P(H|W) = 0.025$ (2.5%)\n",
    "\n",
    "남성이면서 180cm 이상일 확률 $P(H|M) = 0.15$ (15%)\n",
    "\n",
    "3단계: 전체 인구 중 180cm 이상($H$)인 사람의 확률 계산여성 중 180cm 이상: $0.4 \\times 0.025 = 0.01$남성 중 180cm 이상: $0.6 \\times 0.15 = 0.09$전체 180cm 이상인 확률 $P(H) = 0.01 + 0.09 = 0.1$\n",
    "\n",
    "4단계: 180cm인 사람이 여성일 조건부 확률 계산$$P(W|H) = \\frac{P(W \\cap H)}{P(H)} = \\frac{0.01}{0.1} = 0.1$$따라서 180cm 이상인 사람을 무작위로 뽑았을 때 그 사람이 여성일 확률은 **0.1(10%)**입니다.\n",
    "\n",
    "Tip: 조건부 확률 문제는 위와 같이 '전체 조건의 합'을 분모로 두고, '찾고자 하는 특정 경우'를 분자로 두면 쉽게 풀 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c5b56",
   "metadata": {},
   "source": [
    "# 39. 다음 중 중심극한정리에 대한 설명으로 옳지 않은 것은?\n",
    "① 크기가 크면 표준정규분포를 따른다.\n",
    "\n",
    "② 표본의 수가 30보다 클 경우 적용된다.\n",
    "\n",
    "③ 연속형 변수에만 사용 가능하다.\n",
    "\n",
    "④ 반드시 모분포가 정규분포를 따를 필요는 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1649099",
   "metadata": {},
   "source": [
    "정답: ③ 연속형 변수에만 사용 가능하다.\n",
    "\n",
    "해설:중심극한정리(Central Limit Theorem, CLT)는 통계학에서 가장 중요한 정리 중 하나로, 표본의 크기가 커짐에 따라 표본평균의 분포가 어떤 모양을 갖게 되는지를 설명합니다.\n",
    "③ 연속형 변수에만 사용 가능하다 (정답)\n",
    ":중심극한정리는 변수의 종류(이산형, 연속형)에 상관없이 적용됩니다. \n",
    "\n",
    "모집단이 이항분포(이산형)와 같은 분포를 따르더라도 표본의 크기($n$)가 충분히 크다면, 그 표본평균들의 분포는 정규분포에 근사하게 됩니다.① 크기가 크면 표준정규분포를 따른다 (옳음): \n",
    "\n",
    "엄밀하게는 표본평균의 분포가 정규분포를 따르는 것이며, 이를 표준화하면 표준정규분포를 따르게 됩니다.\n",
    "\n",
    "② 표본의 수가 30보다 클 경우 적용된다 (옳음): 일반적으로 모집단의 분포와 상관없이 표본의 크기($n$)가 30 이상이면 중심극한정리가 적용되어 정규분포를 따른다고 가정하는 것이 통계학의 통상적인 규칙입니다.\n",
    "\n",
    "④ 반드시 모분포가 정규분포를 따를 필요는 없다 (옳음): 이것이 중심극한정리의 가장 큰 특징입니다. 모집단이 균등분포, 지수분포, 혹은 아주 불규칙한 모양일지라도 표본만 충분히 뽑는다면 평균의 분포는 종 모양(정규분포)이 됩니다.Tip: 중심극한정리 덕분에 우리는 모집단의 원래 분포를 몰라도 표본평균을 통해 모수를 추정하고 가설 검정을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a312cb0",
   "metadata": {},
   "source": [
    "# 40. 다음 중 가설 검정에 대한 설명으로 옳지 않은 것은?\n",
    "① 유의한 결과가 나오는가 함.\n",
    "\n",
    "② 검정가설 1개, 대립가설 1개 설정한다.\n",
    "\n",
    "③ 양측검정은 가설이 양쪽에 나누어져 있다.\n",
    "\n",
    "④ 귀무가설은 사실이라고 믿는 가설이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216fd99",
   "metadata": {},
   "source": [
    "정답: ④ 귀무가설은 사실이라고 믿는 가설이다.\n",
    "\n",
    "해설:가설 검정은 모집단에 대한 어떤 주장이 통계적으로 타당한지 표본 데이터를 통해 판정하는 과정입니다.\n",
    "\n",
    "④ 귀무가설은 사실이라고 믿는 가설이다 (정답): \n",
    "\n",
    "**귀무가설($H_0$, Null Hypothesis)**은 '차이가 없다', '효과가 없다'와 같이 현재까지 사실로 받아들여지는 기본 상태를 의미합니다. \n",
    "\n",
    "우리가 새롭게 증명하고 싶거나 사실이라고 믿고 싶어 하는 가설은 \n",
    "\n",
    "**대립가설($H_1$, Alternative Hypothesis)**입니다. \n",
    "가설 검정은 귀무가설이 틀렸다는 증거를 찾아내어 대립가설을 채택하기 위한 과정입니다.\n",
    "\n",
    "① 유의한 결과가 나오는가 함 (옳음): 가설 검정의 목적은 관찰된 결과가 단순히 우연히 발생한 것인지, 아니면 통계적으로 유의미한(Significant) 차이가 있는지를 판단하는 것입니다.\n",
    "\n",
    "② 검정가설 1개, 대립가설 1개 설정한다 (옳음): 가설 검정 시에는 기본적으로 비교 대상이 되는 귀무가설과 그에 반대되는 대립가설을 쌍으로 설정하여 분석을 진행합니다.\n",
    "\n",
    "③ 양측검정은 가설이 양쪽에 나누어져 있다 (옳음): 대립가설이 \"차이가 있다($\\neq$)\"인 경우, 기준값보다 너무 크거나 너무 작은 양쪽 극단 영역 모두를 기각역(유의수준)으로 설정하는 것을 의미합니다. 반면 \"크다($>$)\" 혹은 \"작다($<$)\"와 같이 한쪽 방향만 확인하는 것은 단측검정입니다.\n",
    "\n",
    "Tip: 가설 설정의 정석귀무가설($H_0$): 기존의 입장. \"A와 B는 차이가 없다.\" (기각하고 싶은 가설)대립가설($H_1$): 새로운 주장. \"A와 B는 차이가 있다.\" (채택하고 싶은 가설)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504ad5a",
   "metadata": {},
   "source": [
    "# 41. 분류 모델에 대한 설명으로 알맞은 것은?\n",
    "① 고등학생 내신점수로 수능점수 예측\n",
    "\n",
    "② 빵집에서 날씨, 요일, 공휴일, 계절별로 분석해 판매량을 예측\n",
    "\n",
    "③ 배우, 감독, 배급사, 투자비 정보로 이익 예측\n",
    "\n",
    "④ 카드사에서 가입정보로 신용등급 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf9578",
   "metadata": {},
   "source": [
    "정답: ④ 카드사에서 가입정보로 신용등급 예측\n",
    "\n",
    "해설:\n",
    "\n",
    "데이터 분석 모델은 크게 **분류(Classification)**와 **회귀(Regression)**로 나뉩니다. 이 둘을 구분하는 핵심은 예측하려는 결과값(타겟 변수)의 형태입니다.\n",
    "\n",
    "④ 카드사에서 가입정보로 신용등급 예측 (정답): '신용등급'은 1등급, 2등급, 3등급 혹은 고신용, 저신용처럼 미리 정해진 범주(Category) 중 하나를 맞히는 것입니다. 이처럼 출력값이 불연속적인 범주형 데이터일 때 분류 모델을 사용합니다.\n",
    "\n",
    "① 내신점수로 수능점수 예측 (오답): 수능점수는 0점에서 400점 사이의 연속적인 숫자로 나타납니다. 이처럼 연속된 **수치(Value)**를 예측하는 것은 회귀 모델입니다.\n",
    "\n",
    "② 날씨 등에 따른 판매량 예측 (오답): '판매량' 역시 100개, 150개와 같이 수치로 나타나는 데이터이므로 회귀 모델에 해당합니다.\n",
    "\n",
    "③ 정보에 따른 이익 예측 (오답): '이익(금액)'은 연속적인 수치 데이터이므로 이 역시 회귀 모델의 영역입니다.\n",
    "\n",
    "Tip: 분류 vs 회귀 구분법\n",
    "\n",
    "분류 (Classification): 예/아니오, 합격/불합격, 스팸/정상, A/B/C 등급 (이산적/범주형)\n",
    "\n",
    "회귀 (Regression): 온도, 주가, 매출액, 점수, 수명 (연속적/수치형)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f141fcc6",
   "metadata": {},
   "source": [
    "# 42. 다음 중 회귀와 분류 모델 평가표에 대한 설명으로 옳지 않은 것은?\n",
    "① 대표적인 분류 모델 평가지표로는 정확도, 정밀도, F1점수가 있다.\n",
    "\n",
    "② 평균제곱오차(Mean Squared Error, MSE)는 회귀 모델의 평가지표이다.\n",
    "\n",
    "③ 회귀와 분류 모델은 종속변수가 다르지만 동일한 평가지표를 사용한다.\n",
    "\n",
    "④ 데이터와 문제의 특성에 따라 적절한 평가지표를 선택해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763c796",
   "metadata": {},
   "source": [
    "정답: ③ \n",
    "\n",
    "회귀와 분류 모델은 종속변수가 다르지만 동일한 평가지표를 사용한다.해설:회귀 모델과 분류 모델은 예측하려는 종속변수(타겟)의 성격이 완전히 다르기 때문에 평가하는 방법(지표) 또한 명확히 구분됩니다.\n",
    "\n",
    "③ 동일한 평가지표를 사용한다 (정답): * 회귀 모델은 실제 수치와 예측 수치의 **차이(오차)**를 계산합니다. (예: MSE, RMSE, $R^2$)분류 모델은 예측한 범주가 실제 범주와 얼마나 일치하는지를 계산합니다. (예: Accuracy, Precision, Recall)\n",
    "\n",
    "따라서 두 모델에 동일한 평가지표를 적용할 수 없습니다.\n",
    "① 분류 모델 평가지표 (옳음): * 정확도(Accuracy): 전체 중 맞힌 비율정밀도(Precision): \n",
    "\n",
    "모델이 참이라고 한 것 중 실제 참인 비율F1점수: 정밀도와 재현율의 조화 평균\n",
    "\n",
    "② MSE는 회귀 모델의 평가지표 (옳음): 평균제곱오차(MSE)는 예측값과 실제값의 차이를 제곱하여 평균한 값으로, 수치 데이터의 오차를 측정하는 회귀 모델의 핵심 지표입니다.\n",
    "\n",
    "④ 적절한 평가지표 선택 (옳음): 예를 들어 암 진단처럼 실제 환자를 놓치면 안 되는 문제(분류)에서는 정확도보다 **재현율(Recall)**이 중요하며, 이상치에 민감한 회귀 분석에서는 MSE 대신 MAE를 사용하기도 합니다.\n",
    "\n",
    "Tip: '오차'라는 단어가 들어가면 회귀, '정확도/정밀도'라는 단어가 나오면 분류 지표라고 기억하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cbe1d6",
   "metadata": {},
   "source": [
    "# 43. 아래 수식이 설명하는 것은?\n",
    "$$J(\\theta) = MSE(\\theta) + \\alpha \\sum_{i=1}^n |\\theta_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d43e22",
   "metadata": {},
   "source": [
    "① 릿지(Ridge)\n",
    "\n",
    "② 라쏘(Lasso)\n",
    "\n",
    "③ 엘라스틱넷(Elastic Net)\n",
    "\n",
    "④ 로지스틱회귀(Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5ec35",
   "metadata": {},
   "source": [
    "정답: ② 라쏘(Lasso)\n",
    "\n",
    "해설:위 수식은 회귀 모델의 규제(Regularization) 기법 중 하나인 L1 규제가 적용된 비용 함수를 나타냅니다.\n",
    "\n",
    "② 라쏘(Lasso, L1 Regularization) (정답): * 비용 함수에 가중치($\\theta$)의 절댓값 합($\\sum |\\theta_i|$)을 더해주는 방식입니다.이 기법의 가장 큰 특징은 영향력이 적은 변수의 회귀 계수를 0으로 만들어 변수 선택(Feature Selection) 효과를 낸다는 점입니다.\n",
    "\n",
    "① 릿지(Ridge, L2 Regularization) (오답): * 비용 함수에 가중치의 제곱 합($\\sum \\theta_i^2$)을 더해줍니다.가중치들을 전체적으로 작게 만들지만, 0으로 만들지는 않아 변수 선택 효과는 없습니다.\n",
    "\n",
    "③ 엘라스틱넷(Elastic Net) (오답): * 라쏘(L1)와 릿지(L2) 규제를 결합한 형태입니다. 수식에는 절댓값 합과 제곱 합이 모두 포함되어야 합니다.\n",
    "\n",
    "④ 로지스틱 회귀 (오답): * 이는 범주형 데이터를 예측하는 분류 알고리즘의 한 종류이며, 제시된 수식처럼 비용 함수에 특정 규제 항이 추가된 형태 자체를 지칭하는 이름은 아닙니다.\n",
    "\n",
    "Tip: 시험 대비 핵심 수식 구분$|\\theta|$ (절댓값) $\\rightarrow$ L1 $\\rightarrow$ Lasso (라쏘는 0으로 쏜다/없앤다)$\\theta^2$ (제곱) $\\rightarrow$ L2 $\\rightarrow$ Ridge (릿지는 부드럽게 줄인다)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97478e2d",
   "metadata": {},
   "source": [
    "# 45. 의사결정나무의 정지규칙으로 옳지 않은 것은?\n",
    "① 깊이(Depth)가 최대면 멈춘다.\n",
    "\n",
    "② 마지막 가지 끝에 남은 개수가 일정 개수 이하이면 멈춘다.\n",
    "\n",
    "③ 가지에 남은 개수가 같으면 멈춘다.\n",
    "\n",
    "④ 더 이상 나눌 수 없으면 멈춘다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307711cf",
   "metadata": {},
   "source": [
    "정답: ③ 가지에 남은 개수가 같으면 멈춘다.\n",
    "\n",
    "해설:\n",
    "\n",
    "의사결정나무(Decision Tree)에서 **정지규칙(Stopping Rule)**은 모델이 너무 복잡해져서 학습 데이터에만 과하게 맞춰지는 **과적합(Overfitting)**을 방지하기 위해 나무의 성장을 멈추는 기준을 말합니다.\n",
    "\n",
    "③ 가지에 남은 개수가 같으면 멈춘다 (정답): 단순히 노드(가지) 간에 남아 있는 데이터의 개수가 같다고 해서 성장을 멈추지는 않습니다. 데이터 개수가 같더라도 그 안의 범주가 섞여 있어(불순도가 높아) 더 나눌 필요가 있다면 분할을 계속 진행합니다.\n",
    "\n",
    "① 깊이(Depth)가 최대면 멈춘다 (옳음): 나무의 층수(깊이)가 사전에 설정한 최대치(max_depth)에 도달하면 더 이상 가지를 치지 않고 성장을 멈춥니다.\n",
    "\n",
    "② 끝에 남은 개수가 일정 개수 이하이면 멈춘다 (옳음): 끝마디(Terminal Node)에 속한 최소 관측치 수(min_samples_leaf)를 설정하여, 그보다 적은 데이터가 남게 될 경우 분할을 멈춥니다.\n",
    "\n",
    "④ 더 이상 나눌 수 없으면 멈춘다 (옳음): 모든 데이터가 하나의 범주로만 구성되어 불순도가 0이 되거나, 분할을 해도 불순도가 감소하지 않는 경우 자연스럽게 성장이 멈춥니다.\n",
    "\n",
    "Tip: 정지규칙은 나무가 너무 깊어지는 것을 막아 모델의 일반화 성능을 높이는 아주 중요한 단계입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eededb30",
   "metadata": {},
   "source": [
    "# 46. 인공신경망이 보기의 설명과 같을 때 출력값은?\n",
    "\n",
    "마지막 은닉층의 첫 번째 노드: 0.1\n",
    "\n",
    "마지막 은닉층의 두 번째 노드: -0.1\n",
    "\n",
    "첫 번째 노드의 가중치: 0.2\n",
    "\n",
    "두 번째 노드의 가중치: 0.1\n",
    "\n",
    "출력층의 bias가 -0.1\n",
    "\n",
    "출력함수: $f(x) = x$ (if $x \\ge 0$) otherwise $f(x) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1959085",
   "metadata": {},
   "source": [
    "① 1\n",
    "\n",
    "② -1\n",
    "\n",
    "③ 0.09\n",
    "\n",
    "④ 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9adbe",
   "metadata": {},
   "source": [
    "정답: ④ 0\n",
    "해설:인공신경망의 출력값은 **(입력값 × 가중치의 총합) + 편향(bias)**을 구한 뒤, 그 결과를 활성화 함수에 통과시켜 결정합니다.\n",
    "\n",
    "1단계: 가중합(Weighted Sum) 계산(은닉층 1번 노드 × 가중치 1) + (은닉층 2번 노드 × 가중치 2) + 편향(bias)$(0.1 \\times 0.2) + (-0.1 \\times 0.1) + (-0.1)$$0.02 + (-0.01) - 0.1 = -0.09$\n",
    "\n",
    "2단계: 활성화 함수(출력함수) 적용문제에서 주어진 함수는 $x$가 0보다 크거나 같으면 $x$를 그대로 출력하고, 0보다 작으면 0을 출력하는 ReLU(Rectified Linear Unit) 형태의 함수입니다.계산된 값 $x = -0.09$는 0보다 작으므로, 함수 결과값은 0이 됩니다.\n",
    "\n",
    "Tip: 인공신경망 문제에서 마지막에 '출력함수'나 '활성화 함수'의 조건을 확인하지 않으면 계산 실수(예: ③번 선택)를 하기 쉬우니 주의해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afa7d77",
   "metadata": {},
   "source": [
    "# 47. 다음 중 보기의 설명에 들어갈 알맞은 말은?\n",
    "\n",
    "역전파 알고리즘은 출력부터 반대방향으로 순차적으로 ( ㄱ )하면서 ( ㄴ )을 증가시키는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec884ce",
   "metadata": {},
   "source": [
    "① 편미분, 학습률\n",
    "\n",
    "② 정적분, 가중치\n",
    "\n",
    "③ 내적, 가중치\n",
    "\n",
    "④ 편미분, 내적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdca19",
   "metadata": {},
   "source": [
    "정답: ① 편미분, 학습률\n",
    "\n",
    "해설:\n",
    "\n",
    "인공신경망의 핵심 학습 알고리즘인 **역전파(Backpropagation)**의 원리를 묻는 문제입니다.\n",
    "\n",
    "( ㄱ ) 편미분: 역전파는 출력층에서 발생한 오차를 입력층 방향으로 거슬러 올라가며 전달합니다. 이때 각 층의 가중치가 오차에 얼마나 기여했는지 계산하기 위해 **편미분(Partial Differentiation)**을 사용하여 기울기(Gradient)를 구합니다.\n",
    "\n",
    "( ㄴ ) 학습률: 기울기를 구한 뒤에는 가중치를 얼마나 업데이트할지 결정해야 합니다. 이때 '학습률(Learning Rate)'을 곱하여 가중치를 조정함으로써 모델의 **학습률(학습의 진행 정도)**을 조절하고 최적의 가중치를 찾아갑니다. (다만, 문제의 문맥상 '학습' 그 자체를 강화하거나 '가중치 업데이트의 효율'을 의미하는 것으로 해석됩니다.)\n",
    "\n",
    "Tip: 역전파 하면 '오차 역전파', '편미분(체인 룰)', **'가중치 업데이트'**라는 세 키워드를 반드시 기억하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ce470",
   "metadata": {},
   "source": [
    "48. 다음과 같이 연관분석을 계산했을 때 (사과->우유)의 향상도값은?\n",
    "\n",
    "데이터 1: (사과, 달걀, 우유)\n",
    "\n",
    "데이터 2: (사과, 달걀, 우유)\n",
    "\n",
    "데이터 3: (사과, 달걀)\n",
    "\n",
    "데이터 4: (우유, 음료수, 커피)\n",
    "\n",
    "데이터 5: (우유, 음료수, 커피, 사과)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0f553",
   "metadata": {},
   "source": [
    "① 0.7542\n",
    "\n",
    "② 1.125\n",
    "\n",
    "③ 0.9375\n",
    "\n",
    "④ 1.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a300225",
   "metadata": {},
   "source": [
    "정답: ③ 0.9375\n",
    "\n",
    "해설:연관분석의 주요 지표인 향상도(Lift)를 구하기 위해서는 먼저 각 품목의 지지도와 신뢰도를 계산해야 합니다. \n",
    "\n",
    "(데이터 5가 두 번 적혀 있으나, 일반적으로 중복 기재로 간주하여 총 5개의 트랜잭션으로 계산합니다.)\n",
    "\n",
    "1단계: 각 항목의 확률(지지도) 계산전체 데이터 수($N$) = 5사과(A)가 포함된 건수: 1, 2, 3, 5번 (총 4건) \n",
    "\n",
    "$\\rightarrow P(A) = 4/5 = 0.8$우유(B)가 포함된 건수: 1, 2, 4, 5번 (총 4건) \n",
    "\n",
    "$\\rightarrow P(B) = 4/5 = 0.8$사과와 우유가 동시에 포함된 건수(A $\\cap$ B): 1, 2, 5번 (총 3건) \n",
    "\n",
    "$\\rightarrow P(A \\cap B) = 3/5 = 0.6$2단계: 신뢰도(Confidence) 계산신뢰도는 사과를 구매했을 때 우유를 같이 구매할 확률입니다.\n",
    "\n",
    "$Confidence(A \\rightarrow B) = \\frac{P(A \\cap B)}{P(A)} = \\frac{0.6}{0.8} = 0.75$3단계: 향상도(Lift) 계산향상도는 품목 간의 상관관계를 나타내며, 신뢰도를 우유의 지지도(확률)로 나눈 값입니다.\n",
    "\n",
    "$Lift(A \\rightarrow B) = \\frac{Confidence(A \\rightarrow B)}{P(B)} = \\frac{0.75}{0.8} = 0.9375$또는 직접 수식에 대입하면:$$Lift = \\frac{P(A \\cap B)}{P(A) \\times P(B)} = \\frac{0.6}{0.8 \\times 0.8} = \\frac{0.6}{0.64} = 0.9375$$결과 해석:향상도가 1보다 작으므로($0.9375 < 1$), 사과와 우유는 서로 구매를 촉진하는 관계라기보다 독립적이거나 약간의 음의 상관관계를 가진다고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05f5ceb",
   "metadata": {},
   "source": [
    "# 49. 다음 중 사전에 군집을 설정하지 않아도 되는 것은?\n",
    "① 가우시안 혼합모델\n",
    "\n",
    "② 스펙트럴 군집분석\n",
    "\n",
    "③ 계층적 군집분석\n",
    "\n",
    "④ k-평균 군집분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea19015",
   "metadata": {},
   "source": [
    "정답: ③ 계층적 군집분석\n",
    "\n",
    "해설:군집 분석(Clustering)은 크게 군집의 개수($k$)를 미리 정해야 하는 방식과 데이터 간의 유사도를 바탕으로 점진적으로 묶어가는 방식으로 나뉩니다.\n",
    "③ 계층적 군집분석 (정답): 데이터 간의 거리가 가장 가까운 것부터 순차적으로 병합하거나 분할해 나가는 방식입니다. \n",
    "\n",
    "분석 결과가 **덴드로그램(Dendrogram)**이라는 나무 모양의 계층 구조로 나타나며, \n",
    "\n",
    "분석이 끝난 후 이 그림을 보고 적절한 위치에서 잘라 군집의 개수를 결정합니다. \n",
    "\n",
    "즉, 분석 시작 전에 군집 수를 지정할 필요가 없습니다.\n",
    "\n",
    "① 가우시안 혼합모델 (GMM) (오답): 데이터가 $k$개의 가우시안 분포로부터 생성되었다고 가정하고 학습하는 모델로, 사전에 분포의 개수($k$)를 설정해야 합니다.\n",
    "\n",
    "② 스펙트럴 군집분석 (오답): 데이터 간의 관계를 그래프의 고유값으로 변환하여 분석하는 기법으로, 역시 사전에 군집의 개수를 지정해야 합니다.\n",
    "\n",
    "④ k-평균 군집분석 (K-Means) (오답): 가장 대표적인 비계층적 군집 분석 기법으로, 이름에서 알 수 있듯이 $k$개의 군집 중심을 사전에 설정해야만 분석이 시작됩니다.\n",
    "\n",
    "Tip: 덴드로그램이 문제 키워드로 나오거나 '군집 수를 미리 정하지 않는다'는 말이 나오면 계층적 군집 분석을 떠올리세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304bc551",
   "metadata": {},
   "source": [
    "# 50. 종속변수가 없을 때 사용하는 모델 유형으로 적절한 것은?\n",
    "① 나이브 베이즈 분류기\n",
    "\n",
    "② 의사결정나무\n",
    "\n",
    "③ k-최근접 이웃\n",
    "\n",
    "④ k-평균군집"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f526cdbf",
   "metadata": {},
   "source": [
    "정답: ④ k-평균군집\n",
    "\n",
    "해설:머신러닝 모델은 정답(종속변수, 타겟)의 존재 여부에 따라 **지도 학습(Supervised Learning)**과 **비지도 학습(Unsupervised Learning)**으로 구분됩니다.\n",
    "\n",
    "④ k-평균군집 (K-Means Clustering) (정답): 대표적인 비지도 학습 알고리즘입니다. 비지도 학습은 종속변수($y$) 없이 독립변수($x$)들 사이의 유사성만을 바탕으로 데이터를 그룹화합니다. \n",
    "\n",
    "k-평균군집은 데이터를 $k$개의 군집으로 묶어주는 모델로, 사전에 정답(레이블)이 주어지지 않은 상태에서 사용합니다.\n",
    "\n",
    "[Image showing the difference between Supervised Learning (with labels) and Unsupervised Learning (without labels)]\n",
    "\n",
    "① 나이브 베이즈 분류기 (Naive Bayes) (오답): 특정 범주에 속할 확률을 계산하여 분류하는 지도 학습 모델입니다. '분류'를 위해서는 각 데이터가 어떤 그룹에 속하는지 나타내는 종속변수가 반드시 필요합니다.\n",
    "\n",
    "② 의사결정나무 (Decision Tree) (오답): 데이터를 분류하거나 수치를 예측하는 지도 학습 모델입니다. 나무 구조를 만들기 위해 학습 과정에서 정답(종속변수) 데이터가 필요합니다.\n",
    "\n",
    "③ k-최근접 이웃 (k-NN) (오답): 새로운 데이터와 가장 가까운 $k$개의 이웃 데이터를 확인하여 분류하거나 회귀하는 지도 학습 모델입니다. 이웃들이 어떤 정답(종속변수)을 가지고 있는지 알고 있어야 합니다.Tip: 정답 유무에 따른 학습 분류지도 학습(종속변수 有): 분류(Classification), 회귀(Regression)비지도 학습(종속변수 無): 군집화(Clustering), 차원 축소(Dimension Reduction), 연관 규칙(Association Rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a560e7",
   "metadata": {},
   "source": [
    "# 51. 정준상관분석에 대한 설명으로 옳은 것은?\n",
    "① 집단 1개일 때 여러 변수 상관관계\n",
    "\n",
    "② 집단 2개일 때 상관관계\n",
    "\n",
    "③ 다변량 독립변수와 다변량 종속변수의 상관관계\n",
    "\n",
    "④ 암묵적인 상관을 찾고 싶을 때 탐색적으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b7311",
   "metadata": {},
   "source": [
    "정답: ③ 다변량 독립변수와 다변량 종속변수의 상관관계\n",
    "\n",
    "해설:정준상관분석은 두 개의 변수 집단(변수 세트) 사이의 선형 연관성을 분석하는 통계 기법입니다.\n",
    "\n",
    "③ 다변량 독립변수와 다변량 종속변수의 상관관계 (정답): 일반적인 상관분석이 두 변수($1:1$) 사이의 관계를 본다면, 정준상관분석은 **여러 개의 독립변수 집단($X_1, X_2, \\dots, X_p$)**과 여러 개의 종속변수 집단($Y_1, Y_2, \\dots, Y_q$) 사이의 상관관계를 한꺼번에 분석합니다. 각 집단의 변수들을 선형 결합하여 만든 '정준변수'들 사이의 상관계수를 최대화하는 방식으로 진행됩니다.\n",
    "\n",
    "① 집단 1개일 때 여러 변수 상관관계 (오답): 이는 일반적인 상관분석이나 요인분석(Factor Analysis)에 가깝습니다.\n",
    "\n",
    "② 집단 2개일 때 상관관계 (오답): 단순히 두 '집단'의 평균 차이를 보는 것은 t-test나 ANOVA이며, 두 '변수'의 관계를 보는 것은 단순 상관분석입니다. 정준상관분석은 '변수들의 묶음' 대 '변수들의 묶음'을 다룹니다.\n",
    "\n",
    "④ 암묵적인 상관을 찾고 싶을 때 (오답): 이는 주로 탐색적 요인분석(EFA)이나 주성분 분석(PCA)에 대한 설명입니다.\n",
    "\n",
    "Tip: '여러 개의 변수'와 '또 다른 여러 개의 변수' 사이의 관계를 한 번에 분석한다는 키워드가 나오면 정준상관분석을 선택하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ba40a",
   "metadata": {},
   "source": [
    "# 52. 다음 방식 중 언어 모델이 아닌 것은?\n",
    "① GPT(Generative Pre-trained Transformer)\n",
    "\n",
    "② BERT(Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "③ YOLO(You Only Look Once)\n",
    "\n",
    "④ BART(Bidirectional and Auto-Regressive Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9d6b8",
   "metadata": {},
   "source": [
    "정답: ③ YOLO(You Only Look Once)\n",
    "\n",
    "해설:\n",
    "\n",
    "제시된 보기들은 모두 딥러닝 기반의 알고리즘들이지만, 다루는 데이터의 종류(도메인)에서 차이가 있습니다.\n",
    "\n",
    "③ YOLO(You Only Look Once) (정답): YOLO는 텍스트를 처리하는 언어 모델이 아니라, 이미지나 동영상에서 객체를 탐지하는 컴퓨터 비전(Computer Vision) 모델입니다. 이미지 내의 물체를 한 번에(One-stage) 탐지하여 실시간 객체 인식 속도가 매우 빠른 것이 특징입니다.\n",
    "\n",
    "① GPT(Generative Pre-trained Transformer) (오답): 오픈AI에서 개발한 대표적인 생성형 언어 모델입니다. 문장의 앞부분이 주어지면 다음에 올 단어를 예측하는 방식으로 텍스트를 생성합니다.\n",
    "\n",
    "② BERT(Bidirectional Encoder Representations from Transformers) (오답): 구글에서 개발한 모델로, 문장의 앞뒤 문맥을 동시에 파악하는 양방향 언어 모델입니다. 텍스트 분류, 질문 답변 등 자연어 이해(NLU) 작업에 탁월합니다.\n",
    "\n",
    "④ BART(Bidirectional and Auto-Regressive Transformers) (오답): 페이스북(Meta)에서 개발한 모델로, BERT의 양방향 문맥 파악 능력과 GPT의 문장 생성 능력을 결합한 인코더-디코더 구조의 언어 모델입니다. 텍스트 요약 등에 자주 쓰입니다.\n",
    "\n",
    "Tip: GPT, BERT, BART, T5 등 'Transformer' 구조를 기반으로 하는 모델들은 대부분 **자연어 처리(NLP)**를 위한 언어 모델입니다. 반면 YOLO, ResNet, VGG 등은 이미지 처리를 위한 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e26450",
   "metadata": {},
   "source": [
    "# 53. 자연어 처리를 위한 트랜스포머(Transformer) 기법이 아닌 것은?\n",
    "① Forget gate\n",
    "\n",
    "② Self-attention\n",
    "\n",
    "③ Multi-head attention\n",
    "\n",
    "④ Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27d412",
   "metadata": {},
   "source": [
    "정답: ① Forget gate\n",
    "\n",
    "해설:\n",
    "\n",
    "트랜스포머(Transformer)는 기존의 순환 신경망(RNN) 구조를 탈피하여 'Attention' 메커니즘만으로 문장 내 단어 간의 관계를 파악하는 혁신적인 모델입니다.\n",
    "\n",
    "① Forget gate (정답): 이는 트랜스포머가 아니라 기존의 RNN 계열 중 하나인 LSTM(Long Short-Term Memory) 모델의 핵심 구성 요소입니다. 과거의 정보를 얼마나 잊을지를 결정하는 역할을 합니다. 트랜스포머는 이러한 게이트 구조 대신 어텐션 구조를 사용합니다.\n",
    "\n",
    "② Self-attention (옳음): 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산하여 문맥을 파악하는 트랜스포머의 핵심 기술입니다. 자기 자신을 포함한 모든 단어와의 연관성을 수치화합니다.\n",
    "\n",
    "③ Multi-head attention (옳음): 셀프 어텐션을 여러 개(여러 개의 'Head') 병렬로 수행하는 기법입니다. 이를 통해 모델이 문장 내의 다양한 문맥 정보(예: 문법적 관계, 의미적 관계 등)를 동시에 학습할 수 있게 합니다.\n",
    "\n",
    "④ Positional Encoding (옳음): 트랜스포머는 RNN처럼 순차적으로 데이터를 입력받지 않고 한꺼번에 처리하기 때문에 단어의 위치 정보가 유실됩니다. 이를 보완하기 위해 각 단어의 위치 정보를 수치로 더해주는 기법이 바로 포지셔널 인코딩입니다.\n",
    "\n",
    "Tip: LSTM/GRU는 'Gate' 구조를 통해 정보를 관리하고, Transformer는 'Attention'과 'Encoding' 구조를 통해 정보를 처리한다는 점을 구분해서 기억하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee7eb03",
   "metadata": {},
   "source": [
    "# 54. 빈칸에 알맞은 단어를 올바르게 연결한 것은?\n",
    "( ㄱ )는 입력시퀀스를 단일벡터로 바꾸고, ( ㄴ )는 단일벡터를 출력시퀀스로 바꾼다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c67510",
   "metadata": {},
   "source": [
    "① 인코더, 디코더\n",
    "\n",
    "② 디코더, 인코더\n",
    "\n",
    "③ 제너레이터, 디스크리미네이터\n",
    "\n",
    "④ 디스크리미네이터, 제너레이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab79eef",
   "metadata": {},
   "source": [
    "정답: ① 인코더, 디코더\n",
    "\n",
    "해설:\n",
    "\n",
    "자연어 처리나 이미지 캡셔닝 등에서 자주 쓰이는 Seq2Seq(Sequence-to-Sequence) 모델의 핵심 구조를 묻는 문제입니다.\n",
    "\n",
    "① 인코더, 디코더 (정답): * ( ㄱ ) 인코더(Encoder): 입력 데이터(예: 한국어 문장)를 받아 그 의미를 압축한 하나의 고정된 크기의 **단일 벡터(Context Vector)**로 변환하는 역할을 합니다. 정보를 '부호화'한다고 표현합니다.\n",
    "\n",
    "( ㄴ ) 디코더(Decoder): 인코더가 만든 단일 벡터를 다시 입력받아 우리가 원하는 형태의 출력 데이터(예: 영어 번역 문장)로 변환하는 역할을 합니다. 정보를 '복호화'하여 풀어내는 과정입니다.\n",
    "\n",
    "③, ④ 제너레이터, 디스크리미네이터 (오답): 이는 **GAN(Generative Adversarial Network, 생성적 적대 신경망)**의 구성 요소입니다. 제너레이터(생성자)는 가짜 데이터를 만들고, 디스크리미네이터(판별자)는 그것이 진짜인지 가짜인지 판별하며 서로 경쟁하는 구조입니다.\n",
    "\n",
    "Tip: '입력을 벡터로 압축 = 인코더', **'벡터를 출력으로 복원 = 디코더'**라고 연결해서 기억해 두세요. 번역기(NMT)의 가장 기본적인 원리입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df0d53",
   "metadata": {},
   "source": [
    "# 55. 다음은 인코딩 기법에 대한 예시이다. 해당 인코딩 기법에 대해 알맞은 것은?\n",
    "\"Red\" = (1, 0, 0)\n",
    "\n",
    "\"Green\" = (0, 1, 0)\n",
    "\n",
    "\"Blue\" = (0, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e180ab",
   "metadata": {},
   "source": [
    "① 타켓인코딩(Target Encoding)\n",
    "\n",
    "② 원핫인코딩(One-Hot Encoding)\n",
    "\n",
    "③ 레이블인코딩(Label Encoding)\n",
    "\n",
    "④ 빈도인코딩(Frequency Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5446cf4",
   "metadata": {},
   "source": [
    "정답: ② 원핫인코딩(One-Hot Encoding)\n",
    "\n",
    "해설:\n",
    "\n",
    "머신러닝 알고리즘은 문자 데이터를 직접 처리할 수 없기 때문에 숫자로 변환하는 과정(인코딩)이 필요합니다.\n",
    "\n",
    "② 원핫인코딩(One-Hot Encoding) (정답): 범주형 데이터를 0과 1로 이루어진 벡터로 변환하는 방식입니다. 단어의 개수만큼 차원을 만들고, 해당되는 단어의 위치에만 **1(Hot)**을 표시하고 나머지는 모두 0으로 채웁니다. 예시처럼 Red, Green, Blue 세 범주를 각각 겹치지 않는 독립적인 벡터로 표현하고 있습니다.\n",
    "\n",
    "① 타겟인코딩(Target Encoding) (오답): 범주형 변수의 값을 해당 범주의 타겟 변수(정답)의 평균값으로 변환하는 방식입니다. 주로 과적합 위험이 있어 주의해서 사용해야 합니다.\n",
    "\n",
    "③ 레이블인코딩(Label Encoding) (오답): 각 범주에 0, 1, 2, 3... 과 같이 정수 값을 부여하는 방식입니다. (예: Red=0, Green=1, Blue=2). 간단하지만 숫자의 크기(2 > 0)가 모델에 잘못된 순서나 가중치 정보를 줄 수 있다는 단점이 있습니다.\n",
    "\n",
    "④ 빈도인코딩(Frequency Encoding) (오답): 각 범주가 데이터 집합에서 나타나는 빈도수(또는 비율)로 변환하는 방식입니다.\n",
    "\n",
    "Tip: 범주들 사이에 순서나 크기 비교가 의미가 없을 때(예: 색상, 성별, 혈액형)는 원핫인코딩을 주로 사용합니다.\n",
    "\n",
    "드디어 제공해주신 기출 복원 문제의 마지막 번호까지 모두 풀이해 보았습니다! 제7회 빅데이터 분석기사 시험의 전반적인 개념들을 훑어보셨는데, 전반적으로 **통계 기초(2과목)**와 머신러닝 모델의 수식/특징(3과목) 비중이 높았던 것으로 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00636b",
   "metadata": {},
   "source": [
    "# 56. 소셜 미디어 데이터 분석방법으로 옳지 않은 것은?\n",
    "① 텍스트 마이닝\n",
    "\n",
    "② 네트워크 분석\n",
    "\n",
    "③ 워드클라우드 분석\n",
    "\n",
    "④ 맵리듀스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e72b2c",
   "metadata": {},
   "source": [
    "56. 소셜 미디어 데이터 분석방법으로 옳지 않은 것은?\n",
    "정답: ④ 맵리듀스\n",
    "\n",
    "해설:\n",
    "\n",
    "이 문제는 **'분석 방법(기법)'**과 **'처리 기술(플랫폼/프레임워크)'**을 구분할 수 있는지 묻는 문제입니다.\n",
    "\n",
    "④ 맵리듀스 (MapReduce) (정답): 맵리듀스는 소셜 미디어 데이터를 분석하는 '방법론'이 아니라, 대규모의 빅데이터를 병렬로 처리하기 위한 **소프트웨어 프레임워크(컴퓨팅 기술)**입니다. 구글에서 발표하고 하둡(Hadoop)에서 채택한 기술로, 데이터를 나누고(Map) 다시 합치는(Reduce) 분산 처리 방식을 의미합니다.\n",
    "\n",
    "① 텍스트 마이닝 (옳음): 소셜 미디어의 비정형 텍스트 데이터에서 의미 있는 정보를 추출하는 가장 기본적인 분석 방법입니다. 감성 분석(Sentiment Analysis) 등이 포함됩니다.\n",
    "\n",
    "② 네트워크 분석 (옳음): 사용자 간의 팔로우, 좋아요, 공유 관계 등을 노드와 링크로 연결하여 영향력 있는 인물이나 커뮤니티 구조를 파악하는 방법입니다. 소셜 미디어 분석의 핵심 기법 중 하나입니다.\n",
    "\n",
    "③ 워드클라우드 분석 (옳음): 출현 빈도가 높은 핵심 단어들을 시각적으로 크기를 다르게 하여 한눈에 파악할 수 있게 하는 기법으로, 소셜 미디어의 트렌드 파악에 자주 활용됩니다.\n",
    "\n",
    "Tip: **기술(Hadoop, Spark, MapReduce)**과 **기법(Mining, Clustering, Network Analysis)**을 구분하는 문제는 기출에서 자주 등장하므로 꼭 정리해 두세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac24fa0",
   "metadata": {},
   "source": [
    "# 57. 병렬화에 알맞은 모델의 배합으로 옳은 것은?\n",
    "① 배깅 - 아다부스트\n",
    "\n",
    "② 배깅 - 랜덤포레스트\n",
    "\n",
    "③ 부스팅 - 랜덤포레스트\n",
    "\n",
    "④ 부스팅 - 그라디언트부스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835a3c0",
   "metadata": {},
   "source": [
    "57. 병렬화에 알맞은 모델의 배합으로 옳은 것은?\n",
    "정답: ② 배깅 - 랜덤포레스트\n",
    "\n",
    "해설:\n",
    "\n",
    "앙상블 학습 기법은 여러 모델을 결합하는 방식에 따라 병렬(Parallel) 처리가 가능한지, 아니면 순차(Sequential) 처리를 해야 하는지로 나뉩니다.\n",
    "\n",
    "② 배깅(Bagging) - 랜덤포레스트(Random Forest) (정답): * 배깅(Bagging): 여러 개의 샘플(Bootstrap)을 독립적으로 추출하여 각각 모델을 학습시키기 때문에 각 모델을 동시에 만드는 병렬 처리가 가능합니다.\n",
    "\n",
    "랜덤포레스트: 배깅의 대표적인 알고리즘으로, 수많은 의사결정나무를 독립적으로 생성하므로 병렬화에 매우 최적화되어 있습니다.\n",
    "\n",
    "① 아다부스트 / ④ 그라디언트부스트 (부스팅 계열): * 부스팅(Boosting): 이전 모델이 틀린 오차를 다음 모델이 보완해 나가는 방식입니다. 앞 모델의 결과가 나와야 다음 학습을 시작할 수 있는 순차적(Sequential) 방식이므로 병렬화가 어렵습니다(최근 XGBoost 등은 내부 연산을 병렬화하기도 하지만, 기본 원리는 순차적입니다).\n",
    "\n",
    "③ 부스팅 - 랜덤포레스트: 부스팅은 순차적, 랜덤포레스트는 병렬적 기법이므로 병렬화 배합으로 보기 어렵습니다.\n",
    "\n",
    "Tip: 앙상블 핵심 요약\n",
    "\n",
    "배깅(Bagging): 병렬 처리 / 편향 유지, 분산 감소 / (예) 랜덤포레스트\n",
    "\n",
    "부스팅(Boosting): 순차 처리 / 분산 유지, 편향 감소 / (예) AdaBoost, GBM, XGBoost, LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ec466",
   "metadata": {},
   "source": [
    "# 58. 다음 중 잘못 분류된 데이터에 가중치를 부여하는 앙상블 방법은?\n",
    "① 배깅\n",
    "\n",
    "② 부스팅\n",
    "\n",
    "③ 보팅\n",
    "\n",
    "④ 가지치기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708df39f",
   "metadata": {},
   "source": [
    "정답: ② 부스팅\n",
    "\n",
    "해설:\n",
    "\n",
    "앙상블 기법 중 **부스팅(Boosting)**은 성능이 낮은 '약한 학습기(Weak Learner)'들을 결합하여 강력한 모델을 만드는 과정에서 데이터의 가중치를 핵심적으로 활용합니다.\n",
    "\n",
    "② 부스팅 (정답): * 학습 단계에서 이전 모델이 **잘못 분류한 데이터(오답)**에 대해 더 큰 가중치를 부여합니다.\n",
    "\n",
    "다음 모델은 가중치가 높아진 데이터를 더 집중적으로 학습하여 오차를 줄여나갑니다.\n",
    "\n",
    "이 과정을 반복(Sequential)하면서 모델의 예측력을 높이며, 특히 편향(Bias)을 감소시키는 데 효과적입니다.\n",
    "\n",
    "대표적인 예: AdaBoost, Gradient Boosting(GBM), XGBoost, LightGBM.\n",
    "\n",
    "① 배깅 (오답): 각 모델을 독립적으로 학습시키는 방식입니다. 오답에 가중치를 두는 것이 아니라, 전체 데이터에서 무작위로 복원 추출(Bootstrap)하여 여러 모델을 평균내는 방식입니다. (예: 랜덤 포레스트)\n",
    "\n",
    "③ 보팅 (오답): 서로 다른 여러 알고리즘 모델들이 낸 결과를 다수결(하드 보팅)이나 확률 평균(소프트 보팅)으로 최종 결정하는 방식입니다.\n",
    "\n",
    "④ 가지치기 (오답): 의사결정나무의 과적합을 방지하기 위해 불필요한 가지를 제거하는 기법으로, 앙상블 기법 자체를 의미하지는 않습니다.\n",
    "\n",
    "Tip: 시험 단골 키워드\n",
    "\n",
    "부스팅: #순차적 #오답에_가중치 #편향감소 #AdaBoost\n",
    "\n",
    "배깅: #병렬적 #독립적_샘플링 #분산감소 #RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de395365",
   "metadata": {},
   "source": [
    "# 59. 2개의 집단에서 사용되는 비모수 검정방법은?\n",
    "① z검정\n",
    "\n",
    "② 카이제곱검정\n",
    "\n",
    "③ 윌콕슨 순위합 검정\n",
    "\n",
    "④ T검정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f8fd6f",
   "metadata": {},
   "source": [
    "정답: ③ 윌콕슨 순위합 검정\n",
    "\n",
    "해설:\n",
    "\n",
    "통계적 가설 검정은 모집단의 분포에 대한 가정 여부에 따라 모수 검정과 비모수 검정으로 나뉩니다.\n",
    "\n",
    "③ 윌콕슨 순위합 검정 (Wilcoxon Rank-Sum Test) (정답): 두 집단의 독립된 표본을 비교할 때 사용하는 대표적인 비모수 검정 방법입니다. 데이터가 정규분포를 따르지 않거나 표본 수가 적을 때, 실제 값 대신 데이터의 **순위(Rank)**를 매겨 그 합을 비교합니다. (맨-휘트니 U 검정과 사실상 같은 목적의 검정입니다.)\n",
    "\n",
    "① z검정 / ④ T검정 (오답): 모집단이 정규분포를 따른다는 가정이 필요한 모수 검정입니다. 집단이 2개일 때 평균을 비교하는 가장 일반적인 방식입니다.\n",
    "\n",
    "② 카이제곱검정 (오답): 범주형 데이터(빈도) 간의 독립성이나 적합도를 검정할 때 사용합니다. 평균 차이를 비교하는 비모수 검정과는 성격이 다릅니다.\n",
    "\n",
    "Tip: 모수 vs 비모수 대응 관계 (시험 암기용) | 분석 목적 | 모수 검정 (정규성O) | 비모수 검정 (정규성X) | | :--- | :--- | :--- | | 두 집단 평균 비교 | 독립표본 T-test | 윌콕슨 순위합 검정 (또는 맨-휘트니) | | 짝지어진 두 집단 비교 | 대응표본 T-test | 윌콕슨 부호순위 검정 | | 세 집단 이상 비교 | 분산분석 (ANOVA) | 크루스칼-발리스 검정 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2958c",
   "metadata": {},
   "source": [
    "# 60. 차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 형상화 학습 기법은?\n",
    "① 다차원척도법\n",
    "\n",
    "② 자기조직화지도(SOM)\n",
    "\n",
    "③ 인공신경망\n",
    "\n",
    "④ 로지스틱회귀분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a26432",
   "metadata": {},
   "source": [
    "정답: ② 자기조직화지도(SOM, Self-Organizing Map)\n",
    "\n",
    "해설:\n",
    "\n",
    "이 문제는 비지도 학습의 대표적인 기법 중 하나인 SOM의 정의를 묻고 있습니다.\n",
    "\n",
    "② 자기조직화지도(SOM) (정답): * 고차원의 데이터를 2차원이나 3차원의 저차원 격자(Grid) 형태로 변환하여 시각적으로 이해하기 쉽게 나타내는 기법입니다.\n",
    "\n",
    "인공신경망의 구조를 따르지만, 정답(Label) 없이 입력 데이터의 유사성을 바탕으로 뉴런들이 스스로 지도를 구성하는 비지도 학습입니다.\n",
    "\n",
    "데이터의 특징을 보존하면서 차원을 축소하기 때문에 군집화와 시각화에 동시에 사용됩니다.\n",
    "\n",
    "① 다차원척도법 (MDS) (오답): 개체들 사이의 거리나 유사성을 보존하면서 저차원 공간에 점으로 배치하는 기법이지만, '뉴런'이나 '신경망' 구조를 사용하지는 않습니다.\n",
    "\n",
    "③ 인공신경망 (ANN) (오답): 신경망을 기반으로 한 학습 모델의 총칭입니다. 주로 지도 학습(분류/회귀)에 사용되며, 단순히 '저차원 뉴런으로의 형상화'보다는 복잡한 패턴 학습에 초점이 맞춰져 있습니다.\n",
    "\n",
    "④ 로지스틱 회귀분석 (오답): 선형 회귀 방식을 분류에 적용한 지도 학습 알고리즘으로, 차원 축소나 지도를 만드는 기법과는 거리가 멉니다.\n",
    "\n",
    "Tip: SOM의 핵심 키워드\n",
    "\n",
    "#비지도학습 #저차원_격자 #시각화 #경쟁학습(Winner-take-all)\n",
    "\n",
    "축하드립니다! 제7회 빅데이터 분석기사 복원 문제 60번까지 모든 풀이를 마쳤습니다.\n",
    "\n",
    "시험이 얼마 남지 않으셨다면, 오늘 푼 문제들 중 **가장 헷갈렸던 세 가지 개념(예: 48번 향상도 계산, 57번 앙상블 병렬화, 59번 비모수 검정)**만 다시 한번 눈에 익히시면 합격에 훨씬 가까워지실 거예요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfc773d",
   "metadata": {},
   "source": [
    "# 61. 혼동행렬을 계산한 값으로 옳은 것을 고르시오.\n",
    "\n",
    "실제값 \\ 예측값     0      1\n",
    "\n",
    "0              TN = 3 FP = 2\n",
    "\n",
    "1              FN = 2 TP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5b2ad",
   "metadata": {},
   "source": [
    "① 정분류율 = 3/4\n",
    "\n",
    "② 민감도 = 5/7\n",
    "\n",
    "③ 특이도 = 3/5\n",
    "\n",
    "④ 재현율 = 5/7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d195f0",
   "metadata": {},
   "source": [
    "정답: ③ 특이도 = 3/5\n",
    "\n",
    "해설:제시된 혼동행렬(Confusion Matrix)의 수치를 바탕으로 각 평가지표를 계산해 보겠습니다.\n",
    "\n",
    "1단계: 혼동행렬 구성 확인TN (True Negative): 3FP (False Positive): 2FN (False Negative): 2TP (True Positive): 3전체 데이터 수: $3 + 2 + 2 + 3 = 10$\n",
    "\n",
    "2단계: 각 지표 계산① 정분류율 (Accuracy): 전체 중 맞힌 비율$$\\frac{TP + TN}{Total} = \\frac{3 + 3}{10} = 0.6 \\text{ (또는 } 3/5)$$(오답: 보기의 3/4은 0.75이므로 틀림)\n",
    "\n",
    "② 민감도 (Sensitivity) = 재현율 (Recall): 실제 '1'인 것 중 '1'이라고 맞힌 비율$$\\frac{TP}{TP + FN} = \\frac{3}{3 + 2} = \\frac{3}{5} = 0.6$$(오답: 보기의 5/7는 틀림)\n",
    "\n",
    "③ 특이도 (Specificity): 실제 '0'인 것 중 '0'이라고 맞힌 비율$$\\frac{TN}{TN + FP} = \\frac{3}{3 + 2} = \\frac{3}{5} = 0.6$$** (정답: 3/5이 맞음)**④ 재현율 (Recall): 민감도와 같으므로 위 계산과 동일하게 3/5입니다.(오답: 보기의 5/7는 틀림)\n",
    "\n",
    "Tip: 헷갈리기 쉬운 공식 정리정확도는 전체 중에 맞힌 것재현율(민감도)은 실제(1) 중에 맞힌 것특이도는 실제(0) 중에 맞힌 것정밀도는 예측(1) 중에 맞힌 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623be498",
   "metadata": {},
   "source": [
    "# 62. 전기 사용량 계산에 사용할 지표로 적절하지 않은 것은?\n",
    "① MSE\n",
    "\n",
    "② RMSE\n",
    "\n",
    "③ MAPE\n",
    "\n",
    "④ F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d902a571",
   "metadata": {},
   "source": [
    "정답: ④ F1-Score\n",
    "\n",
    "해설:이 문제는 회귀(Regression) 모델의 평가지표와 분류(Classification) 모델의 평가지표를 구분할 수 있는지 묻는 문제입니다.\n",
    "\n",
    "전기 사용량: 150kWh, 230.5kWh 등과 같이 연속적인 숫자로 나타나는 데이터입니다. 따라서 이를 예측하는 모델은 회귀 모델입니다.\n",
    "\n",
    "④ F1-Score (정답): F1-Score는 정밀도(Precision)와 재현율(Recall)의 조화 평균으로, 분류 모델에서 모델의 성능을 측정할 때 사용하는 지표입니다. 수치 자체를 맞히는 전기 사용량 계산에는 사용할 수 없습니다.\n",
    "\n",
    "① MSE (Mean Squared Error) (오답): 평균제곱오차. 실제 사용량과 예측 사용량 차이의 제곱 평균으로, 회귀 모델의 가장 대표적인 지표입니다.\n",
    "\n",
    "② RMSE (Root Mean Squared Error) (오답): MSE에 루트를 씌운 값으로, 오차의 단위를 실제 사용량 단위와 맞춰주어 해석이 용이하게 만든 지표입니다.\n",
    "\n",
    "③ MAPE (Mean Absolute Percentage Error) (오답): 평균 절대 백분율 오차. 오차가 실제값에서 차지하는 비율을 퍼센트로 나타내어, 데이터의 크기와 상관없이 오차율을 비교하기 좋습니다.\n",
    "\n",
    "Tip: 시험 대비 암기 공식수치 예측(회귀): MSE, RMSE, MAE, MAPE, $R^2$ (결정계수)범주 예측(분류): 정확도, 정밀도, 재현율, F1-Score, ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ae490",
   "metadata": {},
   "source": [
    "# 63. 다음 중 F1-Score를 올바르게 표현한 것은?\n",
    "\n",
    "① $2 \\times (\\text{Precision} + \\text{Recall}) / (\\text{Precision} \\times \\text{Recall})$\n",
    "\n",
    "② $2 \\times (\\text{Precision} \\times \\text{Recall}) / (\\text{Precision} \\times \\text{Recall})$\n",
    "\n",
    "③ $2 \\times (\\text{Precision} \\times \\text{Recall}) / (\\text{Precision} + \\text{Recall})$\n",
    "\n",
    "④ $2 \\times \\text{Precision} \\times \\text{Recall} / (\\text{Precision} + \\text{Recall})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c82b36f",
   "metadata": {},
   "source": [
    "정답: ③ $2 \\times (\\text{Precision} \\times \\text{Recall}) / (\\text{Precision} + \\text{Recall})$\n",
    "\n",
    "(※ 참고: ④번도 수식적으로는 동일하지만, 일반적으로 괄호가 포함된 ③번의 형태가 표준적인 조화 평균 공식 표현으로 쓰입니다.)\n",
    "\n",
    "해설:F1-Score는 데이터가 불균형할 때(예: 암 환자 판별 등) 모델의 성능을 정확하게 평가하기 위해 \n",
    "\n",
    "**정밀도(Precision)**와 **재현율(Recall)**을 결합한 지표입니다.\n",
    "\n",
    "1. 조화 평균(Harmonic Mean)F1-Score는 단순히 산술 평균을 내는 것이 아니라 조화 평균을 사용합니다. \n",
    "\n",
    "조화 평균은 두 값 중 하나라도 매우 낮으면 전체 점수가 낮게 나오도록 설계되어 있어, 정밀도와 재현율이 균형을 이룰 때 높은 값을 가집니다.\n",
    "\n",
    "2. 수식 도출 과정조화 평균의 정의에 따라:$$\\text{F1-Score} = \\frac{2}{\\frac{1}{\\text{Precision}} + \\frac{1}{\\text{Recall}}}$$이 식을 통분하여 정리하면 다음과 같습니다:$$\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$3. 각 지표의 의미정밀도(Precision): 모델이 '참'이라고 예측한 것 중 실제 '참'인 비율재현율(Recall): 실제 '참'인 것 중 모델이 '참'이라고 맞힌 비율Tip: 시험에서 헷갈리지 않는 법분자는 곱셈($\\times$), 분모는 **덧셈($+$)**입니다. \"곱해서 2배 한 것을 더한 것으로 나눈다\"라고 기억하세요!63번까지 완벽하게 풀이하셨습니다! \n",
    "\n",
    "이제 **64번(ROC 커브와 AUC의 관계)**이나 65번(교차 검증 기법 - K-Fold 등) 문제로 넘어가 볼까요? 아니면 오늘 공부하신 **분류 평가지표 4총사(정확도, 정밀도, 재현율, F1)**를 완벽하게 비교한 요약표가 필요하신가요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a10387",
   "metadata": {},
   "source": [
    "# 64. k-평균군집분석에서 최적 k 평균을 구하는 방법으로 올바른 것은?\n",
    "① 엘보우 메소드(Elbow Method)\n",
    "\n",
    "② ROC Curve(Receiver Operating Characteristic Curve)\n",
    "\n",
    "③ 혼동행렬(Confusion Matrix)\n",
    "\n",
    "④ 특이도(Specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3086df5",
   "metadata": {},
   "source": [
    "정답: ① 엘보우 메소드(Elbow Method)\n",
    "\n",
    "해설:k-평균 군집화(k-means clustering)는 비지도 학습의 특성상 사전에 군집의 수($k$)를 지정해야 하는데, \n",
    "\n",
    "이를 결정하는 가장 대표적인 방법이 엘보우 메소드입니다.\n",
    "\n",
    "① 엘보우 메소드 (정답): 군집 내 오차 제곱합(SSE, Sum of Squared Errors)의 합을 그래프로 그렸을 때, 군집 수가 늘어남에 따라 오차가 급격히 줄어들다가 어느 순간 완만하게 변하는 지점(팔꿈치 모양처럼 꺾이는 부분)이 발생합니다. \n",
    "\n",
    "이 지점의 $k$를 최적의 군집 수로 판단합니다.\n",
    "[Image showing an Elbow Method graph with SSE on the y-axis and number of clusters k on the x-axis, highlighting the 'elbow' point]\n",
    "\n",
    "② ROC Curve (오답): 분류 모델의 성능을 평가하기 위해 민감도와 특이도의 관계를 나타낸 곡선입니다.\n",
    "\n",
    "③ 혼동행렬 (오답): 분류 모델의 예측값과 실제 정답을 교차 표 형태로 나타내어 성능을 평가하는 도구입니다.\n",
    "\n",
    "④ 특이도 (오답): 실제 음성인 데이터를 모델이 음성으로 올바르게 예측한 비율을 나타내는 분류 지표입니다.\n",
    "\n",
    "Tip: 최적의 $k$를 구하는 또 다른 방법으로는 **실루엣 계수(Silhouette Coefficient)**가 있습니다. 엘보우는 '오차'를 보고, 실루엣은 '군집 간의 거리와 군집 내 응집도'를 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f50f2d9",
   "metadata": {},
   "source": [
    "# 65. ROC 곡선에 대한 설명으로 옳지 않은 것은?\n",
    "\n",
    "① 머신러닝 모델을 평가한다. \n",
    "\n",
    "② 특이도와 민감도를 이용한다. \n",
    "\n",
    "③ X축은 특이도, Y축은 민감도이다. \n",
    "\n",
    "④ 아래 면적이 클수록 좋은 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e27167",
   "metadata": {},
   "source": [
    "정답: ③ X축은 특이도, Y축은 민감도이다.\n",
    "\n",
    "해설:ROC(Receiver Operating Characteristic) 곡선은 이진 분류 모델의 성능을 판단하는 매우 중요한 시각화 도구입니다.\n",
    "\n",
    "③ X축은 특이도, Y축은 민감도이다 (정답): \n",
    "\n",
    "* X축은 **1 - 특이도(1 - Specificity)**를 의미하는 **위양성률(FPR, False Positive Rate)**입니다. \n",
    "\n",
    "단순히 특이도 그 자체가 아닙니다.Y축은 **민감도(Sensitivity)**를 의미하는 **진양성률(TPR, True Positive Rate)**입니다.\n",
    "\n",
    "① 머신러닝 모델을 평가한다 (옳음): 주로 이진 분류 모델의 임곗값(Threshold) 변화에 따른 성능 변화를 평가할 때 사용합니다.\n",
    "\n",
    "② 특이도와 민감도를 이용한다 (옳음): ROC 곡선의 두 축인 FPR(1-특이도)과 TPR(민감도)은 모두 특이도와 민감도에서 유도된 값들입니다.\n",
    "\n",
    "④ 아래 면적이 클수록 좋은 모델이다 (옳음): ROC 곡선 아래의 면적을 **AUC(Area Under the Curve)**라고 합니다. \n",
    "\n",
    "AUC가 1에 가까울수록(곡선이 왼쪽 상단 구석에 붙을수록) 완벽한 모델이며, \n",
    "\n",
    "0.5에 가까우면 무작위 추측(쓸모없는 모델) 수준임을 의미합니다.\n",
    "\n",
    "Tip: 시험 암기 공식X축: $1 - \\text{Specificity}$ (가짜를 진짜라고 잘못 말할 확률)Y축: $\\text{Sensitivity}$ (진짜를 진짜라고 잘 맞힐 확률)드디어 제7회 빅데이터 분석기사 기출 복원 문제 65번까지 모든 해설을 마쳤습니다!통계(Z-score, 조건부 확률, CLT)부터 머신러닝 모델링(Lasso, 인공신경망, 앙상블), 그리고 모델 평가(혼동행렬, ROC)까지 데이터 분석의 핵심 전 과정을 훑어보셨네요. 이 정도 수준의 문제들을 완벽히 이해하셨다면 필기 합격권에 충분히 도달하신 겁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf6897",
   "metadata": {},
   "source": [
    "# 66. 교차검증에 대한 설명으로 옳지 않은 것은?\n",
    "\n",
    "① $k$폴드는 $k$개를 학습 데이터로 사용한다.\n",
    "\n",
    "② $k$폴드는 $k$개로 나눈다.\n",
    "\n",
    "③ 홀드아웃보다 계산이 빠르지 않다.\n",
    "\n",
    "④ 2번 나눈 폴드보다 $k$를 10으로 하면 더 신뢰할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c84f80",
   "metadata": {},
   "source": [
    "정답: ① $k$폴드는 $k$개를 학습 데이터로 사용한다.\n",
    "\n",
    "해설:교차검증은 데이터를 여러 개의 폴드(Fold)로 나누어 학습과 검증을 반복함으로써 모델의 일반화 성능을 높이는 기법입니다.\n",
    "\n",
    "① $k$폴드는 $k$개를 학습 데이터로 사용한다 (정답): $k$-폴드 교차검증은 데이터를 총 $k$개의 그룹(폴드)으로 나눕니다. \n",
    "\n",
    "그중 $k-1$개를 학습(Training) 데이터로 사용하고, 나머지 1개를 검증(Validation) 데이터로 사용하여 총 $k$번 반복합니다. \n",
    "\n",
    "따라서 $k$개 전체를 학습에 사용하는 것이 아닙니다.\n",
    "\n",
    "② $k$폴드는 $k$개로 나눈다 (옳음): 전체 데이터를 중복되지 않게 $k$개의 동일한 크기 그룹으로 등분하는 것이 기본 원리입니다.\n",
    "\n",
    "③ 홀드아웃보다 계산이 빠르지 않다 (옳음): 홀드아웃(Hold-out)은 데이터를 단순히 학습/검증으로 1번만 나누지만, 교차검증은 $k$번 반복 학습해야 하므로 시간이 더 오래 걸리고 계산 비용이 높습니다.\n",
    "\n",
    "④ 2번 나눈 폴드보다 $k$를 10으로 하면 더 신뢰할 수 있다 (옳음): $k$값이 커질수록(예: 10-fold) 더 많은 데이터 조합으로 모델을 검증하게 되므로, 특정 데이터 분할에 따른 편향이 줄어들어 결과의 신뢰도가 높아집니다.\n",
    "\n",
    "Tip: $k$-Fold 과정 요약데이터를 $k$개로 나눈다.$(k-1)$개로 학습하고 1개로 검증한다.이 과정을 검증 데이터 위치를 바꿔가며 $k$번 반복한다.$k$개 결과의 평균을 최종 성능으로 사용한다.지금까지 66번까지의 문제를 통해 빅데이터 분석기사의 핵심 이론들을 아주 잘 정리해 오셨습니다! 특히 모델 평가와 검증 파트는 실기에서도 매우 중요한 개념입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c71db8",
   "metadata": {},
   "source": [
    "# 67. 학습 데이터(Training Data)와 평가 데이터(Test Data)에 대한 설명으로 적절하지 않은 것은?\n",
    "\n",
    "① 평가 데이터를 학습에 사용해 모델의 성능을 높인다. \n",
    "\n",
    "② 학습 데이터와 평가 데이터는 전체 데이터의 개수에 따라 나눈다. \n",
    "\n",
    "③ 학습이 잘되었을 때 평가 데이터와 학습 데이터의 성능 차이가 작으면 모델이 적합하다고 할 수 있다. \n",
    "\n",
    "④ 모델을 구축할 때 학습 데이터가 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d69159",
   "metadata": {},
   "source": [
    "정답: ① 평가 데이터를 학습에 사용해 모델의 성능을 높인다.\n",
    "\n",
    "해설:\n",
    "\n",
    "데이터 분석 과정에서 데이터를 나누는 가장 큰 목적은 모델이 '처음 보는 데이터'에 대해서도 잘 작동하는지(일반화 성능)를 확인하기 위함입니다.\n",
    "\n",
    "① 평가 데이터를 학습에 사용해 모델의 성능을 높인다 (정답): 평가 데이터(Test Data)는 모델의 최종 성능을 측정하기 위해 '꽁꽁 숨겨두어야 하는' 데이터입니다. 만약 평가 데이터를 학습 과정에 포함시키면, 모델이 시험지 정답을 미리 보고 시험을 치르는 것과 같아집니다. 이를 **데이터 누수(Data Leakage)**라고 하며, 실제 성능보다 과하게 좋게 평가되는 오류를 범하게 됩니다.\n",
    "\n",
    "② 학습 데이터와 평가 데이터는 전체 데이터의 개수에 따라 나눈다 (옳음): 일반적으로 전체 데이터를 7:3 또는 8:2 비율로 나눕니다. 데이터가 매우 많으면 평가 데이터의 비율을 줄이기도 합니다.\n",
    "\n",
    "③ 성능 차이가 작으면 모델이 적합하다고 할 수 있다 (옳음): 학습 데이터에서는 성능이 매우 좋지만 평가 데이터에서 성능이 뚝 떨어진다면 '과적합(Overfitting)'입니다. 반대로 두 데이터의 성능 차이가 작으면서 둘 다 좋은 성능을 낸다면 모델이 일반화가 잘 되었다고(적합하다고) 판단합니다.\n",
    "\n",
    "④ 모델을 구축할 때 학습 데이터가 사용된다 (옳음): 학습 데이터(Training Data)는 모델이 데이터의 패턴을 파악하고 가중치(Parameter)를 업데이트하는 데 직접적으로 사용되는 데이터입니다.\n",
    "\n",
    "Tip: 데이터 세트의 3단계 구분\n",
    "\n",
    "Training Data: 모델 학습용 (공부하는 교과서)\n",
    "\n",
    "Validation Data: 하이퍼파라미터 튜닝 및 모델 선택용 (중간고사)\n",
    "\n",
    "Test Data: 최종 성능 검증용 (수능 시험)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88cb01",
   "metadata": {},
   "source": [
    "# 68. 다음 중 k-fold에서 k=10일 때 옳지 않은 것은?\n",
    "\n",
    "① 학습 데이터로 1개를 사용한다. \n",
    "\n",
    "② 평가 데이터로 1개를 사용한다. \n",
    "\n",
    "③ 평가 데이터는 전체 데이터의 10%를 차지한다.\n",
    "\n",
    "④ 평가 데이터를 통해 과적합을 방지할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904bc729",
   "metadata": {},
   "source": [
    "정답: ① 학습 데이터로 1개를 사용한다.\n",
    "\n",
    "해설:k-폴드 교차검증(k-fold Cross Validation)의 원리를 이해하면 쉽게 풀 수 있는 문제입니다. \n",
    "\n",
    "k=10이라는 것은 데이터를 총 **10개의 덩어리(폴드)**로 나누었다는 뜻입니다.\n",
    "\n",
    "① 학습 데이터로 1개를 사용한다 (정답): 10개로 나눈 데이터 중 1개는 검증(평가)용으로 사용하고, 나머지 9개($k-1$)를 학습용으로 사용합니다. 따라서 학습 데이터로 1개를 사용한다는 설명은 틀렸습니다.\n",
    "\n",
    "② 평가 데이터로 1개를 사용한다 (옳음): 분할된 10개의 폴드 중 한 번의 시행마다 교대로 1개씩을 평가 데이터로 활용합니다.\n",
    "\n",
    "③ 평가 데이터는 전체 데이터의 10%를 차지한다 (옳음): 전체를 10등분($k=10$)했으므로, 평가에 사용되는 1개의 폴드는 전체 데이터의 1/10인 10%가 됩니다.\n",
    "\n",
    "④ 평가 데이터를 통해 과적합을 방지할 수 있다 (옳음): 데이터를 고정해서 학습하는 것이 아니라, 검증 데이터를 바꿔가며 $k$번 반복 검증하기 때문에 특정 데이터셋에만 과도하게 맞춰지는 과적합(Overfitting) 문제를 파악하고 방지하는 데 효과적입니다.\n",
    "\n",
    "Tip: k-폴드 핵심 공식학습 데이터 양: $(k-1) / k$평가 데이터 양: $1 / k$반복 횟수: $k$번"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d5378",
   "metadata": {},
   "source": [
    "# 69. 적합도 검정(Fitness Validation)에 대한 설명으로 잘못된 것은?\n",
    "\n",
    "① 데이터 집합이 특정 확률분포(예: 이항분포, 다항분포)를 따르는지 확인하는 데 사용한다. \n",
    "\n",
    "② 카이제곱검정(Chi-Square Test)이 대표적이다. \n",
    "\n",
    "③ 범주형 데이터의 분포가 기대되는 분포와 일치하는지 검정하는 통계적인 방법 중 하나이다. \n",
    "\n",
    "④ 카이제곱검정에서 관측빈도수가 작을 때 관측빈도수와 기대빈도수의 합은 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4bae45",
   "metadata": {},
   "source": [
    "정답: ④ 카이제곱검정에서 관측빈도수가 작을 때 관측빈도수와 기대빈도수의 합은 동일하다.\n",
    "\n",
    "해설:적합도 검정(Goodness of Fit Test)은 표본 데이터의 분포가 가정된 특정 이론적 분포와 얼마나 잘 맞는지를 통계적으로 확인하는 방법입니다.\n",
    "\n",
    "④ 카이제곱검정에서 관측빈도수가 작을 때 관측빈도수와 기대빈도수의 합은 동일하다 (정답): 이 설명은 문맥상 함정이 있습니다. \n",
    "\n",
    "카이제곱검정의 기본 성질에 따르면, 관측빈도의 합($\\sum O$)과 기대빈도의 합($\\sum E$)은 데이터의 크기와 상관없이 항상 전체 사례 수($N$)와 같아야 합니다. \n",
    "\n",
    "관측빈도수가 작다고 해서 이 합의 법칙이 깨지거나 변하는 특수한 성질이 있는 것이 아닙니다. \n",
    "\n",
    "따라서 \"관측빈도수가 작을 때\"라는 전제 조건 자체가 논리적으로 어색하며, 문항의 의도는 오답을 유도하는 것입니다.\n",
    "\n",
    "① 데이터 집합이 특정 확률분포를 따르는지 확인하는 데 사용한다 (옳음): 수집한 데이터가 정규분포, 이항분포, 포아송분포 등 특정 이론적 모델에 부합하는지 검정하는 것이 적합도 검정의 본래 목적입니다.\n",
    "\n",
    "② 카이제곱검정이 대표적이다 (옳음): 카이제곱 적합도 검정은 각 범주(Category)의 관측빈도와 기대빈도의 차이를 이용하여 검정 통계량을 계산하는 가장 대중적인 방법입니다.\n",
    "\n",
    "③ 범주형 데이터의 분포가 기대되는 분포와 일치하는지 검정하는 통계적 방법이다 (옳음): 예를 들어, 주사위를 60번 던졌을 때 각 눈이 10번씩(기대분포) 나올 것이라는 가정과 실제 나온 횟수(관측분포)를 비교하는 데 사용됩니다.\n",
    "\n",
    "Tip: 카이제곱 적합도 검정 시 주의사항카이제곱검정은 표본의 크기가 충분히 커야 신뢰도가 높습니다. 일반적으로 특정 범주의 기대빈도가 5 미만인 셀이 전체의 20%를 넘지 않아야 한다는 조건이 붙습니다. 빈도수가 너무 작을 때는 '피셔의 정확 검정(Fisher's Exact Test)' 등을 고려해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f438ff",
   "metadata": {},
   "source": [
    "# 70. 다음 Q-Q Plot과 회귀선에 대한 설명으로 옳은 것은?\n",
    "\n",
    "(가) 왜도가 0 이상이다. \n",
    "\n",
    "(나) 분포가 좌측에 치우쳐져 있다. \n",
    "\n",
    "(다) 종속변수에 로그를 취하면 정규화된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6043869f",
   "metadata": {},
   "source": [
    "① 가, 나 \n",
    "\n",
    "② 가, 나, 다 \n",
    "\n",
    "③ 나, 다 \n",
    "\n",
    "④ 가, 다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb51fa02",
   "metadata": {},
   "source": [
    "정답: ② 가, 나, 다\n",
    "\n",
    "해설:\n",
    "\n",
    "Q-Q Plot(Quantile-Quantile Plot)은 데이터의 분포가 정규분포를 따르는지 시각적으로 확인하는 도구입니다. 문제에서 제시된 그래프가 '오른쪽으로 꼬리가 긴(Right-Skewed)' 형태를 보일 때의 해석을 묻고 있습니다.\n",
    "\n",
    "(가) 왜도(Skewness)가 0 이상이다 (옳음): 왜도는 분포의 비대칭 정도를 나타냅니다. 오른쪽으로 긴 꼬리를 가진 분포(Positive Skewness)는 왜도가 0보다 큽니다. 그래프의 점들이 회귀선(정규성 기준선)의 양 끝에서 위로 휘어지는 양상을 보이면 이는 오른쪽 꼬리가 두껍거나 긴 분포임을 의미합니다.\n",
    "\n",
    "(나) 분포가 좌측에 치우쳐져 있다 (옳음): 오른쪽으로 꼬리가 길다는 것은 데이터의 본체(최빈값)가 **왼쪽(좌측)**에 몰려 있다는 뜻입니다. 이를 '정적 왜도(Positive Skew)'라고도 부릅니다.\n",
    "\n",
    "(다) 종속변수에 로그를 취하면 정규화된다 (옳음): 오른쪽으로 치우친(Right-skewed) 데이터는 큰 값들이 드문드문 존재하여 정규성을 해칩니다. 이때 **로그 변환(Log Transformation)**을 수행하면 큰 값들 사이의 간격이 줄어들어 분포가 정규분포에 가깝게(종 모양으로) 보정됩니다. 이는 회귀 분석의 전제 조건인 정규성을 만족시키기 위한 대표적인 기법입니다.\n",
    "\n",
    "Tip: Q-Q Plot 보는 법 요약\n",
    "\n",
    "직선 위에 점들이 있다: 정규분포를 따름.\n",
    "\n",
    "오른쪽 끝이 위로 휨: 오른쪽 꼬리가 김 (왜도 > 0, 로그 변환 필요).\n",
    "\n",
    "왼쪽 끝이 아래로 휨: 왼쪽 꼬리가 김 (왜도 < 0, 제곱 변환 필요)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a710b4",
   "metadata": {},
   "source": [
    "# 71. 선형회귀가 과적합일 때 대처방법은?\n",
    "\n",
    "① 데이터의 양을 줄인다. \n",
    "\n",
    "② 모델의 복잡성을 높인다. \n",
    "\n",
    "③ 편향-분산 트레이드오프(Bias-Variance Trade-off) 관계를 확인한다. \n",
    "\n",
    "④ SSE를 구해서 확인한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372bc6c",
   "metadata": {},
   "source": [
    "정답: ③ 편향-분산 트레이드오프(Bias-Variance Trade-off) 관계를 확인한다.\n",
    "\n",
    "해설:과적합은 모델이 학습 데이터의 노이즈까지 너무 상세하게 학습하여, 실제 새로운 데이터(평가 데이터)에 대한 예측력이 떨어지는 상태를 말합니다.\n",
    "\n",
    "③ 편향-분산 트레이드오프 관계를 확인한다 (정답): 과적합된 모델은 **낮은 편향(Low Bias)**과 **높은 분산(High Variance)**을 가집니다. \n",
    "\n",
    "즉, 학습 데이터에는 딱 맞지만 외부 데이터에는 민감하게 반응하여 변동성이 큰 상태입니다. 이를 해결하기 위해 모델의 복잡도를 적절히 조절하여 편향을 약간 높이더라도 분산을 낮추는(오차의 총합을 줄이는) 지점을 찾는 것이 핵심 대처 방법입니다.\n",
    "\n",
    "① 데이터의 양을 줄인다 (오답): 오히려 데이터의 양을 늘리는 것이 과적합 방지에 도움이 됩니다. 데이터가 많아지면 특정 데이터의 노이즈에 모델이 휘둘릴 가능성이 낮아지기 때문입니다.\n",
    "\n",
    "② 모델의 복잡성을 높인다 (오답): 복잡성을 높이면 과적합이 더 심해집니다. 과적합 시에는 변수의 개수를 줄이거나(변수 선택), 가중치에 규제를 가하는(Lasso, Ridge) 등 모델을 단순화해야 합니다.\n",
    "\n",
    "④ SSE를 구해서 확인한다 (오답): SSE(오차 제곱합)는 단순히 모델의 오차 크기를 나타내는 지표일 뿐입니다. 학습 데이터의 SSE가 매우 낮다고 해서 그것이 과적합의 직접적인 해결책이 되지는 않습니다.\n",
    "\n",
    "Tip: 선형회귀 과적합 방지 주요 기법데이터 증강: 더 많은 학습 데이터를 확보합니다.변수 선택: 중요도가 낮은 독립변수를 제거하여 차원을 축소합니다.규제(Regularization) 적용: 가중치 크기를 제한하는 **Lasso($L1$ 규제)**나 Ridge($L2$ 규제) 모델을 사용합니다.교차 검증: $k$-fold 등을 통해 일반화 성능을 상시 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c3bec",
   "metadata": {},
   "source": [
    "# 72. 과적합에 대한 설명으로 틀린 것은?\n",
    "\n",
    "① 파라미터 수를 늘리면 과적합이 된다. \n",
    "\n",
    "② 과적합은 학습 데이터와 검증 데이터 간 성능 차이가 크지만, 과소적합은 차이가 작다. \n",
    "\n",
    "③ 학습 데이터에 너무 적합하게 학습되어, 학습 데이터에 대한 성능은 매우 우수하지만 검증 데이터나 테스트 데이터에 대한 성능이 크게 저하되는 경우를 말한다. \n",
    "\n",
    "④ 과적합이나 과소적합 모두 모델의 일반화 능력을 저하시키므로 균형을 찾는 것이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a919b25",
   "metadata": {},
   "source": [
    "정답: ② 과적합은 학습 데이터와 검증 데이터 간 성능 차이가 크지만, 과소적합은 차이가 작다. (오답 논의)(※ 이 문제는 문항의 해석에 따라 정답이 갈릴 수 있으나, 일반적으로 시험에서는 과소적합의 정의와 비교했을 때 상대적인 특징을 묻습니다. 해설을 참고하세요.)\n",
    "\n",
    "해설:과적합과 과소적합은 모델의 **복잡도(Complexity)**와 데이터 학습 수준에 따른 상태를 나타냅니다.\n",
    "\n",
    "② 과적합은 학습 데이터와 검증 데이터 간 성능 차이가 크지만, 과소적합은 차이가 작다 (틀린 설명일 가능성이 높은 이유): * 과적합: 학습 데이터는 완벽히 맞히지만 검증 데이터는 못 맞히므로 성능 차이가 매우 큽니다. (맞는 설명)과소적합: 모델이 너무 단순해서 학습 데이터조차 제대로 학습하지 못한 상태입니다. 따라서 학습 데이터 성능도 낮고, 검증 데이터 성능도 낮습니다. 결과적으로 둘 다 성능이 낮아서 차이가 작아 보일 수 있지만, 근본적으로는 모델이 데이터의 패턴을 전혀 잡지 못한 상태이므로 \"성능 차이가 작다\"는 것이 과소적합의 핵심 정의나 특징은 아닙니다.만약 이 문항이 정답이라면, 과소적합은 성능 차이의 문제가 아니라 전체적인 성능 저하의 문제임을 강조하는 의도일 것입니다.\n",
    "\n",
    "① 파라미터 수를 늘리면 과적합이 된다 (옳음): 모델의 파라미터(가중치)가 많아진다는 것은 모델이 더 복잡해진다는 뜻입니다. 너무 복잡한 모델은 학습 데이터의 아주 미세한 노이즈까지 다 외워버리므로 과적합이 발생하기 쉽습니다.\n",
    "\n",
    "③ 학습 데이터에 성능은 우수하지만 검증 데이터 성능이 저하되는 경우 (옳음): 과적합의 가장 교과서적인 정의입니다. '암기'는 잘하지만 '응용'은 못하는 상태입니다.\n",
    "\n",
    "④ 모델의 일반화 능력을 저하시키므로 균형을 찾는 것이 중요하다 (옳음): 과소적합은 너무 게으른 모델, 과적합은 너무 예민한 모델입니다. 우리는 그 중간 어디쯤에서 새로운 데이터도 잘 맞히는 일반화(Generalization) 성능이 극대화되는 지점을 찾아야 합니다.\n",
    "\n",
    "Tip: 과적합 vs 과소적합 한눈에 보기| 구분 | 과소적합 (Underfitting) | 과적합 (Overfitting) || :--- | :--- | :--- || 모델 복잡도 | 매우 낮음 (단순) | 매우 높음 (복잡) || 편향 (Bias) | 높음 (정답에서 멀어짐) | 낮음 (학습 데이터에 근접) || 분산 (Variance) | 낮음 (변동성 작음) | 높음 (변동성 큼) || 해결 방법 | 모델 복잡도 ↑, 변수 추가 | 데이터 증강, 규제($L1/L2$), 변수 제거 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae2b7f",
   "metadata": {},
   "source": [
    "# 73. 과소적합에 대한 설명으로 올바른 것은?\n",
    "\n",
    "① 학습 데이터 정확도 70%, 평가 데이터 정확도 70% \n",
    "\n",
    "② 학습 데이터 정확도 70%, 평가 데이터 정확도 90% \n",
    "\n",
    "③ 학습 데이터 정확도 90%, 평가 데이터 정확도 70% \n",
    "\n",
    "④ 학습 데이터 정확도 90%, 평가 데이터 정확도 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2256661",
   "metadata": {},
   "source": [
    "정답: ① 학습 데이터 정확도 70%, 평가 데이터 정확도 70%\n",
    "\n",
    "해설:과소적합과 과적합을 구분하는 가장 쉬운 기준은 **\"학습 데이터(Training Data)를 충분히 잘 맞히고 있는가?\"**와 **\"학습과 평가 데이터 간의 성능 차이가 어떠한가?\"**를 보는 것입니다.\n",
    "\n",
    "① 학습 데이터 70%, 평가 데이터 70% (정답):학습 데이터의 정확도가 낮다는 것은 모델이 데이터의 패턴을 충분히 학습하지 못했음을 의미합니다. 즉, 모델이 너무 단순하여 학습 데이터조차 제대로 맞히지 못하는 상태가 전형적인 과소적합의 모습입니다. 이때는 보통 평가 데이터의 점수도 함께 낮게 나옵니다.\n",
    "\n",
    "② 학습 데이터 70%, 평가 데이터 90% (오답):현실적으로 거의 일어나기 힘든 사례입니다. 학습하지 않은 데이터를 학습한 데이터보다 훨씬 더 잘 맞히는 경우는 데이터 분할이 잘못되었거나 운이 좋은 경우로, 일반적인 과소/과적합의 범주로 설명하지 않습니다.\n",
    "\n",
    "③ 학습 데이터 90%, 평가 데이터 70% (오답):학습 데이터는 매우 잘 맞히지만(암기), 새로운 평가 데이터는 잘 맞히지 못하는(응용 실패) 상태입니다. 이것은 전형적인 **과적합(Overfitting)**의 사례입니다.\n",
    "\n",
    "④ 학습 데이터 90%, 평가 데이터 90% (오답):학습도 잘 되었고, 새로운 데이터에 대한 일반화 성능도 훌륭한 상태입니다. 우리가 지향해야 할 가장 이상적인 모델의 모습입니다.\n",
    "\n",
    "Tip: 시험장 직관 풀이법둘 다 낮으면? $\\rightarrow$ 과소적합 (공부 자체가 부족함)학습만 높고 평가가 낮으면? $\\rightarrow$ 과적합 (문제만 달달 외움)둘 다 높으면? $\\rightarrow$ 적정/우수 (공부도 잘하고 응용도 잘함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0766678a",
   "metadata": {},
   "source": [
    "# 74. 초매개변수(하이퍼파라미터) 최적화에 대한 설명으로 옳지 않은 것은?\n",
    "\n",
    "① 초매개변수는 변경이 가능하다. \n",
    "\n",
    "② 모델의 성능은 이미 정해진 손실함수에 의해 결정된다. \n",
    "\n",
    "③ 초매개변수 선택은 모델 선택 전 데이터 집합 수준에서 결정 가능하다. \n",
    "\n",
    "④ 초매개변수 사용 시 경험이 많은 전문가가 유리하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a78b24",
   "metadata": {},
   "source": [
    "정답: ② 모델의 성능은 이미 정해진 손실함수에 의해 결정된다.\n",
    "\n",
    "해설:하이퍼파라미터(초매개변수)는 모델 학습 전에 사용자가 직접 설정해주는 변수로, 모델의 내부 가중치를 업데이트하는 '학습 방식' 자체를 통제합니다.\n",
    "\n",
    "② 모델의 성능은 이미 정해진 손실함수에 의해 결정된다 (정답): 모델의 최종 성능은 단순히 손실함수(Loss Function) 하나로 결정되지 않습니다. **어떤 하이퍼파라미터(학습률, 배치 크기, 은닉층 수 등)**를 선택하느냐에 따라 동일한 손실함수를 사용하더라도 모델이 최적값(Global Minimum)을 찾아가는 경로와 결과가 완전히 달라집니다. 즉, 성능은 모델 구조, 데이터 품질, 하이퍼파라미터 튜닝의 조합으로 결정됩니다.\n",
    "\n",
    "① 초매개변수는 변경이 가능하다 (옳음): 사용자가 직접 설정하는 값이므로, 그리드 서치(Grid Search)나 랜덤 서치(Random Search) 등을 통해 최적의 값을 찾기 위해 계속해서 변경하고 실험할 수 있습니다.\n",
    "\n",
    "③ 초매개변수 선택은 모델 선택 전 데이터 집합 수준에서 결정 가능하다 (옳음): 데이터의 특성(데이터의 양, 차원, 노이즈 정도 등)에 따라 적절한 하이퍼파라미터의 범위나 초깃값을 어느 정도 사전에 가늠해 볼 수 있습니다. 예를 들어, 데이터 양이 적을 때는 과적합을 막기 위해 더 강한 규제 계수를 설정하는 식입니다.\n",
    "\n",
    "④ 초매개변수 사용 시 경험이 많은 전문가가 유리하다 (옳음): 하이퍼파라미터는 명확한 수학적 정답이 정해져 있지 않은 경우가 많습니다. 따라서 비슷한 유형의 데이터를 다뤄본 전문가의 직관이나 경험이 최적의 조합을 빨리 찾아내는 데 큰 도움이 됩니다.\n",
    "\n",
    "Tip: 매개변수(Parameter) vs 초매개변수(Hyperparameter)매개변수: 모델 내부에서 학습을 통해 자동으로 결정되는 값 (예: 가중치($w$), 편향($b$))초매개변수: 사용자가 학습 전에 수동으로 설정하는 값 (예: 학습률(Learning Rate), K-NN의 $k$값, 나무의 깊이)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe32eac8",
   "metadata": {},
   "source": [
    "# 75. 다음 중 앙상블 기법이 적용된 것으로 올바르지 않은 것은?\n",
    "\n",
    "① 가, 나 (가: k=1, 5, 7인 KNN 기법을 결합시킨다) \n",
    "\n",
    "② 나, 다 (나: 로지스틱 회귀분석, 의사결정나무, 나이브 베이즈 모델을 결합시킨다) \n",
    "\n",
    "③ 가, 다 (다: 선형회귀 모델을 결합시킨다) \n",
    "\n",
    "④ 가, 나, 다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6624de52",
   "metadata": {},
   "source": [
    "정답: ③ 가, 다 (※ 문항 구성상 단일 모델 결합에 대한 정의를 묻는 문제입니다.)\n",
    "\n",
    "해설:앙상블(Ensemble) 기법의 핵심은 **\"여러 개의 기초 학습기(Weak Learners)를 결합하여 하나의 강력한 학습기를 만드는 것\"**입니다. 하지만 무조건 합친다고 해서 앙상블의 효과가 나는 것은 아니며, 각 기법의 특성에 따라 구분됩니다.\n",
    "\n",
    "(가) $k=1, 5, 7$인 KNN 기법을 결합시킨다 (올바르지 않음/논란): 일반적인 앙상블은 서로 다른 샘플링(배깅)이나 오차 보정(부스팅)을 통해 모델을 다양화합니다. 단순히 동일한 알고리즘에서 하이퍼파라미터($k$)만 바꾼 모델들을 결합하는 것은 앙상블의 기본 정의보다는 **모델 선택(Model Selection)**이나 단순 보팅에 가깝습니다. 특히 KNN은 모델 자체가 데이터를 저장하는 방식이라 앙상블의 효과가 다른 알고리즘에 비해 매우 낮습니다.\n",
    "\n",
    "(나) 로지스틱 회귀분석, 의사결정나무, 나이브 베이즈 모델을 결합시킨다 (올바름): 이는 전형적인 보팅(Voting) 또는 스태킹(Stacking) 앙상블 기법입니다. 서로 다른 원리를 가진 알고리즘들을 결합하여 각 모델의 단점을 상호 보완하는 방식이므로 앙상블의 아주 좋은 사례입니다.\n",
    "\n",
    "(다) 선형회귀 모델을 결합시킨다 (올바르지 않음): 단순히 동일한 선형회귀 모델 여러 개를 아무런 장치(데이터 분할 등) 없이 결합하는 것은 수학적으로 결국 하나의 또 다른 선형 모델이 될 뿐입니다. 앙상블로서의 성능 향상(분산 감소나 편향 감소)을 기대하기 어렵기 때문에 적절한 앙상블 사례로 보지 않습니다.\n",
    "\n",
    "Tip: 앙상블이 되기 위한 조건다양성(Diversity): 개별 모델들이 서로 다른 실수를 해야 합니다.독립성: 모델들이 서로 독립적으로 학습되거나(배깅), 서로를 보완해야 합니다(부스팅)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52992fdd",
   "metadata": {},
   "source": [
    "# 76. 시공간 데이터에 대한 설명으로 옳지 않은 것은?\n",
    "\n",
    "① 기상 데이터는 특정 위치에서 시간에 따른 기후 조건을 기록한 것이며, 이러한 데이터는 공간상의 여러 위치에서 수집된다. \n",
    "\n",
    "② 시간적 변동성을 분석하여 추세, 계절성, 주기성 등을 파악하며, 공간적 변동성을 통해 서로 다른 위치에서의 데이터 패턴을 비교하고 해석한다. \n",
    "\n",
    "③ 시공간 데이터는 다차원 데이터로 간주되며, 각 시간 스텝에서 여러 공간 위치에서 관측된 값들로 이루어져 있다. \n",
    "\n",
    "④ 공간 데이터는 시간데이터를 계산하여 추출할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe114b8",
   "metadata": {},
   "source": [
    "정답: ④ 공간 데이터는 시간데이터를 계산하여 추출할 수 있다.\n",
    "\n",
    "해설:\n",
    "\n",
    "시공간 데이터는 공간(Space) 정보와 시간(Time) 정보가 결합된 고차원 데이터입니다.\n",
    "\n",
    "④ 공간 데이터는 시간데이터를 계산하여 추출할 수 있다 (정답): 공간 데이터(위도, 경도, 고도 등)와 시간 데이터(날짜, 시각 등)는 서로 독립적인 차원입니다. \"시간이 흐른다고 해서 위치 좌표가 자동으로 계산\"되거나 \"위치를 안다고 해서 특정 시각이 도출\"되는 인과관계가 아니기 때문에, 한 쪽을 계산하여 다른 한 쪽을 추출할 수 있다는 설명은 틀렸습니다.\n",
    "\n",
    "① 기상 데이터의 수집 방식 (옳음): 기상 데이터는 전형적인 시공간 데이터입니다. 특정 지점(공간)에서 매시간(시간) 기온, 습도 등을 기록하며, 이러한 관측소들이 전국(여러 공간 위치)에 퍼져 있는 구조입니다.\n",
    "\n",
    "② 시간적/공간적 변동성 분석 (옳음): * 시간적 분석: 시계열 분석 기법을 통해 데이터의 흐름(추세), 계절적 요인 등을 파악합니다.\n",
    "\n",
    "공간적 분석: 지리 통계 기법을 통해 지역 간의 차이나 인접 지역 간의 유사성(공간적 자기상관)을 분석합니다.\n",
    "\n",
    "③ 다차원 데이터의 구성 (옳음): 시공간 데이터는 기본적으로 (위치, 시간, 속성값)의 구조를 가집니다. 이를 행렬이나 텐서(Tensor) 형태의 다차원 데이터로 표현하여 분석에 활용합니다.\n",
    "\n",
    "Tip: 시공간 데이터의 3대 요소\n",
    "\n",
    "공간(Where): 위치 정보 (GIS, 위경도 등)\n",
    "\n",
    "시간(When): 시점 정보 (Timestamp)\n",
    "\n",
    "속성(What): 측정값 (미세먼지 농도, 교통량 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e15ffbf",
   "metadata": {},
   "source": [
    "# 77. 명목형 데이터 요약 시 사용하는 그래프가 아닌 것은?\n",
    "\n",
    "① 막대그래프 \n",
    "\n",
    "② 원형그래프 \n",
    "\n",
    "③ 파레토그램 \n",
    "\n",
    "④ 히스토그램"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe1d41",
   "metadata": {},
   "source": [
    "정답: ④ 히스토그램\n",
    "\n",
    "해설:\n",
    "\n",
    "데이터의 성격(질적 데이터 vs 양적 데이터)에 따라 데이터를 시각화하는 도구는 완전히 달라집니다.\n",
    "\n",
    "④ 히스토그램 (정답): 히스토그램은 연속형(양적) 데이터의 분포를 나타낼 때 사용합니다. 가로축이 구간(Bin)으로 나뉘어 있어 데이터가 특정 범위에 얼마나 몰려 있는지를 보여줍니다. 막대 사이의 간격이 없는 것이 특징입니다.\n",
    "\n",
    "① 막대그래프 (오답): 각 **범주(명목형)**별 빈도나 수치를 막대 높이로 비교하는 가장 대표적인 그래프입니다. 막대 사이에 간격이 있어 각 범주가 독립적임을 나타냅니다.\n",
    "\n",
    "② 원형그래프 (오답): 전체에서 각 범주가 차지하는 비율(상대도수)을 한눈에 파악하기 위해 사용합니다.\n",
    "\n",
    "③ 파레토그램 (오답): 범주별 빈도를 막대그래프로 나타냄과 동시에, 누적 백분율을 선그래프로 함께 표시하는 그래프입니다. \"어떤 항목이 전체 문제의 80%를 차지하는가?\"와 같은 핵심 범주를 파악할 때 유용합니다.\n",
    "\n",
    "Tip: 데이터 종류별 시각화 도구 (시험 빈출)\n",
    "\n",
    "명목형/범주형: 막대그래프, 원형그래프, 파레토그램\n",
    "\n",
    "연속형/수치형: 히스토그램, 상자그림(Box Plot), 줄기-잎 그림, 산점도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e5d48",
   "metadata": {},
   "source": [
    "# 78. 시간 시각화에 대한 설명으로 적절한 것은?\n",
    "\n",
    "① 주로 시간의 경과에 따른 데이터의 변화를 나타낼 때 사용한다. \n",
    "\n",
    "② 시계열 데이터 시각화에는 선그래프, 영역그래프 등이 있다. \n",
    "\n",
    "③ 시간 데이터는 연속형과 이산형으로 나누어질 수 있다. \n",
    "\n",
    "④ 대표적인 시각화 도구로는 산점도, 막대그래프, 교통 데이터 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd82bf6",
   "metadata": {},
   "source": [
    "이 문제는 **시간 흐름에 따른 데이터의 특성(시계열 데이터)**을 어떻게 표현하고 분류하는지를 묻고 있습니다. 문항의 구성을 보면 가장 포괄적이고 시계열 데이터의 핵심 정의를 담고 있는 보기를 찾는 것이 중요합니다.\n",
    "\n",
    "정답: ② 시계열 데이터 시각화에는 선그래프, 영역그래프 등이 있다. (※ ①번과 ③번도 이론적으로는 맞는 설명이나, '시간 시각화'라는 주제에 가장 구체적인 방법론을 제시하는 ②번이 문제의 의도상 가장 적절한 정답으로 간주됩니다.)\n",
    "\n",
    "해설:\n",
    "\n",
    "② 시계열 데이터 시각화의 종류 (정답):\n",
    "\n",
    "선그래프(Line Chart): 시간의 흐름에 따른 연속적인 변화를 보여주는 가장 대표적인 도구입니다.\n",
    "\n",
    "영역그래프(Area Chart): 선그래프 아래의 면적을 채워 데이터의 누적 합계나 크기 변화를 강조할 때 사용합니다.\n",
    "\n",
    "① 시간의 경과에 따른 데이터 변화 (옳음): 시간 시각화의 근본적인 목적입니다. 다만, ②번이 시각화의 '수단'을 구체적으로 명시하고 있어 더 적절한 답안으로 채택되는 경우가 많습니다.\n",
    "\n",
    "③ 시간 데이터의 분류 (옳음):\n",
    "\n",
    "연속형 시간: 시, 분, 초 단위처럼 끊임없이 흐르는 시간.\n",
    "\n",
    "이산형 시간: 일, 월, 분기, 연도처럼 특정 단위로 끊어서 표현하는 시간.\n",
    "\n",
    "④ 시각화 도구의 잘못된 예시 (오답):\n",
    "\n",
    "산점도: 주로 두 변수 간의 상관관계를 나타낼 때 사용합니다.\n",
    "\n",
    "교통 데이터: 이것은 시각화 '도구'가 아니라 시각화의 '대상(데이터)'입니다. 시간 시각화의 대표 도구로는 **막대그래프(이산형 시간)**나 간트 차트(Gantt Chart) 등이 있습니다.\n",
    "\n",
    "Tip: 시간 시각화의 핵심 포인트\n",
    "\n",
    "연속적 흐름: 선그래프, 계단식 그래프\n",
    "\n",
    "비율과 누적: 영역그래프, 누적 막대그래프\n",
    "\n",
    "주기성 확인: 히트맵(Heatmap), 캘린더 차트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931b6dc",
   "metadata": {},
   "source": [
    "# 80. 아래와 같이 자동차별 수치를 활용하기에 적절한 시각화 기법은 무엇인가?\n",
    "항목별 점수 (주행 9.3, 가격 8.5, 거주성 9.6, 품질 9.6, 디자인 9.6, 연비 7.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2808b85",
   "metadata": {},
   "source": [
    "① 레이더차트\n",
    "\n",
    "② 산점도행렬 \n",
    "\n",
    "③ 버블차트 \n",
    "\n",
    "④ 히스토그램"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6776608",
   "metadata": {},
   "source": [
    "정답: ① 레이더 차트 (Radar Chart)\n",
    "\n",
    "해설:\n",
    "\n",
    "제시된 데이터는 '주행', '가격', '거주성' 등 **여러 가지 평가 항목(변수)**에 대한 점수로 이루어져 있습니다. 이러한 다변량 데이터를 시각화하는 데 가장 적합한 도구는 레이더 차트입니다.\n",
    "\n",
    "① 레이더 차트 (정답):\n",
    "\n",
    "방사형 차트라고도 불리며, 중심점에서 퍼져 나가는 축 위에 각 항목의 점수를 표시하고 이를 선으로 연결한 그래프입니다.\n",
    "\n",
    "여러 측정 항목을 한눈에 비교하여 **균형(Balance)**이나 강점과 약점을 파악하기에 매우 용이합니다. 자동차 성능 비교, 선수 능력치 비교 등에 자주 사용됩니다.\n",
    "\n",
    "② 산점도 행렬 (오답): 여러 변수 쌍 사이의 상관관계를 한꺼번에 파악하기 위해 산점도를 행렬 형태로 나열한 것입니다. 항목별 점수 요약에는 적합하지 않습니다.\n",
    "\n",
    "③ 버블 차트 (오답): 산점도의 변형으로, 데이터의 위치(X, Y축)뿐만 아니라 원의 크기(Z)를 통해 세 가지 정보를 동시에 나타낼 때 사용합니다.\n",
    "\n",
    "④ 히스토그램 (오답): 단일 연속형 변수의 분포(빈도)를 확인하기 위한 도구입니다.\n",
    "\n",
    "Tip: 데이터 특징별 시각화\n",
    "\n",
    "다변량 항목 비교: 레이더 차트, 평행 좌표 그래프\n",
    "\n",
    "상관관계 분석: 산점도, 버블 차트\n",
    "\n",
    "분포 확인: 히스토그램, 상자 그림(Box Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e90577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
